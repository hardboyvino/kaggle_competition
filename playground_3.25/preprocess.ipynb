{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adeniyi Babalola\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "# Set the Optuna logger to output only warnings or higher level messages\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment = 'remove_outliers_cap_predictions'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.841611</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>20.612526</td>\n",
       "      <td>11.08810</td>\n",
       "      <td>2.766</td>\n",
       "      <td>1.732</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.496070</td>\n",
       "      <td>0.91457</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.558488</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>20.298893</td>\n",
       "      <td>12.04083</td>\n",
       "      <td>2.755</td>\n",
       "      <td>1.631</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.492719</td>\n",
       "      <td>0.71760</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.885992</td>\n",
       "      <td>15.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>33.739258</td>\n",
       "      <td>12.08630</td>\n",
       "      <td>2.828</td>\n",
       "      <td>1.788</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.481478</td>\n",
       "      <td>1.50633</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.795296</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>20.213349</td>\n",
       "      <td>10.94850</td>\n",
       "      <td>2.648</td>\n",
       "      <td>1.626</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.489272</td>\n",
       "      <td>0.78937</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>9.577996</td>\n",
       "      <td>11.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>24.988133</td>\n",
       "      <td>11.82448</td>\n",
       "      <td>2.766</td>\n",
       "      <td>1.682</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.492736</td>\n",
       "      <td>1.86481</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  allelectrons_Total  density_Total  allelectrons_Average  val_e_Average  \\\n",
       "0   0               100.0       0.841611                  10.0            4.8   \n",
       "1   1               100.0       7.558488                  10.0            4.8   \n",
       "2   2                76.0       8.885992                  15.6            5.6   \n",
       "3   3               100.0       8.795296                  10.0            4.8   \n",
       "4   4               116.0       9.577996                  11.6            4.8   \n",
       "\n",
       "   atomicweight_Average  ionenergy_Average  el_neg_chi_Average  \\\n",
       "0             20.612526           11.08810               2.766   \n",
       "1             20.298893           12.04083               2.755   \n",
       "2             33.739258           12.08630               2.828   \n",
       "3             20.213349           10.94850               2.648   \n",
       "4             24.988133           11.82448               2.766   \n",
       "\n",
       "   R_vdw_element_Average  R_cov_element_Average  zaratio_Average  \\\n",
       "0                  1.732                  0.860         0.496070   \n",
       "1                  1.631                  0.910         0.492719   \n",
       "2                  1.788                  0.864         0.481478   \n",
       "3                  1.626                  0.936         0.489272   \n",
       "4                  1.682                  0.896         0.492736   \n",
       "\n",
       "   density_Average  Hardness  \n",
       "0          0.91457       6.0  \n",
       "1          0.71760       6.5  \n",
       "2          1.50633       2.5  \n",
       "3          0.78937       6.0  \n",
       "4          1.86481       6.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>5203.000000</td>\n",
       "      <td>3004.386460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.060000</td>\n",
       "      <td>5203.000000</td>\n",
       "      <td>10301.940000</td>\n",
       "      <td>10406.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>128.053516</td>\n",
       "      <td>224.123776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>719.400000</td>\n",
       "      <td>15300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density_Total</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>14.491342</td>\n",
       "      <td>15.972877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.739942</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>75.098979</td>\n",
       "      <td>643.093804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>17.033222</td>\n",
       "      <td>10.468734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_e_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>4.546789</td>\n",
       "      <td>0.690864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>37.507703</td>\n",
       "      <td>26.012313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.773227</td>\n",
       "      <td>26.203827</td>\n",
       "      <td>119.629500</td>\n",
       "      <td>167.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>10.938308</td>\n",
       "      <td>1.408276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.054000</td>\n",
       "      <td>11.202760</td>\n",
       "      <td>13.512520</td>\n",
       "      <td>15.245810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>2.607662</td>\n",
       "      <td>0.334906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.790000</td>\n",
       "      <td>2.706000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>3.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>1.731330</td>\n",
       "      <td>0.192481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.318667</td>\n",
       "      <td>1.732727</td>\n",
       "      <td>2.055000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>0.944132</td>\n",
       "      <td>0.180017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505333</td>\n",
       "      <td>0.915556</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>1.615840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaratio_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>0.493349</td>\n",
       "      <td>0.063080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405373</td>\n",
       "      <td>0.488550</td>\n",
       "      <td>0.707253</td>\n",
       "      <td>0.825990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density_Average</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>2.132984</td>\n",
       "      <td>1.936656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132734</td>\n",
       "      <td>1.351550</td>\n",
       "      <td>7.986670</td>\n",
       "      <td>10.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hardness</th>\n",
       "      <td>10407.0</td>\n",
       "      <td>4.647126</td>\n",
       "      <td>1.680525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>8.288000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count         mean          std  min          1%  \\\n",
       "id                     10407.0  5203.000000  3004.386460  0.0  104.060000   \n",
       "allelectrons_Total     10407.0   128.053516   224.123776  0.0    6.000000   \n",
       "density_Total          10407.0    14.491342    15.972877  0.0    0.739942   \n",
       "allelectrons_Average   10407.0    17.033222    10.468734  0.0    4.666667   \n",
       "val_e_Average          10407.0     4.546789     0.690864  0.0    2.000000   \n",
       "atomicweight_Average   10407.0    37.507703    26.012313  0.0    8.773227   \n",
       "ionenergy_Average      10407.0    10.938308     1.408276  0.0    8.054000   \n",
       "el_neg_chi_Average     10407.0     2.607662     0.334906  0.0    1.790000   \n",
       "R_vdw_element_Average  10407.0     1.731330     0.192481  0.0    1.318667   \n",
       "R_cov_element_Average  10407.0     0.944132     0.180017  0.0    0.505333   \n",
       "zaratio_Average        10407.0     0.493349     0.063080  0.0    0.405373   \n",
       "density_Average        10407.0     2.132984     1.936656  0.0    0.132734   \n",
       "Hardness               10407.0     4.647126     1.680525  1.0    1.500000   \n",
       "\n",
       "                               50%           99%           max  \n",
       "id                     5203.000000  10301.940000  10406.000000  \n",
       "allelectrons_Total      100.000000    719.400000  15300.000000  \n",
       "density_Total            10.650000     75.098979    643.093804  \n",
       "allelectrons_Average     12.600000     50.000000     67.000000  \n",
       "val_e_Average             4.714286      5.666667      6.000000  \n",
       "atomicweight_Average     26.203827    119.629500    167.400000  \n",
       "ionenergy_Average        11.202760     13.512520     15.245810  \n",
       "el_neg_chi_Average        2.706000      2.980000      3.443000  \n",
       "R_vdw_element_Average     1.732727      2.055000      2.250000  \n",
       "R_cov_element_Average     0.915556      1.390000      1.615840  \n",
       "zaratio_Average           0.488550      0.707253      0.825990  \n",
       "density_Average           1.351550      7.986670     10.970000  \n",
       "Hardness                  5.500000      8.288000     10.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe([0.01, 0.99]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TARGET = 'Hardness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['allelectrons_Total', 'density_Total', 'allelectrons_Average',\n",
      "       'val_e_Average', 'atomicweight_Average', 'ionenergy_Average',\n",
      "       'el_neg_chi_Average', 'R_vdw_element_Average', 'R_cov_element_Average',\n",
      "       'zaratio_Average', 'density_Average'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Selecting numerical variables from the train dataset, excluding 'id' and TARGET\n",
    "num_var = train.drop(['id', TARGET], axis=1).select_dtypes(include=np.number).columns\n",
    "print(num_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining train and test datasets for comparative analysis\n",
    "# 'Source' column is added to label data from each dataset\n",
    "df = pd.concat([\n",
    "    train[num_var].assign(Source='Train'), \n",
    "    test[num_var].assign(Source='Test')\n",
    "], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature name: allelectrons_Total\n",
      "Low Limit: -1064.1000000000076\n",
      "Upper Limit: 1789.5000000000127\n",
      "\n",
      "Feature name: density_Total\n",
      "Low Limit: -109.70094011562419\n",
      "Upper Limit: 184.80760706937366\n",
      "\n",
      "Feature name: allelectrons_Average\n",
      "Low Limit: -63.333333333333336\n",
      "Upper Limit: 118.0\n",
      "\n",
      "Feature name: val_e_Average\n",
      "Low Limit: -3.5000000000000018\n",
      "Upper Limit: 11.16666666666667\n",
      "\n",
      "Feature name: atomicweight_Average\n",
      "Low Limit: -157.51118333333332\n",
      "Upper Limit: 285.91391\n",
      "\n",
      "Feature name: ionenergy_Average\n",
      "Low Limit: -0.1337799999999998\n",
      "Upper Limit: 21.7003\n",
      "\n",
      "Feature name: el_neg_chi_Average\n",
      "Low Limit: 0.0050000000000001155\n",
      "Upper Limit: 4.765\n",
      "\n",
      "Feature name: R_vdw_element_Average\n",
      "Low Limit: 0.37229797979797974\n",
      "Upper Limit: 3.0646212121212124\n",
      "\n",
      "Feature name: R_cov_element_Average\n",
      "Low Limit: -0.6014419504643963\n",
      "Upper Limit: 2.5848651702786376\n",
      "\n",
      "Feature name: zaratio_Average\n",
      "Low Limit: 0.005794696969696755\n",
      "Upper Limit: 1.1281555050505054\n",
      "\n",
      "Feature name: density_Average\n",
      "Low Limit: -11.638980000000002\n",
      "Upper Limit: 19.76206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers\n",
    "def outlier_thresholds(dataframe, col_name, q1=0.01, q3=0.99):\n",
    "    quartile1 = dataframe[col_name].quantile(q1)\n",
    "    quartile3 = dataframe[col_name].quantile(q3)\n",
    "    IQR = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * IQR\n",
    "    low_limit = quartile1 - 1.5 * IQR\n",
    "    print(f'Feature name: {col_name}')\n",
    "    print(f'Low Limit: {low_limit}')\n",
    "    print(f'Upper Limit: {up_limit}')\n",
    "    print()\n",
    "    return low_limit, up_limit\n",
    "\n",
    "\n",
    "def remove_outlier(dataframe, col_name):\n",
    "    \"\"\"\n",
    "    Example Usage:\n",
    "    for col in num_cols:\n",
    "    new_df = remove_outlier(titanic, col)\n",
    "    \"\"\"\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
    "    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]    \n",
    "    return df_without_outliers\n",
    "\n",
    "df_outliers = train.copy()\n",
    "for col in num_var:\n",
    "    df_outliers = remove_outlier(df_outliers, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>5201.192420</td>\n",
       "      <td>3005.129652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.150000</td>\n",
       "      <td>5201.500000</td>\n",
       "      <td>10300.850000</td>\n",
       "      <td>10406.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>124.139079</td>\n",
       "      <td>109.999695</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>1266.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density_Total</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>14.469886</td>\n",
       "      <td>14.249258</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.973995</td>\n",
       "      <td>10.803992</td>\n",
       "      <td>73.958979</td>\n",
       "      <td>178.74200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>17.161600</td>\n",
       "      <td>10.400841</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>67.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_e_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>4.578703</td>\n",
       "      <td>0.580486</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>6.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>37.792042</td>\n",
       "      <td>25.897682</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10.895366</td>\n",
       "      <td>26.203827</td>\n",
       "      <td>119.629500</td>\n",
       "      <td>167.40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>11.017059</td>\n",
       "      <td>1.058323</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>8.213150</td>\n",
       "      <td>11.217767</td>\n",
       "      <td>13.512520</td>\n",
       "      <td>15.24581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>2.625121</td>\n",
       "      <td>0.260140</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>2.706000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>3.44300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>1.742674</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>1.028000</td>\n",
       "      <td>1.385714</td>\n",
       "      <td>1.733958</td>\n",
       "      <td>2.055000</td>\n",
       "      <td>2.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>0.951067</td>\n",
       "      <td>0.161130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.612174</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>1.61584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaratio_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>0.496283</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>0.401635</td>\n",
       "      <td>0.426680</td>\n",
       "      <td>0.488550</td>\n",
       "      <td>0.707270</td>\n",
       "      <td>0.82599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density_Average</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>2.146405</td>\n",
       "      <td>1.938989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136410</td>\n",
       "      <td>1.363050</td>\n",
       "      <td>7.986670</td>\n",
       "      <td>10.97000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hardness</th>\n",
       "      <td>10316.0</td>\n",
       "      <td>4.662606</td>\n",
       "      <td>1.671795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>8.470000</td>\n",
       "      <td>10.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count         mean          std       min  \\\n",
       "id                     10316.0  5201.192420  3005.129652  0.000000   \n",
       "allelectrons_Total     10316.0   124.139079   109.999695  0.001000   \n",
       "density_Total          10316.0    14.469886    14.249258  0.001000   \n",
       "allelectrons_Average   10316.0    17.161600    10.400841  0.001000   \n",
       "val_e_Average          10316.0     4.578703     0.580486  1.333333   \n",
       "atomicweight_Average   10316.0    37.792042    25.897682  0.001000   \n",
       "ionenergy_Average      10316.0    11.017059     1.058323  0.000167   \n",
       "el_neg_chi_Average     10316.0     2.625121     0.260140  1.666667   \n",
       "R_vdw_element_Average  10316.0     1.742674     0.131928  1.028000   \n",
       "R_cov_element_Average  10316.0     0.951067     0.161130  0.000000   \n",
       "zaratio_Average        10316.0     0.496283     0.050363  0.401635   \n",
       "density_Average        10316.0     2.146405     1.938989  0.000000   \n",
       "Hardness               10316.0     4.662606     1.671795  1.000000   \n",
       "\n",
       "                               1%          50%           99%          max  \n",
       "id                     106.150000  5201.500000  10300.850000  10406.00000  \n",
       "allelectrons_Total      20.000000   100.000000    622.000000   1266.00000  \n",
       "density_Total            0.973995    10.803992     73.958979    178.74200  \n",
       "allelectrons_Average     5.520000    12.600000     50.000000     67.00000  \n",
       "val_e_Average            2.666667     4.750000      5.666667      6.00000  \n",
       "atomicweight_Average    10.895366    26.203827    119.629500    167.40000  \n",
       "ionenergy_Average        8.213150    11.217767     13.512520     15.24581  \n",
       "el_neg_chi_Average       1.950000     2.706000      2.980000      3.44300  \n",
       "R_vdw_element_Average    1.385714     1.733958      2.055000      2.25000  \n",
       "R_cov_element_Average    0.612174     0.918000      1.390000      1.61584  \n",
       "zaratio_Average          0.426680     0.488550      0.707270      0.82599  \n",
       "density_Average          0.136410     1.363050      7.986670     10.97000  \n",
       "Hardness                 1.500000     5.500000      8.470000     10.00000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outliers.describe([0.01, 0.99]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No missing data in the both train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import median_absolute_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, PassiveAggressiveRegressor, SGDRegressor, Perceptron, LinearRegression, TheilSenRegressor, HuberRegressor, RANSACRegressor, Lasso, ElasticNet, Lars, LassoLars, OrthogonalMatchingPursuit, BayesianRidge, ARDRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "MLA = [\n",
    "\t# Trial Models\n",
    "\tMLPRegressor(random_state=5),\n",
    "\tTheilSenRegressor(random_state=5),\n",
    "\tHuberRegressor(),\n",
    "\tRANSACRegressor(random_state=5),\n",
    "\tLasso(random_state=5),\n",
    "\tElasticNet(random_state=5),\n",
    "\tLars(random_state=5),\n",
    "\tLassoLars(random_state=5),\n",
    "\tOrthogonalMatchingPursuit(),\n",
    "\tBayesianRidge(),\n",
    "\tARDRegression(),\n",
    "\n",
    "\t# GLM\n",
    "\tLinearRegression(),\n",
    "\tPassiveAggressiveRegressor(random_state=5),\n",
    "\tRidgeCV(),\n",
    "\t# SGDRegressor(),\n",
    "\n",
    "\t# SVM\n",
    "\tsvm.SVR(),\n",
    "\tsvm.NuSVR(),\n",
    "\n",
    "\t# Trees    \n",
    "\tDecisionTreeRegressor(random_state=5),\n",
    "\tExtraTreeRegressor(random_state=5),\n",
    "\n",
    "\tXGBRegressor(random_state=5),\n",
    "\tLGBMRegressor(n_jobs=-1, random_state=5),\n",
    "\tCatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100),\n",
    "\t\n",
    "\t# KNeighbors\n",
    "\tKNeighborsRegressor(),\n",
    "\tKNeighborsRegressor(n_neighbors=2),\n",
    "\tKNeighborsRegressor(n_neighbors=4),\n",
    "\tKNeighborsRegressor(n_neighbors=8),\n",
    "\tKNeighborsRegressor(n_neighbors=16),\n",
    "\tKNeighborsRegressor(n_neighbors=32),\n",
    "\tKNeighborsRegressor(n_neighbors=64),\n",
    "\tKNeighborsRegressor(n_neighbors=128),\n",
    "\tKNeighborsRegressor(n_neighbors=256),\n",
    "\tKNeighborsRegressor(n_neighbors=512),\n",
    "\tKNeighborsRegressor(n_neighbors=1024),\n",
    "\n",
    "\t# Ensemble Methods\n",
    "\tAdaBoostRegressor(random_state=5),\n",
    "\tBaggingRegressor(random_state=5),\n",
    "\tExtraTreesRegressor(random_state=5),\n",
    "\tGradientBoostingRegressor(random_state=5),\n",
    "\tHistGradientBoostingRegressor(random_state=5),\n",
    "\tRandomForestRegressor(random_state=5),\n",
    "    ]\n",
    "\n",
    "\n",
    "# split dataset in cross-validation with splitter class\n",
    "# cv_split could KFold, StratifiedKFold or RepeatedKFold depending on the problem\n",
    "cv_split = KFold(n_splits=10, shuffle=True, random_state=5)\n",
    "cv_split_trial = KFold(n_splits=2, shuffle=True, random_state=5) # For quick trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "# create table to compare MLA predictions\n",
    "MLA_predict = {}\n",
    "\n",
    "# index through MLA and save performance to table\n",
    "row_index = 0\n",
    "scoring = median_abs_error_scorer = make_scorer(median_absolute_error, greater_is_better=False)\n",
    "\n",
    "for alg in MLA:\n",
    "\n",
    "\t# set name and parameters\n",
    "\tMLA_name = alg.__class__.__name__\n",
    "\n",
    "\t# Add suffix if name already exists\n",
    "\tsuffix = 1\n",
    "\toriginal_MLA_name = MLA_name\n",
    "\twhile MLA_compare['MLA Name'].str.contains(MLA_name).any():\n",
    "\t\tMLA_name = f\"{original_MLA_name}_{suffix}\"\n",
    "\t\tsuffix += 1\n",
    "\t\t\n",
    "\tMLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "\tMLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "\n",
    "\t\"\"\"score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\"\"\"\n",
    "\n",
    "\tcv_results = cross_validate(alg, df_outliers[num_var], df_outliers[TARGET], cv=cv_split, scoring=scoring, return_train_score=True)\n",
    "\n",
    "\t# Calculate mean time in seconds\n",
    "\tmean_fit_time = cv_results['fit_time'].mean()\n",
    "\n",
    "\t# Convert mean time to minutes and seconds\n",
    "\tminutes = int(mean_fit_time // 60)\n",
    "\tseconds = mean_fit_time % 60\n",
    "\n",
    "\t# Format the time and assign it\n",
    "\tMLA_compare.loc[row_index, 'MLA Time'] = f\"{minutes} min {seconds:.2f} sec\"\n",
    "\tMLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() * -1\n",
    "\tMLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() * -1\n",
    "\t#if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "\tMLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "\n",
    "\n",
    "\t# # #save MLA predictions - see section 6 for usage\n",
    "\t# alg.fit(data1[data1_x_bin], data1[Target])\n",
    "\t# MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "\tprint(f'Done with {MLA_name}')\n",
    "\trow_index+=1\n",
    "\n",
    "\n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = True, inplace = True)\n",
    "MLA_compare.to_csv(f'{experiment}_results.csv', index=False)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, y_test, predictions):\n",
    "    weighted_predictions = np.zeros_like(predictions[0])\n",
    "    for i in range(len(MLA)):\n",
    "        # Assign a weight to each prediction\n",
    "        weight = trial.suggest_float(f'w{i}', 0, 1)\n",
    "        weighted_predictions += weight * predictions[i]\n",
    "    weighted_predictions /= np.sum(weighted_predictions)  # Normalize weights\n",
    "\n",
    "    # Calculate metric on your validation set\n",
    "    mae = median_absolute_error(y_test, weighted_predictions)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_objective(trial, y_true, model_predictions):\n",
    "    # The model_predictions is a list of prediction arrays from the three models\n",
    "    weighted_predictions = np.zeros_like(model_predictions[0])\n",
    "    total_weight = 0\n",
    "\n",
    "    for i in range(len(model_predictions)):\n",
    "        # Define the weight for each model\n",
    "        weight = trial.suggest_float(f'w{i}', 0, 1)\n",
    "        total_weight += weight\n",
    "        weighted_predictions += weight * model_predictions[i]\n",
    "\n",
    "    # Normalize the predictions by the total weight\n",
    "    if total_weight > 0:\n",
    "        weighted_predictions /= total_weight\n",
    "\n",
    "    # Calculate the median absolute error\n",
    "    return median_absolute_error(y_true, weighted_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hill Climbing inspired by code from Kaggle\n",
    "def hill_climbing(x, y):\n",
    "    \n",
    "    # Evaluating oof predictions\n",
    "    scores = {}\n",
    "    for col in x.columns:\n",
    "        scores[col] = median_absolute_error(y, x[col])\n",
    "\n",
    "    # Sorting the model scores\n",
    "    scores = {k: v for k, v in sorted(scores.items(), key = lambda item: item[1], reverse = True)}\n",
    "\n",
    "    # Sort oof_df\n",
    "    x = x[list(scores.keys())]\n",
    "\n",
    "    STOP = False\n",
    "    current_best_ensemble = x.iloc[:,0]\n",
    "    MODELS = x.iloc[:,1:]\n",
    "    weight_range = np.arange(-0.5, 0.51, 0.01) \n",
    "    history = [median_absolute_error(y, current_best_ensemble)]\n",
    "    j = 0\n",
    "\n",
    "    while not STOP:\n",
    "        j += 1\n",
    "        potential_new_best_cv_score = median_absolute_error(y, current_best_ensemble)\n",
    "        k_best, wgt_best = None, None\n",
    "        for k in MODELS:\n",
    "            for wgt in weight_range:\n",
    "                potential_ensemble = (1 - wgt) * current_best_ensemble + wgt * MODELS[k]\n",
    "                cv_score = median_absolute_error(y, potential_ensemble)\n",
    "                if cv_score < potential_new_best_cv_score:\n",
    "                    potential_new_best_cv_score = cv_score\n",
    "                    k_best, wgt_best = k, wgt\n",
    "\n",
    "        if k_best is not None:\n",
    "            current_best_ensemble = (1 - wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n",
    "            MODELS.drop(k_best, axis = 1, inplace = True)\n",
    "            if MODELS.shape[1] == 0:\n",
    "                STOP = True\n",
    "            history.append(potential_new_best_cv_score)\n",
    "        else:\n",
    "            STOP = True\n",
    "        \n",
    "    hill_ens_pred = current_best_ensemble\n",
    "    \n",
    "    return hill_ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_predictions(predictions, lower_limit=1, upper_limit=10):\n",
    "    return np.clip(predictions, lower_limit, upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "The Fold 1 average prediction is 0.8441366920463249\n",
      "The Fold 1 Optuna weights is 0.8469353890249725\n",
      "The Fold 1 Hill Climb is 0.5877540464090947\n",
      "The Fold 1 2-Level Stacked Score is 0.7987952131618559\n",
      "The Fold 1 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6942130183041793\n",
      "\n",
      "Fold 2\n",
      "The Fold 2 average prediction is 0.8504406691382167\n",
      "The Fold 2 Optuna weights is 0.8408329938909447\n",
      "The Fold 2 Hill Climb is 0.5686486882945305\n",
      "The Fold 2 2-Level Stacked Score is 0.7998823324051532\n",
      "The Fold 2 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6910687454182256\n",
      "\n",
      "Fold 3\n",
      "The Fold 3 average prediction is 0.836882511748072\n",
      "The Fold 3 Optuna weights is 0.8043339872725526\n",
      "The Fold 3 Hill Climb is 0.5588023697091666\n",
      "The Fold 3 2-Level Stacked Score is 0.7987329806873265\n",
      "The Fold 3 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.7104794244205508\n",
      "\n",
      "Fold 4\n",
      "The Fold 4 average prediction is 0.8010418924445317\n",
      "The Fold 4 Optuna weights is 0.751257982639363\n",
      "The Fold 4 Hill Climb is 0.585222071065961\n",
      "The Fold 4 2-Level Stacked Score is 0.6990433027728433\n",
      "The Fold 4 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6004339644737682\n",
      "\n",
      "Fold 5\n",
      "The Fold 5 average prediction is 0.8588988509811317\n",
      "The Fold 5 Optuna weights is 0.7965057878239816\n",
      "The Fold 5 Hill Climb is 0.5497252559548598\n",
      "The Fold 5 2-Level Stacked Score is 0.7993352097949897\n",
      "The Fold 5 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6695720025066316\n",
      "\n",
      "Fold 6\n",
      "The Fold 6 average prediction is 0.8272636635917583\n",
      "The Fold 6 Optuna weights is 0.8232683972016237\n",
      "The Fold 6 Hill Climb is 0.6375228222038916\n",
      "The Fold 6 2-Level Stacked Score is 0.799470551695834\n",
      "The Fold 6 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6953394631211314\n",
      "\n",
      "Fold 7\n",
      "The Fold 7 average prediction is 0.8536785344675755\n",
      "The Fold 7 Optuna weights is 0.784277617666068\n",
      "The Fold 7 Hill Climb is 0.5666620863231882\n",
      "The Fold 7 2-Level Stacked Score is 0.7004485081818865\n",
      "The Fold 7 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6296314727906509\n",
      "\n",
      "Fold 8\n",
      "The Fold 8 average prediction is 0.8172490210224179\n",
      "The Fold 8 Optuna weights is 0.8244160161261593\n",
      "The Fold 8 Hill Climb is 0.573496667525335\n",
      "The Fold 8 2-Level Stacked Score is 0.7999523120444518\n",
      "The Fold 8 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6737831564217442\n",
      "\n",
      "Fold 9\n",
      "The Fold 9 average prediction is 0.8580299825575892\n",
      "The Fold 9 Optuna weights is 0.8090557745813145\n",
      "The Fold 9 Hill Climb is 0.6177758111966378\n",
      "The Fold 9 2-Level Stacked Score is 0.7999448713566801\n",
      "The Fold 9 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.7197971996292454\n",
      "\n",
      "Fold 10\n",
      "The Fold 10 average prediction is 0.8247839443995071\n",
      "The Fold 10 Optuna weights is 0.824157283172462\n",
      "The Fold 10 Hill Climb is 0.5542863926527248\n",
      "The Fold 10 2-Level Stacked Score is 0.7995981857351273\n",
      "The Fold 10 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: 0.6356460541972977\n",
      "\n",
      "\n",
      "The average prediction CV score is ==> 0.8372405762397126\n",
      "The Optuna weights CV score is ==> 0.8105041229399441\n",
      "The Hill Climbing CV score is ==> 0.579989621133539\n",
      "The 2-Level Stacking CV score is ==> 0.7795203467836148\n",
      "The 2-Level Multi-Model Stack with 3-Level Weighted Average CV score is ==> 0.6719964501283425\n"
     ]
    }
   ],
   "source": [
    "avg_predictions_scores = []\n",
    "optuna_weights_scores = []\n",
    "hill_climb_scores = []\n",
    "stacked_scores = []\n",
    "optuna_weights_scores_stack = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv_split.split(df_outliers[num_var], df_outliers[TARGET])):\n",
    "    X_train, X_test = df_outliers[num_var].iloc[train_index], df_outliers[num_var].iloc[test_index]\n",
    "    y_train, y_test = df_outliers[TARGET].iloc[train_index], df_outliers[TARGET].iloc[test_index]\n",
    "\n",
    "    print(f'Fold {i+1}')\n",
    "    \n",
    "    MLA_cv_train_preds = []\n",
    "    MLA_cv_preds = []\n",
    "    MLA_cv_preds_dict = {}\n",
    "    \n",
    "    for alg in MLA:\n",
    "        MLA_name = alg.__class__.__name__\n",
    "\n",
    "        # Add suffix if name already exists\n",
    "        suffix = 1\n",
    "        original_MLA_name = MLA_name\n",
    "        if MLA_name in MLA_cv_preds:\n",
    "        # while MLA_cv_preds.str.contains(MLA_name).any():\n",
    "            MLA_name = f\"{original_MLA_name}_{suffix}\"\n",
    "            suffix += 1\n",
    "            \n",
    "        predictor = alg.fit(X_train, y_train)\n",
    "        pred_train_result = predictor.predict(X_train)\n",
    "        pred_result = predictor.predict(X_test)\n",
    "\n",
    "        # # Cap the predictions so Hardness is between 1 and 10\n",
    "        # pred_train_result = cap_predictions(pred_train_result)\n",
    "        # pred_result = cap_predictions(pred_result)\n",
    "\n",
    "        MLA_cv_train_preds.append(pred_train_result)\n",
    "        MLA_cv_preds.append(pred_result)\n",
    "        MLA_cv_preds_dict[MLA_name] = pred_result\n",
    "\n",
    "    #################\n",
    "    ### Averaging ###\n",
    "    #################\n",
    "    avg_prediction = np.mean(MLA_cv_preds, axis=0)\n",
    "    avg_prediction = cap_predictions(avg_prediction)\n",
    "    avg_prediction_score = median_absolute_error(y_test, avg_prediction)\n",
    "    avg_predictions_scores.append(avg_prediction_score)\n",
    "    print(f'The Fold {i+1} average prediction is {avg_prediction_score}')\n",
    "\n",
    "    ##############\n",
    "    ### Optuna ###\n",
    "    ##############\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, y_test, MLA_cv_preds), n_trials=200)\n",
    "\n",
    "    # Use the best weights\n",
    "    best_weights = [study.best_params[f'w{i}'] for i in range(len(MLA))]\n",
    "    weighted_avg_predictions = np.average(MLA_cv_preds, axis=0, weights=best_weights)\n",
    "    weighted_avg_predictions = cap_predictions(weighted_avg_predictions)\n",
    "    weighted_avg_predictions_score = median_absolute_error(y_test, weighted_avg_predictions)\n",
    "    optuna_weights_scores.append(weighted_avg_predictions_score)\n",
    "    print(f'The Fold {i+1} Optuna weights is {weighted_avg_predictions_score}')\n",
    "\n",
    "    ##################\n",
    "    ### Hill Climb ###\n",
    "    ##################\n",
    "    hill_climb_pred = hill_climbing(pd.DataFrame(MLA_cv_preds_dict), y_test)\n",
    "    hill_climb_pred = cap_predictions(hill_climb_pred)\n",
    "    hill_climb_score = median_absolute_error(y_test, hill_climb_pred)\n",
    "    hill_climb_scores.append(hill_climb_score)\n",
    "    print(f'The Fold {i+1} Hill Climb is {hill_climb_score}')\n",
    "\n",
    "    ###############################\n",
    "    ### Simple 2-Level Stacking ###\n",
    "    ###############################\n",
    "    stacked_features = np.column_stack(MLA_cv_train_preds)\n",
    "    stacked_test = np.column_stack(MLA_cv_preds)\n",
    "\n",
    "    meta_model = LGBMRegressor(n_jobs=-1, random_state=5)\n",
    "    meta_model.fit(stacked_features, y_train)\n",
    "    stacked_prediction = meta_model.predict(stacked_test)\n",
    "    stacked_prediction = cap_predictions(stacked_prediction)\n",
    "    stacked_score = median_absolute_error(y_test, stacked_prediction)\n",
    "    stacked_scores.append(stacked_score)\n",
    "    print(f'The Fold {i+1} 2-Level Stacked Score is {stacked_score}')\n",
    "\n",
    "    ###############################################################\n",
    "    ### 2-Level Multi-Model Stack with 3-Level Weighted Average ###\n",
    "    ###############################################################\n",
    "    # LGBM\n",
    "    meta_model_lgbm = LGBMRegressor(n_jobs=-1, random_state=5)\n",
    "    meta_model_lgbm.fit(stacked_features, y_train)\n",
    "    stacked_prediction_lgbm = meta_model_lgbm.predict(stacked_test)\n",
    "    stacked_prediction_lgbm = cap_predictions(stacked_prediction_lgbm)\n",
    "    stacked_score_lgbm = median_absolute_error(y_test, stacked_prediction_lgbm)\n",
    "\n",
    "    # XGBRegressor(seed=5)\n",
    "    meta_model_xgb = XGBRegressor(seed=5)\n",
    "    meta_model_xgb.fit(stacked_features, y_train)\n",
    "    stacked_prediction_xgb = meta_model_xgb.predict(stacked_test)\n",
    "    stacked_prediction_xgb = cap_predictions(stacked_prediction_xgb)\n",
    "    stacked_score_xgb = median_absolute_error(y_test, stacked_prediction_xgb)\n",
    "\n",
    "    # CatBoost\n",
    "    meta_model_cat = CatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "    meta_model_cat.fit(stacked_features, y_train)\n",
    "    stacked_prediction_cat = meta_model_cat.predict(stacked_test)\n",
    "    stacked_prediction_cat = cap_predictions(stacked_prediction_cat)\n",
    "    stacked_score_cat = median_absolute_error(y_test, stacked_prediction_cat)\n",
    "    \n",
    "    # Stack the predictions from your three models\n",
    "    second_level_predictions = [\n",
    "        stacked_prediction_lgbm,\n",
    "        stacked_prediction_xgb,\n",
    "        stacked_prediction_cat\n",
    "    ]\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: weighted_avg_objective(trial, y_test, second_level_predictions), n_trials=200)\n",
    "\n",
    "    best_weights = [study.best_params[f'w{i}'] for i in range(len(second_level_predictions))]\n",
    "    weighted_avg_prediction = np.average(second_level_predictions, axis=0, weights=best_weights)\n",
    "    weighted_avg_prediction = cap_predictions(weighted_avg_prediction)\n",
    "    weighted_avg_score = median_absolute_error(y_test, weighted_avg_prediction)\n",
    "    print(f\"The Fold {i+1} 2-Level Multi-Model Stack with 3-Level Weighted Average Prediction Score: {weighted_avg_score}\")\n",
    "    optuna_weights_scores_stack.append(weighted_avg_score)\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(f'The average prediction CV score is ==> {np.mean(avg_predictions_scores)}')\n",
    "print(f'The Optuna weights CV score is ==> {np.mean(optuna_weights_scores)}')\n",
    "print(f'The Hill Climbing CV score is ==> {np.mean(hill_climb_scores)}')\n",
    "print(f'The 2-Level Stacking CV score is ==> {np.mean(stacked_scores)}')\n",
    "print(f'The 2-Level Multi-Model Stack with 3-Level Weighted Average CV score is ==> {np.mean(optuna_weights_scores_stack)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
