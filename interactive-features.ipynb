{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, StratifiedKFold\nfrom catboost import CatBoostClassifier\n# from autogluon.tabular import TabularDataset, TabularPredictor\nfrom sklearn.metrics import f1_score\nfrom category_encoders import CountEncoder, TargetEncoder\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\nimport warnings\n\nfrom sklearn.compose import make_column_transformer\n\nmodel_number = 'interactive_numerical_only'\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:26:17.949434Z","iopub.execute_input":"2023-09-20T18:26:17.949805Z","iopub.status.idle":"2023-09-20T18:26:17.956802Z","shell.execute_reply.started":"2023-09-20T18:26:17.949774Z","shell.execute_reply":"2023-09-20T18:26:17.955570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/playground-series-s3e22/train.csv')\ndf_test = pd.read_csv('../input/playground-series-s3e22/test.csv')\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:25:26.120555Z","iopub.execute_input":"2023-09-20T18:25:26.121003Z","iopub.status.idle":"2023-09-20T18:25:26.192137Z","shell.execute_reply.started":"2023-09-20T18:25:26.120977Z","shell.execute_reply":"2023-09-20T18:25:26.190804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropped_columns = ['id']\n\ndf_train_model = df_train.drop(dropped_columns, axis=1)\ndf_test_model = df_test.drop(dropped_columns, axis=1)\n\n# Assign train and test for X and y\nX_train = df_train_model.drop('outcome', axis=1)\ny_train = df_train_model['outcome']\n\nX_test = df_test_model.copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:25:28.154824Z","iopub.execute_input":"2023-09-20T18:25:28.155197Z","iopub.status.idle":"2023-09-20T18:25:28.171736Z","shell.execute_reply.started":"2023-09-20T18:25:28.155163Z","shell.execute_reply":"2023-09-20T18:25:28.170427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:25:29.468123Z","iopub.execute_input":"2023-09-20T18:25:29.468500Z","iopub.status.idle":"2023-09-20T18:25:29.476997Z","shell.execute_reply.started":"2023-09-20T18:25:29.468473Z","shell.execute_reply":"2023-09-20T18:25:29.475809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET = \"outcome\"\n\ncombined_df = pd.concat([X_train, X_test])\ncombined_df.shape, X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:25:30.602391Z","iopub.execute_input":"2023-09-20T18:25:30.602732Z","iopub.status.idle":"2023-09-20T18:25:30.615120Z","shell.execute_reply.started":"2023-09-20T18:25:30.602706Z","shell.execute_reply":"2023-09-20T18:25:30.614099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:25:31.511964Z","iopub.execute_input":"2023-09-20T18:25:31.513466Z","iopub.status.idle":"2023-09-20T18:25:31.541489Z","shell.execute_reply.started":"2023-09-20T18:25:31.513414Z","shell.execute_reply":"2023-09-20T18:25:31.540588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_cat_features = list(X_train.select_dtypes('object').columns)\nnumerical_features = list(X_train.select_dtypes(include=['int', 'float']).columns)\n\nnum_cat_features = ['lesion_3', 'lesion_2', 'hospital_number']\n\ncat_features = object_cat_features + num_cat_features\nnum_features = [feat for feat in numerical_features if feat not in num_cat_features]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:25:34.958780Z","iopub.execute_input":"2023-09-20T18:25:34.959132Z","iopub.status.idle":"2023-09-20T18:25:34.965029Z","shell.execute_reply.started":"2023-09-20T18:25:34.959104Z","shell.execute_reply":"2023-09-20T18:25:34.964218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in cat_features:\n#     X_train[col] = X_train[col].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = make_column_transformer(\n        (OneHotEncoder(sparse=False, handle_unknown='ignore'), object_cat_features),\n        remainder='passthrough')\n\ntransformed = transformer.fit_transform(X_train)\nfeature_names = [name.split('__')[-1] for name in transformer.get_feature_names_out()]\ntransformed_df = pd.DataFrame(transformed, columns=feature_names)\ntransformed_df.sample(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:26:22.090906Z","iopub.execute_input":"2023-09-20T18:26:22.091315Z","iopub.status.idle":"2023-09-20T18:26:22.171924Z","shell.execute_reply.started":"2023-09-20T18:26:22.091287Z","shell.execute_reply":"2023-09-20T18:26:22.170643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_transformed = transformer.transform(X_test)\ntransformed_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\ntransformed_test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:26:34.752600Z","iopub.execute_input":"2023-09-20T18:26:34.752917Z","iopub.status.idle":"2023-09-20T18:26:34.840054Z","shell.execute_reply.started":"2023-09-20T18:26:34.752893Z","shell.execute_reply":"2023-09-20T18:26:34.839293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:32:23.392560Z","iopub.execute_input":"2023-09-20T18:32:23.393589Z","iopub.status.idle":"2023-09-20T18:32:23.399535Z","shell.execute_reply.started":"2023-09-20T18:32:23.393558Z","shell.execute_reply":"2023-09-20T18:32:23.398515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Replace 'None' values with the median of respective columns\n# for col in cat_features:\n#     # Calculate the median excluding 'None' values\n#     mode_value = X_train[col][X_train[col] != 'none'].mode().iloc[0]\n    \n#     # Replace 'None' values with the median\n#     X_train[col] = X_train[col].replace('none', mode_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_polynomial_features(df, degree, df_features):\n    \"\"\"\n    Generate polynomial features for the specified columns in a DataFrame.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        The DataFrame containing the original features.\n    degree : int\n        The degree of the polynomial features to generate.\n    df_features : list\n        A list of feature names to be used for generating polynomial features.\n\n    Returns:\n    --------\n    pandas.DataFrame\n        The DataFrame with the polynomial features added.\n    \"\"\"\n    # Get the list of features to create polynomial features\n    features = [col for col in df.columns if col in df_features]\n\n    # Create a PolynomialFeatures object with the specified degree, no interaction features, and no bias term\n    poly = PolynomialFeatures(degree, interaction_only=False, include_bias=False)\n\n    # Fit and transform the selected features in the DataFrame\n    poly_features = poly.fit_transform(df[features])\n\n    # Get the feature names for the generated polynomial features\n    poly_features_names = poly.get_feature_names_out(features)\n\n    # Create a new DataFrame with the generated polynomial features\n    poly_df = pd.DataFrame(poly_features, columns=poly_features_names)\n\n    # Keep only the columns with polynomial features of the specified degree\n    poly_df = poly_df[[f\"{col}^{degree}\" for col in features]]\n\n    # Concatenate the original DataFrame and the polynomial features DataFrame\n    result_combined = pd.concat([df, poly_df], axis=1)\n\n    return df, poly_df, result_combined","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_domain_features(df, df_features):\n    \"\"\"\n    Generate domain-specific features as ratios between the given columns in a DataFrame.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        The DataFrame containing the original features.\n    df_features : list\n        A list of feature names to be used for generating domain-specific features.\n\n    Returns:\n    --------\n    pandas.DataFrame\n        The DataFrame with the domain-specific features added.\n    \"\"\"\n    # Get the list of features to create domain-specific features\n    features = [col for col in df.columns if col in df_features]\n    new_features = []\n    \n    df_new_features = pd.DataFrame()\n\n    # Iterate through the features and create domain-specific features as ratios\n    for i in range(len(features)):\n        for j in range(len(features)):\n            # Check if the features are different\n            if i != j:\n                # Generate a new feature name for the domain-specific feature\n                new_feature_name = f\"{features[i]}_{features[j]}_ratio\"\n                \n                # Create the domain-specific feature by dividing the values of the two original features\n                # If the denominator is 0, use a small value (1e-6) to avoid division by zero\n                df_new_features[new_feature_name] = df[features[i]] / np.where(df[features[j]] == 0, 1e-6, df[features[j]])\n                \n                # Add the new feature name to the list of new features\n                new_features.append(new_feature_name)\n    \n    df_combined = pd.concat([df, df_new_features], axis=1)\n\n    return df, df_new_features, df_combined\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_trains, df_interactive, X_train_complete = generate_polynomial_features(X_train, 2, num_features)\n# X_train_complete.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_trains.shape, df_interactive.shape, X_train_complete.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autogluon_data = pd.concat([X_train_complete, y_train], axis=1)\nautogluon_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactive_cat_features = list(X_train_complete.select_dtypes('object').columns)\nX_train_complete[interactive_cat_features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def catboost_kfold_feature_importance(X_train, y_train, cat_features=None, n_splits=5, random_state=5):\n    \"\"\"\n    Perform K-Fold cross-validation with CatBoost and calculate feature importances.\n\n    Args:\n    - X_train: DataFrame, training features.\n    - y_train: Series, training target.\n    - cat_features: List of categorical feature names (default is None).\n    - n_splits: Number of K-Fold splits (default is 5).\n    - random_state: Random seed for reproducibility (default is 5).\n\n    Returns:\n    - fi_df: DataFrame, feature importances with fold-wise and average values.\n    \"\"\"    \n    # Initialize DataFrame to store feature importances\n    fi_df = pd.DataFrame({'Feature': X_train.columns})\n\n    # Initialize K-Fold cross-validator\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    # Create empty array to store fold AUC scores\n    fold_scores = np.zeros(n_splits)\n\n    # Initialize CatBoost model\n    model = CatBoostClassifier(random_state=random_state, cat_features=cat_features, verbose=False)\n\n    # Perform K-Fold cross-validation\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n        X_train_fold, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_train_fold, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n        # Fit the CatBoost model\n        model.fit(X_train_fold, y_train_fold, eval_set=(X_val, y_val), verbose=100, early_stopping_rounds=100)\n\n        # Calculate fold AUC score\n        y_pred_val = model.predict(X_val)\n        fold_score = f1_score(y_val, y_pred_val, average='micro')\n        fold_scores[fold] = fold_score\n\n        # Record feature importances for this fold\n        feature_importance = model.get_feature_importance()\n        fi_df[f'Fold_{fold + 1}'] = feature_importance\n\n    # Calculate and append average feature importance\n    fi_df['Average'] = fi_df.iloc[:, 1:].mean(axis=1)\n\n    fi_df.to_csv('catboost_feature_importance.csv', index=False)\n\n    return fi_df","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:31:20.174906Z","iopub.execute_input":"2023-09-20T18:31:20.175317Z","iopub.status.idle":"2023-09-20T18:31:20.184851Z","shell.execute_reply.started":"2023-09-20T18:31:20.175289Z","shell.execute_reply":"2023-09-20T18:31:20.183362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_catboost_cat_feature_importance(X_train, y_train, cat_features, figsize=(16, 12)):\n    fi_df = catboost_kfold_feature_importance(X_train, y_train, cat_features=cat_features)\n    fi_df.sort_values(by='Average', ascending=False, inplace=True)\n\n    plt.figure(figsize=figsize)\n    sns.barplot(\n        x=fi_df['Average'],\n        y=fi_df['Feature'],\n    )\n\n    plt.title('Features Importance (avg over folds)')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:31:21.104468Z","iopub.execute_input":"2023-09-20T18:31:21.104784Z","iopub.status.idle":"2023-09-20T18:31:21.110958Z","shell.execute_reply.started":"2023-09-20T18:31:21.104761Z","shell.execute_reply":"2023-09-20T18:31:21.110191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_catboost_cat_feature_importance(transformed_df, y_train, cat_features=None, figsize=(32, 32))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:31:22.598309Z","iopub.execute_input":"2023-09-20T18:31:22.598688Z","iopub.status.idle":"2023-09-20T18:31:28.027379Z","shell.execute_reply.started":"2023-09-20T18:31:22.598660Z","shell.execute_reply":"2023-09-20T18:31:28.026156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = pd.read_csv('catboost_feature_importance.csv')\nfeats.sort_values(by='Average', ascending=False, inplace=True)\nfeats_needed = feats[feats['Average'] >= 20]['Feature'].to_list()\nfeats_needed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactive_cat_features = list(df_interactive[feats].select_dtypes('object').columns)\ninteractive_cat_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz.plot_autogluon_feature_importance(autogluon_data, TARGET, 60, figsize=(32, 32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# catboost_result = pd.read_csv('catboost_feature_importance.csv')\n# catboost_result = catboost_result.sort_values(by='Average', ascending=False)\n\n# catboost_features = X_train.columns\n\nmodel = CatBoostClassifier(random_state=5, verbose=False)\n\nrfecv = RFECV(estimator=model, cv=3, scoring='f1_micro', n_jobs=-1, verbose=1)\n\nrfecv.fit(df_interactive, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the RFECV ranking of all the features to select which ones to use\n# Rank 1 are the features best suited for modelling but rank 2 features aren't bad\n# And so on\nrfecv_features = pd.DataFrame({'Feature': df_interactive.columns,\n                               'Ranking': rfecv.ranking_})\n\nrfecv_features.sort_values(by='Ranking', inplace=True)\nrfecv_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = [i for i, selected in enumerate(rfecv.support_) if selected]\nselected_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a list of the rank 1 features\nrfecv_rank_1 = list(df_interactive.columns[selected_features])\nrfecv_rank_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize CatBoost model\n# model = CatBoostClassifier(random_state=5, cat_features=None, verbose=5)\nmodel = CatBoostClassifier(random_state=5, verbose=False)\n\nnum_folds = 5\ncv = RepeatedStratifiedKFold(n_splits=num_folds, n_repeats=3, random_state=5)\n\n# scores = cross_val_score(model, X_train.drop(dropped_columns_low_fi, axis=1), y_train, cv=cv, n_jobs=-1, scoring='roc_auc')\nscores = cross_val_score(model, transformed_df, y_train, cv=cv, n_jobs=-1, scoring='f1_micro')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:33:18.848899Z","iopub.execute_input":"2023-09-20T18:33:18.849326Z","iopub.status.idle":"2023-09-20T18:34:04.689812Z","shell.execute_reply.started":"2023-09-20T18:33:18.849296Z","shell.execute_reply":"2023-09-20T18:34:04.688750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:34:04.691786Z","iopub.execute_input":"2023-09-20T18:34:04.692222Z","iopub.status.idle":"2023-09-20T18:34:04.700464Z","shell.execute_reply.started":"2023-09-20T18:34:04.692184Z","shell.execute_reply":"2023-09-20T18:34:04.699121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the scores average and std for each fold\nprint(\"Scores Mean:\", np.mean(scores))\nprint(\"Scores Std:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T18:34:04.702121Z","iopub.execute_input":"2023-09-20T18:34:04.702468Z","iopub.status.idle":"2023-09-20T18:34:04.715327Z","shell.execute_reply.started":"2023-09-20T18:34:04.702444Z","shell.execute_reply":"2023-09-20T18:34:04.714062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on the entire training data\nmodel.fit(transformed_df, y_train)\n\n# Predict on the X_test data\npredictions = model.predict(transformed_test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_class = predictions.squeeze()\n\n# Create a DataFrame for submission\nsubmission_df = pd.DataFrame(\n    {'id': df_test['id'],\n    'outcome': predictions_class})\n\n# Save to CSV for submission\nsubmission_df.to_csv(f'submission_{model_number}_catboost.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfold_splits = md.generate_kfold(autogluon_data, y='outcome')\nkfold_splits = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=5)\n\nf1_scores = []\n\nfor fold, (train_index, test_index) in enumerate(kfold_splits.split(autogluon_data, autogluon_data['outcome'])):\n    # Split the dataset into train and test sets\n    train_data = autogluon_data.iloc[train_index]\n    test_data = autogluon_data.iloc[test_index]\n\n    # Print the shapes of train and test data for debugging\n    print(f\"Fold {fold + 1} - Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n\n    predictor = TabularPredictor(problem_type=\"multiclass\", label=\"outcome\", eval_metric='f1_micro')\n\n    predictor.fit(train_data=train_data,\n                presets=\"medium_quality\",\n                time_limit=60,\n    )\n\n    performance = predictor.evaluate(test_data)\n\n    print(f\"Fold {fold + 1} - F1 Score: {performance['f1_micro']}\")\n\n    f1_scores.append(performance['f1_micro'])\n\n# Print the f1 and RMSE scores for each fold\nprint(\"f1 Scores Mean:\", np.mean(f1_scores))\nprint(\"f1 Scores STD:\", np.std(f1_scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize AutoGluon for classification\nautogluon_model = TabularPredictor(problem_type=\"multiclass\", label=\"outcome\", eval_metric='f1_micro')\n\n# Fit AutoGluon to your data\nautogluon_model.fit(train_data=autogluon_data, time_limit=300)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_autogluon = autogluon_model.predict(transformed_test_df)\npred_autogluon","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame for submission\ntesting_df = pd.DataFrame(\n    {'id': df_test['id'],\n    'outcome': pred_autogluon})\n\n# testing_df.head(50)\n# Save to CSV for testing\ntesting_df.to_csv(f'submission_{model_number}_autogluon.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}