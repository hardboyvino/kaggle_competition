{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from helper_functions import brute_force_feat_engineering\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment_name = 'feat_eng_lgbm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate the engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('df_train.csv')\n",
    "test = pd.read_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90615, 11), (60411, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.2380</td>\n",
       "      <td>0.6185</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.4785</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>0.2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex_F  Sex_I  Sex_M  Length  Diameter  Height  Whole_weight  \\\n",
       "0    0.0    0.0    1.0   0.645     0.475   0.155        1.2380   \n",
       "1    0.0    0.0    1.0   0.580     0.460   0.160        0.9830   \n",
       "2    0.0    0.0    1.0   0.560     0.420   0.140        0.8395   \n",
       "3    0.0    0.0    1.0   0.570     0.490   0.145        0.8740   \n",
       "4    0.0    1.0    0.0   0.415     0.325   0.110        0.3580   \n",
       "\n",
       "   Shucked_weight  Viscera_weight  Shell_weight  \n",
       "0          0.6185          0.3125        0.3005  \n",
       "1          0.4785          0.2195        0.2750  \n",
       "2          0.3525          0.1845        0.2405  \n",
       "3          0.3525          0.1865        0.2350  \n",
       "4          0.1575          0.0670        0.1050  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.33 s\n",
      "Wall time: 3.64 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90615, 1145)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_df_train = brute_force_feat_engineering(train, numerical_features)\n",
    "\n",
    "new_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'significant_features_unique.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    new_feats = [line.strip() for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90615, 766)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_train = new_df_train[new_feats]\n",
    "final_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = list(final_df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'Rings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df_train\n",
    "y = train[TARGET]\n",
    "\n",
    "n_splits = 3\n",
    "sk10 = KFold(n_splits=n_splits, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_params_1 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.645831785537246, 'learning_rate': 0.13425614802120903, 'max_depth': 42, 'min_child_samples': 203, 'min_child_weight': 1.1732747258353804, 'min_split_gain': 0.9789391350146991, 'n_estimators': 639, 'num_leaves': 82, 'reg_alpha': 0.6380061711817115, 'reg_lambda': 0.6773770268796825}\n",
    "# lgbm_params_2 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.6293651061348586, 'learning_rate': 0.10096296495645429, 'max_depth': 31, 'min_child_samples': 225, 'min_child_weight': 3.3475628548945573, 'min_split_gain': 0.9997873682483012, 'n_estimators': 674, 'num_leaves': 48, 'reg_alpha': 0.005601881132355869, 'reg_lambda': 0.014315121505575882}\n",
    "# lgbm_params_3 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.5934335428151741, 'learning_rate': 0.13064405012196745, 'max_depth': 55, 'min_child_samples': 86, 'min_child_weight': 4.936258647279815, 'min_split_gain': 0.9987951237547827, 'n_estimators': 548, 'num_leaves': 49, 'reg_alpha': 0.7182759565064444, 'reg_lambda': 0.6464375985789672}\n",
    "# lgbm_params_4 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.7783982819520713, 'learning_rate': 0.106849626777837, 'max_depth': 55, 'min_child_samples': 144, 'min_child_weight': 4.78221373340921, 'min_split_gain': 0.9962013661512532, 'n_estimators': 881, 'num_leaves': 73, 'reg_alpha': 0.6725415981122735, 'reg_lambda': 0.5691606484723241}\n",
    "# xgb_params_1 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 1910, 'n_estimators': 114, 'max_depth': 7, 'min_child_weight': 30, 'gamma': 0.07058151945617566, 'learning_rate': 0.11049946445523824, 'subsample': 0.8247120605397805, 'colsample_bytree': 0.8865698498028886, 'colsample_bylevel': 0.6702964552995809, 'colsample_bynode': 0.8408329349933361, 'reg_alpha': 0.10512246978624393, 'reg_lambda': 0.7492332818823758, 'max_bin': 470, 'grow_policy': 'depthwise', 'max_leaves': 168}\n",
    "# xgb_params_2 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 1847, 'n_estimators': 137, 'max_depth': 10, 'min_child_weight': 11, 'gamma': 0.009038694720046182, 'learning_rate': 0.03817953360871485, 'subsample': 0.8477644110167295, 'colsample_bytree': 0.9277554141706083, 'colsample_bylevel': 0.7237726407553582, 'colsample_bynode': 0.6770089667557697, 'reg_alpha': 0.24474247489256953, 'reg_lambda': 0.7195072858628302, 'max_bin': 378, 'grow_policy': 'depthwise', 'max_leaves': 254}\n",
    "# xgb_params_3 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 272, 'n_estimators': 230, 'max_depth': 9, 'min_child_weight': 11, 'gamma': 0.044843515145694, 'learning_rate': 0.019162331263840444, 'subsample': 0.931366685201835, 'colsample_bytree': 0.8799087346382896, 'colsample_bylevel': 0.6655591824860992, 'colsample_bynode': 0.8795942374460464, 'reg_alpha': 0.28610673472174625, 'reg_lambda': 0.8435492745641229, 'max_bin': 686, 'grow_policy': 'lossguide', 'max_leaves': 248}\n",
    "# xgb_params_4 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 865, 'n_estimators': 246, 'max_depth': 10, 'min_child_weight': 16, 'gamma': 0.07801675175603039, 'learning_rate': 0.0182283286494275, 'subsample': 0.9302927053213158, 'colsample_bytree': 0.9398793839324162, 'colsample_bylevel': 0.8842522189848564, 'colsample_bynode': 0.5975281202232402, 'reg_alpha': 0.25706875496430687, 'reg_lambda': 0.9594748896931249, 'max_bin': 990, 'grow_policy': 'depthwise', 'max_leaves': 75}\n",
    "# hist_params_1 = {'random_state': 5, 'learning_rate': 0.10725554777891158, 'max_iter': 770, 'max_leaf_nodes': 38, 'min_samples_leaf': 208, 'l2_regularization': 0.8537951071003338, 'max_bins': 245, 'max_depth': 23}\n",
    "# hist_params_2 = {'random_state': 5, 'learning_rate': 0.1458108188074952, 'max_iter': 314, 'max_leaf_nodes': 38, 'min_samples_leaf': 199, 'l2_regularization': 0.7905252458308586, 'max_bins': 255, 'max_depth': 53}\n",
    "# hist_params_3 = {'random_state': 5, 'learning_rate': 0.11235116550129322, 'max_iter': 941, 'max_leaf_nodes': 30, 'min_samples_leaf': 214, 'l2_regularization': 0.7327242793316453, 'max_bins': 253, 'max_depth': 64}\n",
    "# hist_params_4 = {'random_state': 5, 'learning_rate': 0.10273156315228334, 'max_iter': 841, 'max_leaf_nodes': 37, 'min_samples_leaf': 136, 'l2_regularization': 0.9094301806535087, 'max_bins': 248, 'max_depth': 33}\n",
    "# cat_params_1 = {'random_state': 5, 'verbose': False, 'n_estimators': 1000, 'learning_rate': 0.07845229262737155, 'max_depth': 7, 'subsample': 0.5027494707261151, 'colsample_bylevel': 0.8471975669265636, 'min_data_in_leaf': 10, 'l2_leaf_reg': 2.1925875095619385, 'random_strength': 0.1930409355787267, 'bagging_temperature': 0.9979677062970205}\n",
    "# cat_params_2 = {'random_state': 5, 'verbose': False, 'early_stopping_rounds': 100, 'loss_function': 'RMSE', 'n_estimators': 1349, 'learning_rate': 0.06263309535822247, 'max_depth': 6, 'subsample': 0.871730356394106, 'colsample_bylevel': 0.9511706719437459, 'min_data_in_leaf': 37, 'l2_leaf_reg': 2.5615538795939923, 'random_strength': 0.031857748396944496, 'bagging_temperature': 0.20981165087726864}\n",
    "# cat_params_3 = {'verbose': False, 'early_stopping_rounds': 100, 'loss_function': 'RMSE', 'n_estimators': 1884, 'learning_rate': 0.059602068583387596, 'max_depth': 6, 'subsample': 0.5002999540682693, 'colsample_bylevel': 0.7319888215923419, 'min_data_in_leaf': 19, 'l2_leaf_reg': 1.4226291560055486, 'random_strength': 0.026986589728532878, 'bagging_temperature': 0.8027411163556936, 'random_state': 832}\n",
    "# cat_params_4 = {'verbose': False, 'early_stopping_rounds': 100, 'loss_function': 'RMSE', 'n_estimators': 2435, 'learning_rate': 0.027540417908588785, 'max_depth': 7, 'subsample': 0.7284463831828892, 'colsample_bylevel': 0.8054488430220728, 'min_data_in_leaf': 13, 'l2_leaf_reg': 2.8222615876183643, 'random_strength': 0.04098122140187414, 'bagging_temperature': 0.8760690509958485, 'random_state': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=50))\n",
    "])\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nystroem', Nystroem(n_components=1000, random_state=5)),\n",
    "    ('ridge', Ridge(alpha=0.1, random_state=5))\n",
    "])\n",
    "\n",
    "# Manually set names\n",
    "knn_pipeline.name = 'KNN'  # Custom name\n",
    "ridge_pipeline.name = 'Nystroem Ridge'  # Custom name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LGBMRegressor(n_jobs=-1, random_state=5),\n",
    "    # LGBMRegressor(**lgbm_params_1),\n",
    "    # LGBMRegressor(**lgbm_params_2),\n",
    "    # LGBMRegressor(**lgbm_params_3),\n",
    "    # LGBMRegressor(**lgbm_params_4),\n",
    "    # XGBRegressor(random_state=5),\n",
    "    # XGBRegressor(**xgb_params_1),\n",
    "    # XGBRegressor(**xgb_params_2),\n",
    "    # XGBRegressor(**xgb_params_3),\n",
    "    # XGBRegressor(**xgb_params_4),\n",
    "    # RandomForestRegressor(random_state=5),\n",
    "    # ExtraTreesRegressor(random_state=5),\n",
    "    # HistGradientBoostingRegressor(random_state=5),\n",
    "    # HistGradientBoostingRegressor(**hist_params_1),\n",
    "    # HistGradientBoostingRegressor(**hist_params_2),\n",
    "    # HistGradientBoostingRegressor(**hist_params_3),\n",
    "    # HistGradientBoostingRegressor(**hist_params_4),\n",
    "    # CatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100),\n",
    "    # CatBoostRegressor(**cat_params_1),\n",
    "    # CatBoostRegressor(**cat_params_2),\n",
    "    # CatBoostRegressor(**cat_params_3),\n",
    "    # CatBoostRegressor(**cat_params_4),\n",
    "    # knn_pipeline,\n",
    "    # ridge_pipeline,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error (RMSLE).\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    # # Add post processing step if required\n",
    "    # y_pred_processed = np.floor(y_pred)\n",
    "    \n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "    MLA_compare = pd.DataFrame(columns=['MLA Name', \n",
    "                                        'MLA Parameters', \n",
    "                                        'MLA Train ROC AUC', \n",
    "                                        'MLA Test ROC AUC', \n",
    "                                        'MLA Test ROC AUC Std', \n",
    "                                        'MLA Time'])\n",
    "    \n",
    "    def evaluate_model(alg, idx):\n",
    "        if hasattr(alg, 'name'):\n",
    "            MLA_name = alg.name\n",
    "        else:\n",
    "            MLA_name = alg.__class__.__name__\n",
    "        features = important_features.get(MLA_name, [])\n",
    "\n",
    "        # Check if the list of important features is empty\n",
    "        if len(features) == 0:\n",
    "            # If empty, return results with zero values\n",
    "            print(f'Skipping {MLA_name} due to no important features.')\n",
    "            return {\n",
    "                'MLA Name': MLA_name,\n",
    "                'MLA Parameters': str(alg.get_params()),\n",
    "                'MLA Train ROC': 0,\n",
    "                'MLA Test ROC': 0,\n",
    "                'MLA Test ROC Std': 0,\n",
    "                'MLA Time': \"0 min 0.00 sec\",\n",
    "            }\n",
    "        \n",
    "        cv_results = cross_validate(alg, \n",
    "                                    X[features], \n",
    "                                    y, cv=cv_split, \n",
    "                                    scoring=rmsle_scorer, \n",
    "                                    return_train_score=True, \n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Time formatting\n",
    "        mean_fit_time = cv_results['fit_time'].mean()\n",
    "        minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "        # Results population\n",
    "        result = {\n",
    "            'MLA Name': MLA_name,\n",
    "            'MLA Parameters': str(alg.get_params()),\n",
    "            'MLA Train ROC AUC': -cv_results['train_score'].mean(),\n",
    "            'MLA Test ROC AUC': -cv_results['test_score'].mean(),\n",
    "            'MLA Test ROC AUC Std': cv_results['test_score'].std(),\n",
    "            'MLA Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "        }\n",
    "\n",
    "        print(f'Done with {MLA_name}.')\n",
    "        return result\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(models)]\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results_list.append(result)\n",
    "\n",
    "    MLA_compare = pd.DataFrame(results_list)\n",
    "\n",
    "    MLA_compare.sort_values(by=['MLA Test ROC AUC'], ascending=True, inplace=True)\n",
    "    MLA_compare.to_csv(f'{experiment_name}_results.csv', index=False)\n",
    "\n",
    "    return MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = {}\n",
    "\n",
    "for model in models:\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    baseline_features[model_name] = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "baseline_models = evaluate_models(models, X, y, baseline_features, sk10, f'{experiment_name}')\n",
    "baseline_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated features (leaving just 1 of each pair)\n",
    "# Leave features highly correlated with the target\n",
    "df_no_corr = X.copy()\n",
    "correlation_matrix_spear = df_no_corr.corr(method='spearman').abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_spear = correlation_matrix_spear.where(np.triu(np.ones(correlation_matrix_spear.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than a threshold (e.g., 0.9 in this case)\n",
    "to_drop_spear = [column for column in upper_spear.columns if any(upper_spear[column] >= 0.9)]\n",
    "\n",
    "# Drop features\n",
    "df_reduced_spear = df_no_corr.drop(to_drop_spear, axis=1)\n",
    "\n",
    "# Get list of low correlation features excluding TARGET\n",
    "low_corr_feats_spear = list(df_reduced_spear.columns)\n",
    "\n",
    "with open('low_corr_spear.txt', 'w') as f:\n",
    "    f.write(str(low_corr_feats_spear))\n",
    "    f.write('\\n')\n",
    "\n",
    "# Print the high correlation features effect\n",
    "# Both pre and post drop dfs contain the TARGET\n",
    "print(f\"Dropped {len(to_drop_spear)} highly correlated features.\\nOld Shape of the dataset was {df_no_corr.shape}\\nNew shape of the dataset is {df_reduced_spear.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "no_corr_features = {}\n",
    "\n",
    "for model in models:\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    no_corr_features[model_name] = list(df_reduced_spear.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "no_corr_models = evaluate_models(models, df_reduced_spear, y, no_corr_features, sk10, f'{experiment_name}_corr')\n",
    "no_corr_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_importance_features = {}\n",
    "\n",
    "# for model in models:\n",
    "#     model_name = model.__class__.__name__\n",
    "\n",
    "#     try:\n",
    "#         # Initialize array to store feature importances\n",
    "#         feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "#         # Loop through each fold and calculate the feature importances\n",
    "#         for train_index, test_index in sk10.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#             y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             # Get the feature importances and them to the total\n",
    "#             feature_importances += model.feature_importances_\n",
    "\n",
    "#         feature_importances /= n_splits\n",
    "\n",
    "#         feature_importances_dict = dict(zip(X.columns, feature_importances))\n",
    "\n",
    "#         df = pd.DataFrame.from_dict(feature_importances_dict, orient='index')\n",
    "\n",
    "#         # Resetting index with a name for the column\n",
    "#         df = df.reset_index().rename(columns={'index': 'Feature', 0: 'Avg_Feat_Importance'})\n",
    "#         df.sort_values(by='Avg_Feat_Importance', ascending=False, inplace=True)\n",
    "\n",
    "#         # Save to CSV\n",
    "#         df.to_csv(f'{model_name}_feature_importances.csv')\n",
    "\n",
    "#         fi_threshold = 0\n",
    "\n",
    "#         fi_feats = df[df['Avg_Feat_Importance'] > fi_threshold]['Feature'].tolist()\n",
    "\n",
    "#         feat_importance_features[model_name] = fi_feats\n",
    "#         print(f'Done with {model_name}')\n",
    "\n",
    "#     except AttributeError:\n",
    "#         feat_importance_features[model_name] = list(X.columns)\n",
    "#         print(f'{model_name} does not have feature_importances_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('featimp_features.txt', mode='w') as f:\n",
    "#     pprint(feat_importance_features, stream=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random feature for X\n",
    "np.random.seed(5)\n",
    "X['random_control_feature'] = np.round(np.random.uniform(-2, 2, X.shape[0]), 6)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "perm_cv = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "perm_importances = {model.__class__.__name__ if not hasattr(model, 'name') else model.name: [] for model in models}\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(perm_cv.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for model in models:\n",
    "        if hasattr(model, 'name'):\n",
    "            model_name = model.name\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "        # Calculate permutation importance\n",
    "        result = permutation_importance(model, X_test, y_test, n_repeats=5, random_state=5, n_jobs=-1, scoring=rmsle_scorer)\n",
    "        perm_importances[model_name].append(result.importances_mean)\n",
    "        print(f'Done with {model_name}.')\n",
    "    \n",
    "    print(f'Done with Fold {i+1}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Average importances across folds and export to CSV\n",
    "for model_name, importances in perm_importances.items():\n",
    "    avg_importance = np.mean(importances, axis=0)\n",
    "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': avg_importance})\n",
    "    importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    # Export to CSV\n",
    "    importance_df.to_csv(f'.\\permutation_importances\\{model_name}_permutation_importance.csv', index=False)\n",
    "\n",
    "print('Done with Permuation Importances', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'permutation_importances'\n",
    "\n",
    "# Initialize a dictionary for the features\n",
    "perm_important_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    csv_path = os.path.join(directory, f'{model_name}_permutation_importance.csv')\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Check for 'random_control_feature' and its importance\n",
    "        if 'random_control_feature' in df['Feature'].values:\n",
    "            random_feature_importance = df.loc[df['Feature'] == 'random_control_feature', 'Importance'].iloc[0]\n",
    "        else:\n",
    "            random_feature_importance = 0\n",
    "\n",
    "        # Determine the threshold\n",
    "        threshold = max(0, random_feature_importance)\n",
    "\n",
    "        # Filter features where importance is greater than 0\n",
    "        important_feats_filtered = df[df['Importance'] > threshold]['Feature'].tolist()\n",
    "\n",
    "        # Reorder important_feats based on the predefined features_list\n",
    "        important_feats_ordered = [feat for feat in features_list if feat in important_feats_filtered]\n",
    "\n",
    "        # Add to importance dictionary\n",
    "        perm_important_features[model_name] = important_feats_ordered\n",
    "\n",
    "    else:\n",
    "        print(f'CSV file for {model_name} not found.')\n",
    "\n",
    "print('Done getting important features dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perm_important_features_feat_eng.txt', mode='w') as f:\n",
    "    pprint(perm_important_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "perm_importance_models = evaluate_models(models, X, y, perm_important_features, sk10, f'{experiment_name}_permimp')\n",
    "perm_importance_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SelectKBest with f_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_list = []\n",
    "kbest_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # Select whichever one had a better CV score generally\n",
    "    # Also, consider computational expense and accuracy balance\n",
    "    \n",
    "    features = perm_important_features[model_name]\n",
    "    # features = list(df_reduced_spear.columns)\n",
    "\n",
    "    # incase there is no feature that had importance, go to the next model\n",
    "    if len(features) == 0:\n",
    "        continue\n",
    "\t\n",
    "    X_kbest = X[features]\n",
    "    best_score = 0\n",
    "    best_k = 0\n",
    "    best_features = []\n",
    "\n",
    "    # Iterate over k from 1 to number of features\n",
    "    for k in range(1, len(features) + 1):\n",
    "        print(f'currently running {k} features on {model_name}')\n",
    "        # Apply SelectKBest\n",
    "        selector = SelectKBest(f_regression, k=k)\n",
    "        X_new = selector.fit_transform(X_kbest, y)\n",
    "\n",
    "        # Get the selected feature names\n",
    "        selected_features = X_kbest.columns[selector.get_support()]\n",
    "\n",
    "        # Evaluate the model\n",
    "        # model = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "        rmsle_scores = cross_validate(model, X_new, y, cv=sk10, scoring=rmsle_scorer, n_jobs=-1)\n",
    "        mean_rmsle_scores = rmsle_scores['test_score'].mean()\n",
    "\n",
    "        if mean_rmsle_scores > best_score:\n",
    "            best_k = k\n",
    "            best_score = mean_rmsle_scores\n",
    "            best_features = list(selected_features)\n",
    "\n",
    "    best_features_list.append({'k': best_k,\n",
    "                    'Selected Features': best_features,\n",
    "                    'RMSLE Score': best_score,\n",
    "                    'Model Name': model_name})\n",
    "    \n",
    "    kbest_features[model_name] = best_features\n",
    "\n",
    "best_features_df = pd.DataFrame(best_features_list)\n",
    "\n",
    "best_features_df.sort_values(by='RMSLE Score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kbest_features.txt', mode='w') as f:\n",
    "    pprint(kbest_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with LGBMRegressor\n",
      "Fitting estimator with 766 features.\n",
      "Fitting estimator with 765 features.\n",
      "Fitting estimator with 764 features.\n",
      "Fitting estimator with 763 features.\n",
      "Fitting estimator with 762 features.\n",
      "Fitting estimator with 761 features.\n",
      "Fitting estimator with 760 features.\n",
      "Fitting estimator with 759 features.\n",
      "Fitting estimator with 758 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 756 features.\n",
      "Fitting estimator with 755 features.\n",
      "Fitting estimator with 754 features.\n",
      "Fitting estimator with 753 features.\n",
      "Fitting estimator with 752 features.\n",
      "Fitting estimator with 751 features.\n",
      "Fitting estimator with 750 features.\n",
      "Fitting estimator with 749 features.\n",
      "Fitting estimator with 748 features.\n",
      "Fitting estimator with 747 features.\n",
      "Fitting estimator with 746 features.\n",
      "Fitting estimator with 745 features.\n",
      "Fitting estimator with 744 features.\n",
      "Fitting estimator with 743 features.\n",
      "Fitting estimator with 742 features.\n",
      "Fitting estimator with 741 features.\n",
      "Fitting estimator with 740 features.\n",
      "Fitting estimator with 739 features.\n",
      "Fitting estimator with 738 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 736 features.\n",
      "Fitting estimator with 735 features.\n",
      "Fitting estimator with 734 features.\n",
      "Fitting estimator with 733 features.\n",
      "Fitting estimator with 732 features.\n",
      "Fitting estimator with 731 features.\n",
      "Fitting estimator with 730 features.\n",
      "Fitting estimator with 729 features.\n",
      "Fitting estimator with 728 features.\n",
      "Fitting estimator with 727 features.\n",
      "Fitting estimator with 726 features.\n",
      "Fitting estimator with 725 features.\n",
      "Fitting estimator with 724 features.\n",
      "Fitting estimator with 723 features.\n",
      "Fitting estimator with 722 features.\n",
      "Fitting estimator with 721 features.\n",
      "Fitting estimator with 720 features.\n",
      "Fitting estimator with 719 features.\n",
      "Fitting estimator with 718 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 716 features.\n",
      "Fitting estimator with 715 features.\n",
      "Fitting estimator with 714 features.\n",
      "Fitting estimator with 713 features.\n",
      "Fitting estimator with 712 features.\n",
      "Fitting estimator with 711 features.\n",
      "Fitting estimator with 710 features.\n",
      "Fitting estimator with 709 features.\n",
      "Fitting estimator with 708 features.\n",
      "Fitting estimator with 707 features.\n",
      "Fitting estimator with 706 features.\n",
      "Fitting estimator with 705 features.\n",
      "Fitting estimator with 704 features.\n",
      "Fitting estimator with 703 features.\n",
      "Fitting estimator with 702 features.\n",
      "Fitting estimator with 701 features.\n",
      "Fitting estimator with 700 features.\n",
      "Fitting estimator with 699 features.\n",
      "Fitting estimator with 698 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 696 features.\n",
      "Fitting estimator with 695 features.\n",
      "Fitting estimator with 694 features.\n",
      "Fitting estimator with 693 features.\n",
      "Fitting estimator with 692 features.\n",
      "Fitting estimator with 691 features.\n",
      "Fitting estimator with 690 features.\n",
      "Fitting estimator with 689 features.\n",
      "Fitting estimator with 688 features.\n",
      "Fitting estimator with 687 features.\n",
      "Fitting estimator with 686 features.\n",
      "Fitting estimator with 685 features.\n",
      "Fitting estimator with 684 features.\n",
      "Fitting estimator with 683 features.\n",
      "Fitting estimator with 682 features.\n",
      "Fitting estimator with 681 features.\n",
      "Fitting estimator with 680 features.\n",
      "Fitting estimator with 679 features.\n",
      "Fitting estimator with 678 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 676 features.\n",
      "Fitting estimator with 675 features.\n",
      "Fitting estimator with 674 features.\n",
      "Fitting estimator with 673 features.\n",
      "Fitting estimator with 672 features.\n",
      "Fitting estimator with 671 features.\n",
      "Fitting estimator with 670 features.\n",
      "Fitting estimator with 669 features.\n",
      "Fitting estimator with 668 features.\n",
      "Fitting estimator with 667 features.\n",
      "Fitting estimator with 666 features.\n",
      "Fitting estimator with 665 features.\n",
      "Fitting estimator with 664 features.\n",
      "Fitting estimator with 663 features.\n",
      "Fitting estimator with 662 features.\n",
      "Fitting estimator with 661 features.\n",
      "Fitting estimator with 660 features.\n",
      "Fitting estimator with 659 features.\n",
      "Fitting estimator with 658 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 656 features.\n",
      "Fitting estimator with 655 features.\n",
      "Fitting estimator with 654 features.\n",
      "Fitting estimator with 653 features.\n",
      "Fitting estimator with 652 features.\n",
      "Fitting estimator with 651 features.\n",
      "Fitting estimator with 650 features.\n",
      "Fitting estimator with 649 features.\n",
      "Fitting estimator with 648 features.\n",
      "Fitting estimator with 647 features.\n",
      "Fitting estimator with 646 features.\n",
      "Fitting estimator with 645 features.\n",
      "Fitting estimator with 644 features.\n",
      "Fitting estimator with 643 features.\n",
      "Fitting estimator with 642 features.\n",
      "Fitting estimator with 641 features.\n",
      "Fitting estimator with 640 features.\n",
      "Fitting estimator with 639 features.\n",
      "Fitting estimator with 638 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 636 features.\n",
      "Fitting estimator with 635 features.\n",
      "Fitting estimator with 634 features.\n",
      "Fitting estimator with 633 features.\n",
      "Fitting estimator with 632 features.\n",
      "Fitting estimator with 631 features.\n",
      "Fitting estimator with 630 features.\n",
      "Fitting estimator with 629 features.\n",
      "Fitting estimator with 628 features.\n",
      "Fitting estimator with 627 features.\n",
      "Fitting estimator with 626 features.\n",
      "Fitting estimator with 625 features.\n",
      "Fitting estimator with 624 features.\n",
      "Fitting estimator with 623 features.\n",
      "Fitting estimator with 622 features.\n",
      "Fitting estimator with 621 features.\n",
      "Fitting estimator with 620 features.\n",
      "Fitting estimator with 619 features.\n",
      "Fitting estimator with 618 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 616 features.\n",
      "Fitting estimator with 615 features.\n",
      "Fitting estimator with 614 features.\n",
      "Fitting estimator with 613 features.\n",
      "Fitting estimator with 612 features.\n",
      "Fitting estimator with 611 features.\n",
      "Fitting estimator with 610 features.\n",
      "Fitting estimator with 609 features.\n",
      "Fitting estimator with 608 features.\n",
      "Fitting estimator with 607 features.\n",
      "Fitting estimator with 606 features.\n",
      "Fitting estimator with 605 features.\n",
      "Fitting estimator with 604 features.\n",
      "Fitting estimator with 603 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 601 features.\n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 599 features.\n",
      "Fitting estimator with 598 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 596 features.\n",
      "Fitting estimator with 595 features.\n",
      "Fitting estimator with 594 features.\n",
      "Fitting estimator with 593 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 591 features.\n",
      "Fitting estimator with 590 features.\n",
      "Fitting estimator with 589 features.\n",
      "Fitting estimator with 588 features.\n",
      "Fitting estimator with 587 features.\n",
      "Fitting estimator with 586 features.\n",
      "Fitting estimator with 585 features.\n",
      "Fitting estimator with 584 features.\n",
      "Fitting estimator with 583 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 581 features.\n",
      "Fitting estimator with 580 features.\n",
      "Fitting estimator with 579 features.\n",
      "Fitting estimator with 578 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 576 features.\n",
      "Fitting estimator with 575 features.\n",
      "Fitting estimator with 574 features.\n",
      "Fitting estimator with 573 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 571 features.\n",
      "Fitting estimator with 570 features.\n",
      "Fitting estimator with 569 features.\n",
      "Fitting estimator with 568 features.\n",
      "Fitting estimator with 567 features.\n",
      "Fitting estimator with 566 features.\n",
      "Fitting estimator with 565 features.\n",
      "Fitting estimator with 564 features.\n",
      "Fitting estimator with 563 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 561 features.\n",
      "Fitting estimator with 560 features.\n",
      "Fitting estimator with 559 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 556 features.\n",
      "Fitting estimator with 555 features.\n",
      "Fitting estimator with 554 features.\n",
      "Fitting estimator with 553 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 551 features.\n",
      "Fitting estimator with 550 features.\n",
      "Fitting estimator with 549 features.\n",
      "Fitting estimator with 548 features.\n",
      "Fitting estimator with 547 features.\n",
      "Fitting estimator with 546 features.\n",
      "Fitting estimator with 545 features.\n",
      "Fitting estimator with 544 features.\n",
      "Fitting estimator with 543 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 541 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 539 features.\n",
      "Fitting estimator with 538 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 536 features.\n",
      "Fitting estimator with 535 features.\n",
      "Fitting estimator with 534 features.\n",
      "Fitting estimator with 533 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 531 features.\n",
      "Fitting estimator with 530 features.\n",
      "Fitting estimator with 529 features.\n",
      "Fitting estimator with 528 features.\n",
      "Fitting estimator with 527 features.\n",
      "Fitting estimator with 526 features.\n",
      "Fitting estimator with 525 features.\n",
      "Fitting estimator with 524 features.\n",
      "Fitting estimator with 523 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 521 features.\n",
      "Fitting estimator with 520 features.\n",
      "Fitting estimator with 519 features.\n",
      "Fitting estimator with 518 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 516 features.\n",
      "Fitting estimator with 515 features.\n",
      "Fitting estimator with 514 features.\n",
      "Fitting estimator with 513 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 511 features.\n",
      "Fitting estimator with 510 features.\n",
      "Fitting estimator with 509 features.\n",
      "Fitting estimator with 508 features.\n",
      "Fitting estimator with 507 features.\n",
      "Fitting estimator with 506 features.\n",
      "Fitting estimator with 505 features.\n",
      "Fitting estimator with 504 features.\n",
      "Fitting estimator with 503 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 501 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "Fitting estimator with 390 features.\n",
      "Fitting estimator with 389 features.\n",
      "Fitting estimator with 388 features.\n",
      "Fitting estimator with 387 features.\n",
      "Fitting estimator with 386 features.\n",
      "Fitting estimator with 385 features.\n",
      "Fitting estimator with 384 features.\n",
      "Fitting estimator with 383 features.\n",
      "Fitting estimator with 382 features.\n",
      "Fitting estimator with 381 features.\n",
      "Fitting estimator with 380 features.\n",
      "Fitting estimator with 379 features.\n",
      "Fitting estimator with 378 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 376 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 374 features.\n",
      "Fitting estimator with 373 features.\n",
      "Fitting estimator with 372 features.\n",
      "Fitting estimator with 371 features.\n",
      "Fitting estimator with 370 features.\n",
      "Fitting estimator with 369 features.\n",
      "Fitting estimator with 368 features.\n",
      "Fitting estimator with 367 features.\n",
      "Fitting estimator with 366 features.\n",
      "Fitting estimator with 365 features.\n",
      "Fitting estimator with 364 features.\n",
      "Fitting estimator with 363 features.\n",
      "Fitting estimator with 362 features.\n",
      "Fitting estimator with 361 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 359 features.\n",
      "Fitting estimator with 358 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 356 features.\n",
      "Fitting estimator with 355 features.\n",
      "Fitting estimator with 354 features.\n",
      "Fitting estimator with 353 features.\n",
      "Fitting estimator with 352 features.\n",
      "Fitting estimator with 351 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 349 features.\n",
      "Fitting estimator with 348 features.\n",
      "Fitting estimator with 347 features.\n",
      "Fitting estimator with 346 features.\n",
      "Fitting estimator with 345 features.\n",
      "Fitting estimator with 344 features.\n",
      "Fitting estimator with 343 features.\n",
      "Fitting estimator with 342 features.\n",
      "Fitting estimator with 341 features.\n",
      "Fitting estimator with 340 features.\n",
      "Fitting estimator with 339 features.\n",
      "Fitting estimator with 338 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 336 features.\n",
      "Fitting estimator with 335 features.\n",
      "Fitting estimator with 334 features.\n",
      "Fitting estimator with 333 features.\n",
      "Fitting estimator with 332 features.\n",
      "Fitting estimator with 331 features.\n",
      "Fitting estimator with 330 features.\n",
      "Fitting estimator with 329 features.\n",
      "Fitting estimator with 328 features.\n",
      "Fitting estimator with 327 features.\n",
      "Fitting estimator with 326 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 324 features.\n",
      "Fitting estimator with 323 features.\n",
      "Fitting estimator with 322 features.\n",
      "Fitting estimator with 321 features.\n",
      "Fitting estimator with 320 features.\n",
      "Fitting estimator with 319 features.\n",
      "Fitting estimator with 318 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 316 features.\n",
      "Fitting estimator with 315 features.\n",
      "Fitting estimator with 314 features.\n",
      "Fitting estimator with 313 features.\n",
      "Fitting estimator with 312 features.\n",
      "Fitting estimator with 311 features.\n",
      "Fitting estimator with 310 features.\n",
      "Fitting estimator with 309 features.\n",
      "Fitting estimator with 308 features.\n",
      "Fitting estimator with 307 features.\n",
      "Fitting estimator with 306 features.\n",
      "Fitting estimator with 305 features.\n",
      "Fitting estimator with 304 features.\n",
      "Fitting estimator with 303 features.\n",
      "Fitting estimator with 302 features.\n",
      "Fitting estimator with 301 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 299 features.\n",
      "Fitting estimator with 298 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 296 features.\n",
      "Fitting estimator with 295 features.\n",
      "Fitting estimator with 294 features.\n",
      "Fitting estimator with 293 features.\n",
      "Fitting estimator with 292 features.\n",
      "Fitting estimator with 291 features.\n",
      "Fitting estimator with 290 features.\n",
      "Fitting estimator with 289 features.\n",
      "Fitting estimator with 288 features.\n",
      "Fitting estimator with 287 features.\n",
      "Fitting estimator with 286 features.\n",
      "Fitting estimator with 285 features.\n",
      "Fitting estimator with 284 features.\n",
      "Fitting estimator with 283 features.\n",
      "Fitting estimator with 282 features.\n",
      "Fitting estimator with 281 features.\n",
      "Fitting estimator with 280 features.\n",
      "Fitting estimator with 279 features.\n",
      "Fitting estimator with 278 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 276 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 274 features.\n",
      "Fitting estimator with 273 features.\n",
      "Fitting estimator with 272 features.\n",
      "Fitting estimator with 271 features.\n",
      "Fitting estimator with 270 features.\n",
      "Fitting estimator with 269 features.\n",
      "Fitting estimator with 268 features.\n",
      "Fitting estimator with 267 features.\n",
      "Fitting estimator with 266 features.\n",
      "Fitting estimator with 265 features.\n",
      "Fitting estimator with 264 features.\n",
      "Fitting estimator with 263 features.\n",
      "Fitting estimator with 262 features.\n",
      "Fitting estimator with 261 features.\n",
      "Fitting estimator with 260 features.\n",
      "Fitting estimator with 259 features.\n",
      "Fitting estimator with 258 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 256 features.\n",
      "Fitting estimator with 255 features.\n",
      "Fitting estimator with 254 features.\n",
      "Fitting estimator with 253 features.\n",
      "Fitting estimator with 252 features.\n",
      "Fitting estimator with 251 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 249 features.\n",
      "Fitting estimator with 248 features.\n",
      "Fitting estimator with 247 features.\n",
      "Fitting estimator with 246 features.\n",
      "Fitting estimator with 245 features.\n",
      "Fitting estimator with 244 features.\n",
      "Fitting estimator with 243 features.\n",
      "Fitting estimator with 242 features.\n",
      "Fitting estimator with 241 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 239 features.\n",
      "Fitting estimator with 238 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 236 features.\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 234 features.\n",
      "Fitting estimator with 233 features.\n",
      "Fitting estimator with 232 features.\n",
      "Fitting estimator with 231 features.\n",
      "Fitting estimator with 230 features.\n",
      "Fitting estimator with 229 features.\n",
      "Fitting estimator with 228 features.\n",
      "Fitting estimator with 227 features.\n",
      "Fitting estimator with 226 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 224 features.\n",
      "Fitting estimator with 223 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 221 features.\n",
      "Fitting estimator with 220 features.\n",
      "Fitting estimator with 219 features.\n",
      "Fitting estimator with 218 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 216 features.\n",
      "Fitting estimator with 215 features.\n",
      "Fitting estimator with 214 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 211 features.\n",
      "Fitting estimator with 210 features.\n",
      "Fitting estimator with 209 features.\n",
      "Fitting estimator with 208 features.\n",
      "Fitting estimator with 207 features.\n",
      "Fitting estimator with 206 features.\n",
      "Fitting estimator with 205 features.\n",
      "Fitting estimator with 204 features.\n",
      "Fitting estimator with 203 features.\n",
      "Fitting estimator with 202 features.\n",
      "Fitting estimator with 201 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 199 features.\n",
      "Fitting estimator with 198 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 196 features.\n",
      "Fitting estimator with 195 features.\n",
      "Fitting estimator with 194 features.\n",
      "Fitting estimator with 193 features.\n",
      "Fitting estimator with 192 features.\n",
      "Fitting estimator with 191 features.\n",
      "Fitting estimator with 190 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 188 features.\n",
      "Fitting estimator with 187 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 185 features.\n",
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 181 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 179 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 176 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 172 features.\n",
      "Fitting estimator with 171 features.\n",
      "Fitting estimator with 170 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 168 features.\n",
      "Fitting estimator with 167 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 165 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 163 features.\n",
      "Fitting estimator with 162 features.\n",
      "Fitting estimator with 161 features.\n",
      "Fitting estimator with 160 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 158 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 156 features.\n",
      "Fitting estimator with 155 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 153 features.\n",
      "Fitting estimator with 152 features.\n",
      "Fitting estimator with 151 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 148 features.\n",
      "Fitting estimator with 147 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 766 features.\n",
      "Fitting estimator with 765 features.\n",
      "Fitting estimator with 764 features.\n",
      "Fitting estimator with 763 features.\n",
      "Fitting estimator with 762 features.\n",
      "Fitting estimator with 761 features.\n",
      "Fitting estimator with 760 features.\n",
      "Fitting estimator with 759 features.\n",
      "Fitting estimator with 758 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 756 features.\n",
      "Fitting estimator with 755 features.\n",
      "Fitting estimator with 754 features.\n",
      "Fitting estimator with 753 features.\n",
      "Fitting estimator with 752 features.\n",
      "Fitting estimator with 751 features.\n",
      "Fitting estimator with 750 features.\n",
      "Fitting estimator with 749 features.\n",
      "Fitting estimator with 748 features.\n",
      "Fitting estimator with 747 features.\n",
      "Fitting estimator with 746 features.\n",
      "Fitting estimator with 745 features.\n",
      "Fitting estimator with 744 features.\n",
      "Fitting estimator with 743 features.\n",
      "Fitting estimator with 742 features.\n",
      "Fitting estimator with 741 features.\n",
      "Fitting estimator with 740 features.\n",
      "Fitting estimator with 739 features.\n",
      "Fitting estimator with 738 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 736 features.\n",
      "Fitting estimator with 735 features.\n",
      "Fitting estimator with 734 features.\n",
      "Fitting estimator with 733 features.\n",
      "Fitting estimator with 732 features.\n",
      "Fitting estimator with 731 features.\n",
      "Fitting estimator with 730 features.\n",
      "Fitting estimator with 729 features.\n",
      "Fitting estimator with 728 features.\n",
      "Fitting estimator with 727 features.\n",
      "Fitting estimator with 726 features.\n",
      "Fitting estimator with 725 features.\n",
      "Fitting estimator with 724 features.\n",
      "Fitting estimator with 723 features.\n",
      "Fitting estimator with 722 features.\n",
      "Fitting estimator with 721 features.\n",
      "Fitting estimator with 720 features.\n",
      "Fitting estimator with 719 features.\n",
      "Fitting estimator with 718 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 716 features.\n",
      "Fitting estimator with 715 features.\n",
      "Fitting estimator with 714 features.\n",
      "Fitting estimator with 713 features.\n",
      "Fitting estimator with 712 features.\n",
      "Fitting estimator with 711 features.\n",
      "Fitting estimator with 710 features.\n",
      "Fitting estimator with 709 features.\n",
      "Fitting estimator with 708 features.\n",
      "Fitting estimator with 707 features.\n",
      "Fitting estimator with 706 features.\n",
      "Fitting estimator with 705 features.\n",
      "Fitting estimator with 704 features.\n",
      "Fitting estimator with 703 features.\n",
      "Fitting estimator with 702 features.\n",
      "Fitting estimator with 701 features.\n",
      "Fitting estimator with 700 features.\n",
      "Fitting estimator with 699 features.\n",
      "Fitting estimator with 698 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 696 features.\n",
      "Fitting estimator with 695 features.\n",
      "Fitting estimator with 694 features.\n",
      "Fitting estimator with 693 features.\n",
      "Fitting estimator with 692 features.\n",
      "Fitting estimator with 691 features.\n",
      "Fitting estimator with 690 features.\n",
      "Fitting estimator with 689 features.\n",
      "Fitting estimator with 688 features.\n",
      "Fitting estimator with 687 features.\n",
      "Fitting estimator with 686 features.\n",
      "Fitting estimator with 685 features.\n",
      "Fitting estimator with 684 features.\n",
      "Fitting estimator with 683 features.\n",
      "Fitting estimator with 682 features.\n",
      "Fitting estimator with 681 features.\n",
      "Fitting estimator with 680 features.\n",
      "Fitting estimator with 679 features.\n",
      "Fitting estimator with 678 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 676 features.\n",
      "Fitting estimator with 675 features.\n",
      "Fitting estimator with 674 features.\n",
      "Fitting estimator with 673 features.\n",
      "Fitting estimator with 672 features.\n",
      "Fitting estimator with 671 features.\n",
      "Fitting estimator with 670 features.\n",
      "Fitting estimator with 669 features.\n",
      "Fitting estimator with 668 features.\n",
      "Fitting estimator with 667 features.\n",
      "Fitting estimator with 666 features.\n",
      "Fitting estimator with 665 features.\n",
      "Fitting estimator with 664 features.\n",
      "Fitting estimator with 663 features.\n",
      "Fitting estimator with 662 features.\n",
      "Fitting estimator with 661 features.\n",
      "Fitting estimator with 660 features.\n",
      "Fitting estimator with 659 features.\n",
      "Fitting estimator with 658 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 656 features.\n",
      "Fitting estimator with 655 features.\n",
      "Fitting estimator with 654 features.\n",
      "Fitting estimator with 653 features.\n",
      "Fitting estimator with 652 features.\n",
      "Fitting estimator with 651 features.\n",
      "Fitting estimator with 650 features.\n",
      "Fitting estimator with 649 features.\n",
      "Fitting estimator with 648 features.\n",
      "Fitting estimator with 647 features.\n",
      "Fitting estimator with 646 features.\n",
      "Fitting estimator with 645 features.\n",
      "Fitting estimator with 644 features.\n",
      "Fitting estimator with 643 features.\n",
      "Fitting estimator with 642 features.\n",
      "Fitting estimator with 641 features.\n",
      "Fitting estimator with 640 features.\n",
      "Fitting estimator with 639 features.\n",
      "Fitting estimator with 638 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 636 features.\n",
      "Fitting estimator with 635 features.\n",
      "Fitting estimator with 634 features.\n",
      "Fitting estimator with 633 features.\n",
      "Fitting estimator with 632 features.\n",
      "Fitting estimator with 631 features.\n",
      "Fitting estimator with 630 features.\n",
      "Fitting estimator with 629 features.\n",
      "Fitting estimator with 628 features.\n",
      "Fitting estimator with 627 features.\n",
      "Fitting estimator with 626 features.\n",
      "Fitting estimator with 625 features.\n",
      "Fitting estimator with 624 features.\n",
      "Fitting estimator with 623 features.\n",
      "Fitting estimator with 622 features.\n",
      "Fitting estimator with 621 features.\n",
      "Fitting estimator with 620 features.\n",
      "Fitting estimator with 619 features.\n",
      "Fitting estimator with 618 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 616 features.\n",
      "Fitting estimator with 615 features.\n",
      "Fitting estimator with 614 features.\n",
      "Fitting estimator with 613 features.\n",
      "Fitting estimator with 612 features.\n",
      "Fitting estimator with 611 features.\n",
      "Fitting estimator with 610 features.\n",
      "Fitting estimator with 609 features.\n",
      "Fitting estimator with 608 features.\n",
      "Fitting estimator with 607 features.\n",
      "Fitting estimator with 606 features.\n",
      "Fitting estimator with 605 features.\n",
      "Fitting estimator with 604 features.\n",
      "Fitting estimator with 603 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 601 features.\n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 599 features.\n",
      "Fitting estimator with 598 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 596 features.\n",
      "Fitting estimator with 595 features.\n",
      "Fitting estimator with 594 features.\n",
      "Fitting estimator with 593 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 591 features.\n",
      "Fitting estimator with 590 features.\n",
      "Fitting estimator with 589 features.\n",
      "Fitting estimator with 588 features.\n",
      "Fitting estimator with 587 features.\n",
      "Fitting estimator with 586 features.\n",
      "Fitting estimator with 585 features.\n",
      "Fitting estimator with 584 features.\n",
      "Fitting estimator with 583 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 581 features.\n",
      "Fitting estimator with 580 features.\n",
      "Fitting estimator with 579 features.\n",
      "Fitting estimator with 578 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 576 features.\n",
      "Fitting estimator with 575 features.\n",
      "Fitting estimator with 574 features.\n",
      "Fitting estimator with 573 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 571 features.\n",
      "Fitting estimator with 570 features.\n",
      "Fitting estimator with 569 features.\n",
      "Fitting estimator with 568 features.\n",
      "Fitting estimator with 567 features.\n",
      "Fitting estimator with 566 features.\n",
      "Fitting estimator with 565 features.\n",
      "Fitting estimator with 564 features.\n",
      "Fitting estimator with 563 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 561 features.\n",
      "Fitting estimator with 560 features.\n",
      "Fitting estimator with 559 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 556 features.\n",
      "Fitting estimator with 555 features.\n",
      "Fitting estimator with 554 features.\n",
      "Fitting estimator with 553 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 551 features.\n",
      "Fitting estimator with 550 features.\n",
      "Fitting estimator with 549 features.\n",
      "Fitting estimator with 548 features.\n",
      "Fitting estimator with 547 features.\n",
      "Fitting estimator with 546 features.\n",
      "Fitting estimator with 545 features.\n",
      "Fitting estimator with 544 features.\n",
      "Fitting estimator with 543 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 541 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 539 features.\n",
      "Fitting estimator with 538 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 536 features.\n",
      "Fitting estimator with 535 features.\n",
      "Fitting estimator with 534 features.\n",
      "Fitting estimator with 533 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 531 features.\n",
      "Fitting estimator with 530 features.\n",
      "Fitting estimator with 529 features.\n",
      "Fitting estimator with 528 features.\n",
      "Fitting estimator with 527 features.\n",
      "Fitting estimator with 526 features.\n",
      "Fitting estimator with 525 features.\n",
      "Fitting estimator with 524 features.\n",
      "Fitting estimator with 523 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 521 features.\n",
      "Fitting estimator with 520 features.\n",
      "Fitting estimator with 519 features.\n",
      "Fitting estimator with 518 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 516 features.\n",
      "Fitting estimator with 515 features.\n",
      "Fitting estimator with 514 features.\n",
      "Fitting estimator with 513 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 511 features.\n",
      "Fitting estimator with 510 features.\n",
      "Fitting estimator with 509 features.\n",
      "Fitting estimator with 508 features.\n",
      "Fitting estimator with 507 features.\n",
      "Fitting estimator with 506 features.\n",
      "Fitting estimator with 505 features.\n",
      "Fitting estimator with 504 features.\n",
      "Fitting estimator with 503 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 501 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "Fitting estimator with 390 features.\n",
      "Fitting estimator with 389 features.\n",
      "Fitting estimator with 388 features.\n",
      "Fitting estimator with 387 features.\n",
      "Fitting estimator with 386 features.\n",
      "Fitting estimator with 385 features.\n",
      "Fitting estimator with 384 features.\n",
      "Fitting estimator with 383 features.\n",
      "Fitting estimator with 382 features.\n",
      "Fitting estimator with 381 features.\n",
      "Fitting estimator with 380 features.\n",
      "Fitting estimator with 379 features.\n",
      "Fitting estimator with 378 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 376 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 374 features.\n",
      "Fitting estimator with 373 features.\n",
      "Fitting estimator with 372 features.\n",
      "Fitting estimator with 371 features.\n",
      "Fitting estimator with 370 features.\n",
      "Fitting estimator with 369 features.\n",
      "Fitting estimator with 368 features.\n",
      "Fitting estimator with 367 features.\n",
      "Fitting estimator with 366 features.\n",
      "Fitting estimator with 365 features.\n",
      "Fitting estimator with 364 features.\n",
      "Fitting estimator with 363 features.\n",
      "Fitting estimator with 362 features.\n",
      "Fitting estimator with 361 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 359 features.\n",
      "Fitting estimator with 358 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 356 features.\n",
      "Fitting estimator with 355 features.\n",
      "Fitting estimator with 354 features.\n",
      "Fitting estimator with 353 features.\n",
      "Fitting estimator with 352 features.\n",
      "Fitting estimator with 351 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 349 features.\n",
      "Fitting estimator with 348 features.\n",
      "Fitting estimator with 347 features.\n",
      "Fitting estimator with 346 features.\n",
      "Fitting estimator with 345 features.\n",
      "Fitting estimator with 344 features.\n",
      "Fitting estimator with 343 features.\n",
      "Fitting estimator with 342 features.\n",
      "Fitting estimator with 341 features.\n",
      "Fitting estimator with 340 features.\n",
      "Fitting estimator with 339 features.\n",
      "Fitting estimator with 338 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 336 features.\n",
      "Fitting estimator with 335 features.\n",
      "Fitting estimator with 334 features.\n",
      "Fitting estimator with 333 features.\n",
      "Fitting estimator with 332 features.\n",
      "Fitting estimator with 331 features.\n",
      "Fitting estimator with 330 features.\n",
      "Fitting estimator with 329 features.\n",
      "Fitting estimator with 328 features.\n",
      "Fitting estimator with 327 features.\n",
      "Fitting estimator with 326 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 324 features.\n",
      "Fitting estimator with 323 features.\n",
      "Fitting estimator with 322 features.\n",
      "Fitting estimator with 321 features.\n",
      "Fitting estimator with 320 features.\n",
      "Fitting estimator with 319 features.\n",
      "Fitting estimator with 318 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 316 features.\n",
      "Fitting estimator with 315 features.\n",
      "Fitting estimator with 314 features.\n",
      "Fitting estimator with 313 features.\n",
      "Fitting estimator with 312 features.\n",
      "Fitting estimator with 311 features.\n",
      "Fitting estimator with 310 features.\n",
      "Fitting estimator with 309 features.\n",
      "Fitting estimator with 308 features.\n",
      "Fitting estimator with 307 features.\n",
      "Fitting estimator with 306 features.\n",
      "Fitting estimator with 305 features.\n",
      "Fitting estimator with 304 features.\n",
      "Fitting estimator with 303 features.\n",
      "Fitting estimator with 302 features.\n",
      "Fitting estimator with 301 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 299 features.\n",
      "Fitting estimator with 298 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 296 features.\n",
      "Fitting estimator with 295 features.\n",
      "Fitting estimator with 294 features.\n",
      "Fitting estimator with 293 features.\n",
      "Fitting estimator with 292 features.\n",
      "Fitting estimator with 291 features.\n",
      "Fitting estimator with 290 features.\n",
      "Fitting estimator with 289 features.\n",
      "Fitting estimator with 288 features.\n",
      "Fitting estimator with 287 features.\n",
      "Fitting estimator with 286 features.\n",
      "Fitting estimator with 285 features.\n",
      "Fitting estimator with 284 features.\n",
      "Fitting estimator with 283 features.\n",
      "Fitting estimator with 282 features.\n",
      "Fitting estimator with 281 features.\n",
      "Fitting estimator with 280 features.\n",
      "Fitting estimator with 279 features.\n",
      "Fitting estimator with 278 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 276 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 274 features.\n",
      "Fitting estimator with 273 features.\n",
      "Fitting estimator with 272 features.\n",
      "Fitting estimator with 271 features.\n",
      "Fitting estimator with 270 features.\n",
      "Fitting estimator with 269 features.\n",
      "Fitting estimator with 268 features.\n",
      "Fitting estimator with 267 features.\n",
      "Fitting estimator with 266 features.\n",
      "Fitting estimator with 265 features.\n",
      "Fitting estimator with 264 features.\n",
      "Fitting estimator with 263 features.\n",
      "Fitting estimator with 262 features.\n",
      "Fitting estimator with 261 features.\n",
      "Fitting estimator with 260 features.\n",
      "Fitting estimator with 259 features.\n",
      "Fitting estimator with 258 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 256 features.\n",
      "Fitting estimator with 255 features.\n",
      "Fitting estimator with 254 features.\n",
      "Fitting estimator with 253 features.\n",
      "Fitting estimator with 252 features.\n",
      "Fitting estimator with 251 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 249 features.\n",
      "Fitting estimator with 248 features.\n",
      "Fitting estimator with 247 features.\n",
      "Fitting estimator with 246 features.\n",
      "Fitting estimator with 245 features.\n",
      "Fitting estimator with 244 features.\n",
      "Fitting estimator with 243 features.\n",
      "Fitting estimator with 242 features.\n",
      "Fitting estimator with 241 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 239 features.\n",
      "Fitting estimator with 238 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 236 features.\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 234 features.\n",
      "Fitting estimator with 233 features.\n",
      "Fitting estimator with 232 features.\n",
      "Fitting estimator with 231 features.\n",
      "Fitting estimator with 230 features.\n",
      "Fitting estimator with 229 features.\n",
      "Fitting estimator with 228 features.\n",
      "Fitting estimator with 227 features.\n",
      "Fitting estimator with 226 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 224 features.\n",
      "Fitting estimator with 223 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 221 features.\n",
      "Fitting estimator with 220 features.\n",
      "Fitting estimator with 219 features.\n",
      "Fitting estimator with 218 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 216 features.\n",
      "Fitting estimator with 215 features.\n",
      "Fitting estimator with 214 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 211 features.\n",
      "Fitting estimator with 210 features.\n",
      "Fitting estimator with 209 features.\n",
      "Fitting estimator with 208 features.\n",
      "Fitting estimator with 207 features.\n",
      "Fitting estimator with 206 features.\n",
      "Fitting estimator with 205 features.\n",
      "Fitting estimator with 204 features.\n",
      "Fitting estimator with 203 features.\n",
      "Fitting estimator with 202 features.\n",
      "Fitting estimator with 201 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 199 features.\n",
      "Fitting estimator with 198 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 196 features.\n",
      "Fitting estimator with 195 features.\n",
      "Fitting estimator with 194 features.\n",
      "Fitting estimator with 193 features.\n",
      "Fitting estimator with 192 features.\n",
      "Fitting estimator with 191 features.\n",
      "Fitting estimator with 190 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 188 features.\n",
      "Fitting estimator with 187 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 185 features.\n",
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 181 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 179 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 176 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 172 features.\n",
      "Fitting estimator with 171 features.\n",
      "Fitting estimator with 170 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 168 features.\n",
      "Fitting estimator with 167 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 165 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 163 features.\n",
      "Fitting estimator with 162 features.\n",
      "Fitting estimator with 161 features.\n",
      "Fitting estimator with 160 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 158 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 156 features.\n",
      "Fitting estimator with 155 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 153 features.\n",
      "Fitting estimator with 152 features.\n",
      "Fitting estimator with 151 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 148 features.\n",
      "Fitting estimator with 147 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 766 features.\n",
      "Fitting estimator with 765 features.\n",
      "Fitting estimator with 764 features.\n",
      "Fitting estimator with 763 features.\n",
      "Fitting estimator with 762 features.\n",
      "Fitting estimator with 761 features.\n",
      "Fitting estimator with 760 features.\n",
      "Fitting estimator with 759 features.\n",
      "Fitting estimator with 758 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 756 features.\n",
      "Fitting estimator with 755 features.\n",
      "Fitting estimator with 754 features.\n",
      "Fitting estimator with 753 features.\n",
      "Fitting estimator with 752 features.\n",
      "Fitting estimator with 751 features.\n",
      "Fitting estimator with 750 features.\n",
      "Fitting estimator with 749 features.\n",
      "Fitting estimator with 748 features.\n",
      "Fitting estimator with 747 features.\n",
      "Fitting estimator with 746 features.\n",
      "Fitting estimator with 745 features.\n",
      "Fitting estimator with 744 features.\n",
      "Fitting estimator with 743 features.\n",
      "Fitting estimator with 742 features.\n",
      "Fitting estimator with 741 features.\n",
      "Fitting estimator with 740 features.\n",
      "Fitting estimator with 739 features.\n",
      "Fitting estimator with 738 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 736 features.\n",
      "Fitting estimator with 735 features.\n",
      "Fitting estimator with 734 features.\n",
      "Fitting estimator with 733 features.\n",
      "Fitting estimator with 732 features.\n",
      "Fitting estimator with 731 features.\n",
      "Fitting estimator with 730 features.\n",
      "Fitting estimator with 729 features.\n",
      "Fitting estimator with 728 features.\n",
      "Fitting estimator with 727 features.\n",
      "Fitting estimator with 726 features.\n",
      "Fitting estimator with 725 features.\n",
      "Fitting estimator with 724 features.\n",
      "Fitting estimator with 723 features.\n",
      "Fitting estimator with 722 features.\n",
      "Fitting estimator with 721 features.\n",
      "Fitting estimator with 720 features.\n",
      "Fitting estimator with 719 features.\n",
      "Fitting estimator with 718 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 716 features.\n",
      "Fitting estimator with 715 features.\n",
      "Fitting estimator with 714 features.\n",
      "Fitting estimator with 713 features.\n",
      "Fitting estimator with 712 features.\n",
      "Fitting estimator with 711 features.\n",
      "Fitting estimator with 710 features.\n",
      "Fitting estimator with 709 features.\n",
      "Fitting estimator with 708 features.\n",
      "Fitting estimator with 707 features.\n",
      "Fitting estimator with 706 features.\n",
      "Fitting estimator with 705 features.\n",
      "Fitting estimator with 704 features.\n",
      "Fitting estimator with 703 features.\n",
      "Fitting estimator with 702 features.\n",
      "Fitting estimator with 701 features.\n",
      "Fitting estimator with 700 features.\n",
      "Fitting estimator with 699 features.\n",
      "Fitting estimator with 698 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 696 features.\n",
      "Fitting estimator with 695 features.\n",
      "Fitting estimator with 694 features.\n",
      "Fitting estimator with 693 features.\n",
      "Fitting estimator with 692 features.\n",
      "Fitting estimator with 691 features.\n",
      "Fitting estimator with 690 features.\n",
      "Fitting estimator with 689 features.\n",
      "Fitting estimator with 688 features.\n",
      "Fitting estimator with 687 features.\n",
      "Fitting estimator with 686 features.\n",
      "Fitting estimator with 685 features.\n",
      "Fitting estimator with 684 features.\n",
      "Fitting estimator with 683 features.\n",
      "Fitting estimator with 682 features.\n",
      "Fitting estimator with 681 features.\n",
      "Fitting estimator with 680 features.\n",
      "Fitting estimator with 679 features.\n",
      "Fitting estimator with 678 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 676 features.\n",
      "Fitting estimator with 675 features.\n",
      "Fitting estimator with 674 features.\n",
      "Fitting estimator with 673 features.\n",
      "Fitting estimator with 672 features.\n",
      "Fitting estimator with 671 features.\n",
      "Fitting estimator with 670 features.\n",
      "Fitting estimator with 669 features.\n",
      "Fitting estimator with 668 features.\n",
      "Fitting estimator with 667 features.\n",
      "Fitting estimator with 666 features.\n",
      "Fitting estimator with 665 features.\n",
      "Fitting estimator with 664 features.\n",
      "Fitting estimator with 663 features.\n",
      "Fitting estimator with 662 features.\n",
      "Fitting estimator with 661 features.\n",
      "Fitting estimator with 660 features.\n",
      "Fitting estimator with 659 features.\n",
      "Fitting estimator with 658 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 656 features.\n",
      "Fitting estimator with 655 features.\n",
      "Fitting estimator with 654 features.\n",
      "Fitting estimator with 653 features.\n",
      "Fitting estimator with 652 features.\n",
      "Fitting estimator with 651 features.\n",
      "Fitting estimator with 650 features.\n",
      "Fitting estimator with 649 features.\n",
      "Fitting estimator with 648 features.\n",
      "Fitting estimator with 647 features.\n",
      "Fitting estimator with 646 features.\n",
      "Fitting estimator with 645 features.\n",
      "Fitting estimator with 644 features.\n",
      "Fitting estimator with 643 features.\n",
      "Fitting estimator with 642 features.\n",
      "Fitting estimator with 641 features.\n",
      "Fitting estimator with 640 features.\n",
      "Fitting estimator with 639 features.\n",
      "Fitting estimator with 638 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 636 features.\n",
      "Fitting estimator with 635 features.\n",
      "Fitting estimator with 634 features.\n",
      "Fitting estimator with 633 features.\n",
      "Fitting estimator with 632 features.\n",
      "Fitting estimator with 631 features.\n",
      "Fitting estimator with 630 features.\n",
      "Fitting estimator with 629 features.\n",
      "Fitting estimator with 628 features.\n",
      "Fitting estimator with 627 features.\n",
      "Fitting estimator with 626 features.\n",
      "Fitting estimator with 625 features.\n",
      "Fitting estimator with 624 features.\n",
      "Fitting estimator with 623 features.\n",
      "Fitting estimator with 622 features.\n",
      "Fitting estimator with 621 features.\n",
      "Fitting estimator with 620 features.\n",
      "Fitting estimator with 619 features.\n",
      "Fitting estimator with 618 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 616 features.\n",
      "Fitting estimator with 615 features.\n",
      "Fitting estimator with 614 features.\n",
      "Fitting estimator with 613 features.\n",
      "Fitting estimator with 612 features.\n",
      "Fitting estimator with 611 features.\n",
      "Fitting estimator with 610 features.\n",
      "Fitting estimator with 609 features.\n",
      "Fitting estimator with 608 features.\n",
      "Fitting estimator with 607 features.\n",
      "Fitting estimator with 606 features.\n",
      "Fitting estimator with 605 features.\n",
      "Fitting estimator with 604 features.\n",
      "Fitting estimator with 603 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 601 features.\n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 599 features.\n",
      "Fitting estimator with 598 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 596 features.\n",
      "Fitting estimator with 595 features.\n",
      "Fitting estimator with 594 features.\n",
      "Fitting estimator with 593 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 591 features.\n",
      "Fitting estimator with 590 features.\n",
      "Fitting estimator with 589 features.\n",
      "Fitting estimator with 588 features.\n",
      "Fitting estimator with 587 features.\n",
      "Fitting estimator with 586 features.\n",
      "Fitting estimator with 585 features.\n",
      "Fitting estimator with 584 features.\n",
      "Fitting estimator with 583 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 581 features.\n",
      "Fitting estimator with 580 features.\n",
      "Fitting estimator with 579 features.\n",
      "Fitting estimator with 578 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 576 features.\n",
      "Fitting estimator with 575 features.\n",
      "Fitting estimator with 574 features.\n",
      "Fitting estimator with 573 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 571 features.\n",
      "Fitting estimator with 570 features.\n",
      "Fitting estimator with 569 features.\n",
      "Fitting estimator with 568 features.\n",
      "Fitting estimator with 567 features.\n",
      "Fitting estimator with 566 features.\n",
      "Fitting estimator with 565 features.\n",
      "Fitting estimator with 564 features.\n",
      "Fitting estimator with 563 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 561 features.\n",
      "Fitting estimator with 560 features.\n",
      "Fitting estimator with 559 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 556 features.\n",
      "Fitting estimator with 555 features.\n",
      "Fitting estimator with 554 features.\n",
      "Fitting estimator with 553 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 551 features.\n",
      "Fitting estimator with 550 features.\n",
      "Fitting estimator with 549 features.\n",
      "Fitting estimator with 548 features.\n",
      "Fitting estimator with 547 features.\n",
      "Fitting estimator with 546 features.\n",
      "Fitting estimator with 545 features.\n",
      "Fitting estimator with 544 features.\n",
      "Fitting estimator with 543 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 541 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 539 features.\n",
      "Fitting estimator with 538 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 536 features.\n",
      "Fitting estimator with 535 features.\n",
      "Fitting estimator with 534 features.\n",
      "Fitting estimator with 533 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 531 features.\n",
      "Fitting estimator with 530 features.\n",
      "Fitting estimator with 529 features.\n",
      "Fitting estimator with 528 features.\n",
      "Fitting estimator with 527 features.\n",
      "Fitting estimator with 526 features.\n",
      "Fitting estimator with 525 features.\n",
      "Fitting estimator with 524 features.\n",
      "Fitting estimator with 523 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 521 features.\n",
      "Fitting estimator with 520 features.\n",
      "Fitting estimator with 519 features.\n",
      "Fitting estimator with 518 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 516 features.\n",
      "Fitting estimator with 515 features.\n",
      "Fitting estimator with 514 features.\n",
      "Fitting estimator with 513 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 511 features.\n",
      "Fitting estimator with 510 features.\n",
      "Fitting estimator with 509 features.\n",
      "Fitting estimator with 508 features.\n",
      "Fitting estimator with 507 features.\n",
      "Fitting estimator with 506 features.\n",
      "Fitting estimator with 505 features.\n",
      "Fitting estimator with 504 features.\n",
      "Fitting estimator with 503 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 501 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "Fitting estimator with 390 features.\n",
      "Fitting estimator with 389 features.\n",
      "Fitting estimator with 388 features.\n",
      "Fitting estimator with 387 features.\n",
      "Fitting estimator with 386 features.\n",
      "Fitting estimator with 385 features.\n",
      "Fitting estimator with 384 features.\n",
      "Fitting estimator with 383 features.\n",
      "Fitting estimator with 382 features.\n",
      "Fitting estimator with 381 features.\n",
      "Fitting estimator with 380 features.\n",
      "Fitting estimator with 379 features.\n",
      "Fitting estimator with 378 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 376 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 374 features.\n",
      "Fitting estimator with 373 features.\n",
      "Fitting estimator with 372 features.\n",
      "Fitting estimator with 371 features.\n",
      "Fitting estimator with 370 features.\n",
      "Fitting estimator with 369 features.\n",
      "Fitting estimator with 368 features.\n",
      "Fitting estimator with 367 features.\n",
      "Fitting estimator with 366 features.\n",
      "Fitting estimator with 365 features.\n",
      "Fitting estimator with 364 features.\n",
      "Fitting estimator with 363 features.\n",
      "Fitting estimator with 362 features.\n",
      "Fitting estimator with 361 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 359 features.\n",
      "Fitting estimator with 358 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 356 features.\n",
      "Fitting estimator with 355 features.\n",
      "Fitting estimator with 354 features.\n",
      "Fitting estimator with 353 features.\n",
      "Fitting estimator with 352 features.\n",
      "Fitting estimator with 351 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 349 features.\n",
      "Fitting estimator with 348 features.\n",
      "Fitting estimator with 347 features.\n",
      "Fitting estimator with 346 features.\n",
      "Fitting estimator with 345 features.\n",
      "Fitting estimator with 344 features.\n",
      "Fitting estimator with 343 features.\n",
      "Fitting estimator with 342 features.\n",
      "Fitting estimator with 341 features.\n",
      "Fitting estimator with 340 features.\n",
      "Fitting estimator with 339 features.\n",
      "Fitting estimator with 338 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 336 features.\n",
      "Fitting estimator with 335 features.\n",
      "Fitting estimator with 334 features.\n",
      "Fitting estimator with 333 features.\n",
      "Fitting estimator with 332 features.\n",
      "Fitting estimator with 331 features.\n",
      "Fitting estimator with 330 features.\n",
      "Fitting estimator with 329 features.\n",
      "Fitting estimator with 328 features.\n",
      "Fitting estimator with 327 features.\n",
      "Fitting estimator with 326 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 324 features.\n",
      "Fitting estimator with 323 features.\n",
      "Fitting estimator with 322 features.\n",
      "Fitting estimator with 321 features.\n",
      "Fitting estimator with 320 features.\n",
      "Fitting estimator with 319 features.\n",
      "Fitting estimator with 318 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 316 features.\n",
      "Fitting estimator with 315 features.\n",
      "Fitting estimator with 314 features.\n",
      "Fitting estimator with 313 features.\n",
      "Fitting estimator with 312 features.\n",
      "Fitting estimator with 311 features.\n",
      "Fitting estimator with 310 features.\n",
      "Fitting estimator with 309 features.\n",
      "Fitting estimator with 308 features.\n",
      "Fitting estimator with 307 features.\n",
      "Fitting estimator with 306 features.\n",
      "Fitting estimator with 305 features.\n",
      "Fitting estimator with 304 features.\n",
      "Fitting estimator with 303 features.\n",
      "Fitting estimator with 302 features.\n",
      "Fitting estimator with 301 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 299 features.\n",
      "Fitting estimator with 298 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 296 features.\n",
      "Fitting estimator with 295 features.\n",
      "Fitting estimator with 294 features.\n",
      "Fitting estimator with 293 features.\n",
      "Fitting estimator with 292 features.\n",
      "Fitting estimator with 291 features.\n",
      "Fitting estimator with 290 features.\n",
      "Fitting estimator with 289 features.\n",
      "Fitting estimator with 288 features.\n",
      "Fitting estimator with 287 features.\n",
      "Fitting estimator with 286 features.\n",
      "Fitting estimator with 285 features.\n",
      "Fitting estimator with 284 features.\n",
      "Fitting estimator with 283 features.\n",
      "Fitting estimator with 282 features.\n",
      "Fitting estimator with 281 features.\n",
      "Fitting estimator with 280 features.\n",
      "Fitting estimator with 279 features.\n",
      "Fitting estimator with 278 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 276 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 274 features.\n",
      "Fitting estimator with 273 features.\n",
      "Fitting estimator with 272 features.\n",
      "Fitting estimator with 271 features.\n",
      "Fitting estimator with 270 features.\n",
      "Fitting estimator with 269 features.\n",
      "Fitting estimator with 268 features.\n",
      "Fitting estimator with 267 features.\n",
      "Fitting estimator with 266 features.\n",
      "Fitting estimator with 265 features.\n",
      "Fitting estimator with 264 features.\n",
      "Fitting estimator with 263 features.\n",
      "Fitting estimator with 262 features.\n",
      "Fitting estimator with 261 features.\n",
      "Fitting estimator with 260 features.\n",
      "Fitting estimator with 259 features.\n",
      "Fitting estimator with 258 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 256 features.\n",
      "Fitting estimator with 255 features.\n",
      "Fitting estimator with 254 features.\n",
      "Fitting estimator with 253 features.\n",
      "Fitting estimator with 252 features.\n",
      "Fitting estimator with 251 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 249 features.\n",
      "Fitting estimator with 248 features.\n",
      "Fitting estimator with 247 features.\n",
      "Fitting estimator with 246 features.\n",
      "Fitting estimator with 245 features.\n",
      "Fitting estimator with 244 features.\n",
      "Fitting estimator with 243 features.\n",
      "Fitting estimator with 242 features.\n",
      "Fitting estimator with 241 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 239 features.\n",
      "Fitting estimator with 238 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 236 features.\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 234 features.\n",
      "Fitting estimator with 233 features.\n",
      "Fitting estimator with 232 features.\n",
      "Fitting estimator with 231 features.\n",
      "Fitting estimator with 230 features.\n",
      "Fitting estimator with 229 features.\n",
      "Fitting estimator with 228 features.\n",
      "Fitting estimator with 227 features.\n",
      "Fitting estimator with 226 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 224 features.\n",
      "Fitting estimator with 223 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 221 features.\n",
      "Fitting estimator with 220 features.\n",
      "Fitting estimator with 219 features.\n",
      "Fitting estimator with 218 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 216 features.\n",
      "Fitting estimator with 215 features.\n",
      "Fitting estimator with 214 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 211 features.\n",
      "Fitting estimator with 210 features.\n",
      "Fitting estimator with 209 features.\n",
      "Fitting estimator with 208 features.\n",
      "Fitting estimator with 207 features.\n",
      "Fitting estimator with 206 features.\n",
      "Fitting estimator with 205 features.\n",
      "Fitting estimator with 204 features.\n",
      "Fitting estimator with 203 features.\n",
      "Fitting estimator with 202 features.\n",
      "Fitting estimator with 201 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 199 features.\n",
      "Fitting estimator with 198 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 196 features.\n",
      "Fitting estimator with 195 features.\n",
      "Fitting estimator with 194 features.\n",
      "Fitting estimator with 193 features.\n",
      "Fitting estimator with 192 features.\n",
      "Fitting estimator with 191 features.\n",
      "Fitting estimator with 190 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 188 features.\n",
      "Fitting estimator with 187 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 185 features.\n",
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 181 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 179 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 176 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 172 features.\n",
      "Fitting estimator with 171 features.\n",
      "Fitting estimator with 170 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 168 features.\n",
      "Fitting estimator with 167 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 165 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 163 features.\n",
      "Fitting estimator with 162 features.\n",
      "Fitting estimator with 161 features.\n",
      "Fitting estimator with 160 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 158 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 156 features.\n",
      "Fitting estimator with 155 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 153 features.\n",
      "Fitting estimator with 152 features.\n",
      "Fitting estimator with 151 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 148 features.\n",
      "Fitting estimator with 147 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 766 features.\n",
      "Fitting estimator with 765 features.\n",
      "Fitting estimator with 764 features.\n",
      "Fitting estimator with 763 features.\n",
      "Fitting estimator with 762 features.\n",
      "Fitting estimator with 761 features.\n",
      "Fitting estimator with 760 features.\n",
      "Fitting estimator with 759 features.\n",
      "Fitting estimator with 758 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 756 features.\n",
      "Fitting estimator with 755 features.\n",
      "Fitting estimator with 754 features.\n",
      "Fitting estimator with 753 features.\n",
      "Fitting estimator with 752 features.\n",
      "Fitting estimator with 751 features.\n",
      "Fitting estimator with 750 features.\n",
      "Fitting estimator with 749 features.\n",
      "Fitting estimator with 748 features.\n",
      "Fitting estimator with 747 features.\n",
      "Fitting estimator with 746 features.\n",
      "Fitting estimator with 745 features.\n",
      "Fitting estimator with 744 features.\n",
      "Fitting estimator with 743 features.\n",
      "Fitting estimator with 742 features.\n",
      "Fitting estimator with 741 features.\n",
      "Fitting estimator with 740 features.\n",
      "Fitting estimator with 739 features.\n",
      "Fitting estimator with 738 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 736 features.\n",
      "Fitting estimator with 735 features.\n",
      "Fitting estimator with 734 features.\n",
      "Fitting estimator with 733 features.\n",
      "Fitting estimator with 732 features.\n",
      "Fitting estimator with 731 features.\n",
      "Fitting estimator with 730 features.\n",
      "Fitting estimator with 729 features.\n",
      "Fitting estimator with 728 features.\n",
      "Fitting estimator with 727 features.\n",
      "Fitting estimator with 726 features.\n",
      "Fitting estimator with 725 features.\n",
      "Fitting estimator with 724 features.\n",
      "Fitting estimator with 723 features.\n",
      "Fitting estimator with 722 features.\n",
      "Fitting estimator with 721 features.\n",
      "Fitting estimator with 720 features.\n",
      "Fitting estimator with 719 features.\n",
      "Fitting estimator with 718 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 716 features.\n",
      "Fitting estimator with 715 features.\n",
      "Fitting estimator with 714 features.\n",
      "Fitting estimator with 713 features.\n",
      "Fitting estimator with 712 features.\n",
      "Fitting estimator with 711 features.\n",
      "Fitting estimator with 710 features.\n",
      "Fitting estimator with 709 features.\n",
      "Fitting estimator with 708 features.\n",
      "Fitting estimator with 707 features.\n",
      "Fitting estimator with 706 features.\n",
      "Fitting estimator with 705 features.\n",
      "Fitting estimator with 704 features.\n",
      "Fitting estimator with 703 features.\n",
      "Fitting estimator with 702 features.\n",
      "Fitting estimator with 701 features.\n",
      "Fitting estimator with 700 features.\n",
      "Fitting estimator with 699 features.\n",
      "Fitting estimator with 698 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 696 features.\n",
      "Fitting estimator with 695 features.\n",
      "Fitting estimator with 694 features.\n",
      "Fitting estimator with 693 features.\n",
      "Fitting estimator with 692 features.\n",
      "Fitting estimator with 691 features.\n",
      "Fitting estimator with 690 features.\n",
      "Fitting estimator with 689 features.\n",
      "Fitting estimator with 688 features.\n",
      "Fitting estimator with 687 features.\n",
      "Fitting estimator with 686 features.\n",
      "Fitting estimator with 685 features.\n",
      "Fitting estimator with 684 features.\n",
      "Fitting estimator with 683 features.\n",
      "Fitting estimator with 682 features.\n",
      "Fitting estimator with 681 features.\n",
      "Fitting estimator with 680 features.\n",
      "Fitting estimator with 679 features.\n",
      "Fitting estimator with 678 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 676 features.\n",
      "Fitting estimator with 675 features.\n",
      "Fitting estimator with 674 features.\n",
      "Fitting estimator with 673 features.\n",
      "Fitting estimator with 672 features.\n",
      "Fitting estimator with 671 features.\n",
      "Fitting estimator with 670 features.\n",
      "Fitting estimator with 669 features.\n",
      "Fitting estimator with 668 features.\n",
      "Fitting estimator with 667 features.\n",
      "Fitting estimator with 666 features.\n",
      "Fitting estimator with 665 features.\n",
      "Fitting estimator with 664 features.\n",
      "Fitting estimator with 663 features.\n",
      "Fitting estimator with 662 features.\n",
      "Fitting estimator with 661 features.\n",
      "Fitting estimator with 660 features.\n",
      "Fitting estimator with 659 features.\n",
      "Fitting estimator with 658 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 656 features.\n",
      "Fitting estimator with 655 features.\n",
      "Fitting estimator with 654 features.\n",
      "Fitting estimator with 653 features.\n",
      "Fitting estimator with 652 features.\n",
      "Fitting estimator with 651 features.\n",
      "Fitting estimator with 650 features.\n",
      "Fitting estimator with 649 features.\n",
      "Fitting estimator with 648 features.\n",
      "Fitting estimator with 647 features.\n",
      "Fitting estimator with 646 features.\n",
      "Fitting estimator with 645 features.\n",
      "Fitting estimator with 644 features.\n",
      "Fitting estimator with 643 features.\n",
      "Fitting estimator with 642 features.\n",
      "Fitting estimator with 641 features.\n",
      "Fitting estimator with 640 features.\n",
      "Fitting estimator with 639 features.\n",
      "Fitting estimator with 638 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 636 features.\n",
      "Fitting estimator with 635 features.\n",
      "Fitting estimator with 634 features.\n",
      "Fitting estimator with 633 features.\n",
      "Fitting estimator with 632 features.\n",
      "Fitting estimator with 631 features.\n",
      "Fitting estimator with 630 features.\n",
      "Fitting estimator with 629 features.\n",
      "Fitting estimator with 628 features.\n",
      "Fitting estimator with 627 features.\n",
      "Fitting estimator with 626 features.\n",
      "Fitting estimator with 625 features.\n",
      "Fitting estimator with 624 features.\n",
      "Fitting estimator with 623 features.\n",
      "Fitting estimator with 622 features.\n",
      "Fitting estimator with 621 features.\n",
      "Fitting estimator with 620 features.\n",
      "Fitting estimator with 619 features.\n",
      "Fitting estimator with 618 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 616 features.\n",
      "Fitting estimator with 615 features.\n",
      "Fitting estimator with 614 features.\n",
      "Fitting estimator with 613 features.\n",
      "Fitting estimator with 612 features.\n",
      "Fitting estimator with 611 features.\n",
      "Fitting estimator with 610 features.\n",
      "Fitting estimator with 609 features.\n",
      "Fitting estimator with 608 features.\n",
      "Fitting estimator with 607 features.\n",
      "Fitting estimator with 606 features.\n",
      "Fitting estimator with 605 features.\n",
      "Fitting estimator with 604 features.\n",
      "Fitting estimator with 603 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 601 features.\n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 599 features.\n",
      "Fitting estimator with 598 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 596 features.\n",
      "Fitting estimator with 595 features.\n",
      "Fitting estimator with 594 features.\n",
      "Fitting estimator with 593 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 591 features.\n",
      "Fitting estimator with 590 features.\n",
      "Fitting estimator with 589 features.\n",
      "Fitting estimator with 588 features.\n",
      "Fitting estimator with 587 features.\n",
      "Fitting estimator with 586 features.\n",
      "Fitting estimator with 585 features.\n",
      "Fitting estimator with 584 features.\n",
      "Fitting estimator with 583 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 581 features.\n",
      "Fitting estimator with 580 features.\n",
      "Fitting estimator with 579 features.\n",
      "Fitting estimator with 578 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 576 features.\n",
      "Fitting estimator with 575 features.\n",
      "Fitting estimator with 574 features.\n",
      "Fitting estimator with 573 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 571 features.\n",
      "Fitting estimator with 570 features.\n",
      "Fitting estimator with 569 features.\n",
      "Fitting estimator with 568 features.\n",
      "Fitting estimator with 567 features.\n",
      "Fitting estimator with 566 features.\n",
      "Fitting estimator with 565 features.\n",
      "Fitting estimator with 564 features.\n",
      "Fitting estimator with 563 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 561 features.\n",
      "Fitting estimator with 560 features.\n",
      "Fitting estimator with 559 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 556 features.\n",
      "Fitting estimator with 555 features.\n",
      "Fitting estimator with 554 features.\n",
      "Fitting estimator with 553 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 551 features.\n",
      "Fitting estimator with 550 features.\n",
      "Fitting estimator with 549 features.\n",
      "Fitting estimator with 548 features.\n",
      "Fitting estimator with 547 features.\n",
      "Fitting estimator with 546 features.\n",
      "Fitting estimator with 545 features.\n",
      "Fitting estimator with 544 features.\n",
      "Fitting estimator with 543 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 541 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 539 features.\n",
      "Fitting estimator with 538 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 536 features.\n",
      "Fitting estimator with 535 features.\n",
      "Fitting estimator with 534 features.\n",
      "Fitting estimator with 533 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 531 features.\n",
      "Fitting estimator with 530 features.\n",
      "Fitting estimator with 529 features.\n",
      "Fitting estimator with 528 features.\n",
      "Fitting estimator with 527 features.\n",
      "Fitting estimator with 526 features.\n",
      "Fitting estimator with 525 features.\n",
      "Fitting estimator with 524 features.\n",
      "Fitting estimator with 523 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 521 features.\n",
      "Fitting estimator with 520 features.\n",
      "Fitting estimator with 519 features.\n",
      "Fitting estimator with 518 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 516 features.\n",
      "Fitting estimator with 515 features.\n",
      "Fitting estimator with 514 features.\n",
      "Fitting estimator with 513 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 511 features.\n",
      "Fitting estimator with 510 features.\n",
      "Fitting estimator with 509 features.\n",
      "Fitting estimator with 508 features.\n",
      "Fitting estimator with 507 features.\n",
      "Fitting estimator with 506 features.\n",
      "Fitting estimator with 505 features.\n",
      "Fitting estimator with 504 features.\n",
      "Fitting estimator with 503 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 501 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "Fitting estimator with 390 features.\n",
      "Fitting estimator with 389 features.\n",
      "Fitting estimator with 388 features.\n",
      "Fitting estimator with 387 features.\n",
      "Fitting estimator with 386 features.\n",
      "Fitting estimator with 385 features.\n",
      "Fitting estimator with 384 features.\n",
      "Fitting estimator with 383 features.\n",
      "Fitting estimator with 382 features.\n",
      "Fitting estimator with 381 features.\n",
      "Fitting estimator with 380 features.\n",
      "Fitting estimator with 379 features.\n",
      "Fitting estimator with 378 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 376 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 374 features.\n",
      "Fitting estimator with 373 features.\n",
      "Fitting estimator with 372 features.\n",
      "Fitting estimator with 371 features.\n",
      "Fitting estimator with 370 features.\n",
      "Fitting estimator with 369 features.\n",
      "Fitting estimator with 368 features.\n",
      "Fitting estimator with 367 features.\n",
      "Fitting estimator with 366 features.\n",
      "Fitting estimator with 365 features.\n",
      "Fitting estimator with 364 features.\n",
      "Fitting estimator with 363 features.\n",
      "Fitting estimator with 362 features.\n",
      "Fitting estimator with 361 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 359 features.\n",
      "Fitting estimator with 358 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 356 features.\n",
      "Fitting estimator with 355 features.\n",
      "Fitting estimator with 354 features.\n",
      "Fitting estimator with 353 features.\n",
      "Fitting estimator with 352 features.\n",
      "Fitting estimator with 351 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 349 features.\n",
      "Fitting estimator with 348 features.\n",
      "Fitting estimator with 347 features.\n",
      "Fitting estimator with 346 features.\n",
      "Fitting estimator with 345 features.\n",
      "Fitting estimator with 344 features.\n",
      "Fitting estimator with 343 features.\n",
      "Fitting estimator with 342 features.\n",
      "Fitting estimator with 341 features.\n",
      "Fitting estimator with 340 features.\n",
      "Fitting estimator with 339 features.\n",
      "Fitting estimator with 338 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 336 features.\n",
      "Fitting estimator with 335 features.\n",
      "Fitting estimator with 334 features.\n",
      "Fitting estimator with 333 features.\n",
      "Fitting estimator with 332 features.\n",
      "Fitting estimator with 331 features.\n",
      "Fitting estimator with 330 features.\n",
      "Fitting estimator with 329 features.\n",
      "Fitting estimator with 328 features.\n",
      "Fitting estimator with 327 features.\n",
      "Fitting estimator with 326 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 324 features.\n",
      "Fitting estimator with 323 features.\n",
      "Fitting estimator with 322 features.\n",
      "Fitting estimator with 321 features.\n",
      "Fitting estimator with 320 features.\n",
      "Fitting estimator with 319 features.\n",
      "Fitting estimator with 318 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 316 features.\n",
      "Fitting estimator with 315 features.\n",
      "Fitting estimator with 314 features.\n",
      "Fitting estimator with 313 features.\n",
      "Fitting estimator with 312 features.\n",
      "Fitting estimator with 311 features.\n",
      "Fitting estimator with 310 features.\n",
      "Fitting estimator with 309 features.\n",
      "Fitting estimator with 308 features.\n",
      "Fitting estimator with 307 features.\n",
      "Fitting estimator with 306 features.\n",
      "Fitting estimator with 305 features.\n",
      "Fitting estimator with 304 features.\n",
      "Fitting estimator with 303 features.\n",
      "Fitting estimator with 302 features.\n",
      "Fitting estimator with 301 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 299 features.\n",
      "Fitting estimator with 298 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 296 features.\n",
      "Fitting estimator with 295 features.\n",
      "Fitting estimator with 294 features.\n",
      "Fitting estimator with 293 features.\n",
      "Fitting estimator with 292 features.\n",
      "Fitting estimator with 291 features.\n",
      "Fitting estimator with 290 features.\n",
      "Fitting estimator with 289 features.\n",
      "Fitting estimator with 288 features.\n",
      "Fitting estimator with 287 features.\n",
      "Fitting estimator with 286 features.\n",
      "Fitting estimator with 285 features.\n",
      "Fitting estimator with 284 features.\n",
      "Fitting estimator with 283 features.\n",
      "Fitting estimator with 282 features.\n",
      "Fitting estimator with 281 features.\n",
      "Fitting estimator with 280 features.\n",
      "Fitting estimator with 279 features.\n",
      "Fitting estimator with 278 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 276 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 274 features.\n",
      "Fitting estimator with 273 features.\n",
      "Fitting estimator with 272 features.\n",
      "Fitting estimator with 271 features.\n",
      "Fitting estimator with 270 features.\n",
      "Fitting estimator with 269 features.\n",
      "Fitting estimator with 268 features.\n",
      "Fitting estimator with 267 features.\n",
      "Fitting estimator with 266 features.\n",
      "Fitting estimator with 265 features.\n",
      "Fitting estimator with 264 features.\n",
      "Fitting estimator with 263 features.\n",
      "Fitting estimator with 262 features.\n",
      "Fitting estimator with 261 features.\n",
      "Fitting estimator with 260 features.\n",
      "Fitting estimator with 259 features.\n",
      "Fitting estimator with 258 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 256 features.\n",
      "Fitting estimator with 255 features.\n",
      "Fitting estimator with 254 features.\n",
      "Fitting estimator with 253 features.\n",
      "Fitting estimator with 252 features.\n",
      "Fitting estimator with 251 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 249 features.\n",
      "Fitting estimator with 248 features.\n",
      "Fitting estimator with 247 features.\n",
      "Fitting estimator with 246 features.\n",
      "Fitting estimator with 245 features.\n",
      "Fitting estimator with 244 features.\n",
      "Fitting estimator with 243 features.\n",
      "Fitting estimator with 242 features.\n",
      "Fitting estimator with 241 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 239 features.\n",
      "Fitting estimator with 238 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 236 features.\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 234 features.\n",
      "Fitting estimator with 233 features.\n",
      "Fitting estimator with 232 features.\n",
      "Fitting estimator with 231 features.\n",
      "Fitting estimator with 230 features.\n",
      "Fitting estimator with 229 features.\n",
      "Fitting estimator with 228 features.\n",
      "Fitting estimator with 227 features.\n",
      "Fitting estimator with 226 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 224 features.\n",
      "Fitting estimator with 223 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 221 features.\n",
      "Fitting estimator with 220 features.\n",
      "Fitting estimator with 219 features.\n",
      "Fitting estimator with 218 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 216 features.\n",
      "Fitting estimator with 215 features.\n",
      "Fitting estimator with 214 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 211 features.\n",
      "Fitting estimator with 210 features.\n",
      "Fitting estimator with 209 features.\n",
      "Fitting estimator with 208 features.\n",
      "Fitting estimator with 207 features.\n",
      "Fitting estimator with 206 features.\n",
      "Fitting estimator with 205 features.\n",
      "Fitting estimator with 204 features.\n",
      "Fitting estimator with 203 features.\n",
      "Fitting estimator with 202 features.\n",
      "Fitting estimator with 201 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 199 features.\n",
      "Fitting estimator with 198 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 196 features.\n",
      "Fitting estimator with 195 features.\n",
      "Fitting estimator with 194 features.\n",
      "Fitting estimator with 193 features.\n",
      "Fitting estimator with 192 features.\n",
      "Fitting estimator with 191 features.\n",
      "Fitting estimator with 190 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 188 features.\n",
      "Fitting estimator with 187 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 185 features.\n",
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 181 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 179 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 176 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 172 features.\n",
      "Fitting estimator with 171 features.\n",
      "Fitting estimator with 170 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 168 features.\n",
      "Fitting estimator with 167 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 165 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 163 features.\n",
      "Fitting estimator with 162 features.\n",
      "Fitting estimator with 161 features.\n",
      "Fitting estimator with 160 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 158 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 156 features.\n",
      "Fitting estimator with 155 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 153 features.\n",
      "Fitting estimator with 152 features.\n",
      "Fitting estimator with 151 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 148 features.\n",
      "Fitting estimator with 147 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Done with LGBMRegressor\n",
      "\n",
      "CPU times: total: 1d 7h 28min 11s\n",
      "Wall time: 6h\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize empty dictionary for RFECV features\n",
    "rfecv_features = {}\n",
    "\n",
    "for alg in models:\n",
    "    # set name\n",
    "    MLA_name = alg.__class__.__name__\n",
    "\t\t\n",
    "    features = baseline_features[MLA_name]\n",
    "\n",
    "    # incase there is no feature that had importance, go to the next model\n",
    "    if len(features) == 0:\n",
    "        continue\n",
    "\t\n",
    "    X_rfecv = X[features]\n",
    "\n",
    "    try:\n",
    "        print(f'Starting with {MLA_name}')\n",
    "        # Create the RFECV object and rank each feature\n",
    "        selector = RFECV(alg, cv=sk10, step=1, scoring=rmsle_scorer, verbose=2)\n",
    "        selector = selector.fit(X_rfecv, y)\n",
    "\n",
    "        selected_features = list(X_rfecv.columns[selector.support_])\n",
    "\n",
    "        # Reorder selected_features based on the predefined features_list\n",
    "        selected_features_ordered = [feat for feat in features_list if feat in selected_features]\n",
    "\n",
    "        rfecv_features[MLA_name] = selected_features_ordered\n",
    "\n",
    "        print(f'Done with {MLA_name}', end='\\n\\n')\n",
    "    \n",
    "    except ValueError:\n",
    "        # In case of an error, keep the original order but filtered by features_list\n",
    "        features_filtered = [feat for feat in features_list if feat in features]\n",
    "        rfecv_features[MLA_name] = features_filtered\n",
    "        print(f'{MLA_name} does not have coef_ or feature_importances_', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rfecv_features_lgbm.txt', mode='w') as f:\n",
    "    pprint(rfecv_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor.\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 6.09 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC Std</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.142496</td>\n",
       "      <td>0.151087</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0 min 4.24 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MLA Name                                     MLA Parameters  \\\n",
       "0  LGBMRegressor  {'boosting_type': 'gbdt', 'class_weight': None...   \n",
       "\n",
       "   MLA Train ROC AUC  MLA Test ROC AUC  MLA Test ROC AUC Std        MLA Time  \n",
       "0           0.142496          0.151087              0.000163  0 min 4.24 sec  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "rfecv_models = evaluate_models(models, X, y, rfecv_features, sk10, f'{experiment_name}_rfecv')\n",
    "rfecv_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rfecv_features['LGBMRegressor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running backward feature selection with LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:  2.4min finished\n",
      "\n",
      "[2024-04-18 01:43:10] Features: 55/1 -- score: -0.15096301164286163[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:  2.0min finished\n",
      "\n",
      "[2024-04-18 01:45:13] Features: 54/1 -- score: -0.15094786311266192[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  2.2min finished\n",
      "\n",
      "[2024-04-18 01:47:25] Features: 53/1 -- score: -0.15094311626577414[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:  2.1min finished\n",
      "\n",
      "[2024-04-18 01:49:30] Features: 52/1 -- score: -0.15097023802750043[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:  1.9min finished\n",
      "\n",
      "[2024-04-18 01:51:27] Features: 51/1 -- score: -0.15096357749038536[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:  2.1min finished\n",
      "\n",
      "[2024-04-18 01:53:36] Features: 50/1 -- score: -0.15088468131535213[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.8min finished\n",
      "\n",
      "[2024-04-18 01:55:27] Features: 49/1 -- score: -0.15092786605722172[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  49 | elapsed:  1.8min finished\n",
      "\n",
      "[2024-04-18 01:57:13] Features: 48/1 -- score: -0.1509444790227719[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  1.6min finished\n",
      "\n",
      "[2024-04-18 01:58:50] Features: 47/1 -- score: -0.15087343717553262[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  47 | elapsed:  1.6min finished\n",
      "\n",
      "[2024-04-18 02:00:25] Features: 46/1 -- score: -0.15091353385273787[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  46 | elapsed:  1.4min finished\n",
      "\n",
      "[2024-04-18 02:01:50] Features: 45/1 -- score: -0.15081006767306912[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  1.4min finished\n",
      "\n",
      "[2024-04-18 02:03:16] Features: 44/1 -- score: -0.15083704466558365[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   56.5s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:  1.4min finished\n",
      "\n",
      "[2024-04-18 02:04:37] Features: 43/1 -- score: -0.15082325070753805[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  43 | elapsed:  1.6min finished\n",
      "\n",
      "[2024-04-18 02:06:13] Features: 42/1 -- score: -0.1508260425300195[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:  1.3min finished\n",
      "\n",
      "[2024-04-18 02:07:31] Features: 41/1 -- score: -0.1507982851963856[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  41 | elapsed:  1.1min finished\n",
      "\n",
      "[2024-04-18 02:08:39] Features: 40/1 -- score: -0.15080410881162923[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.0min finished\n",
      "\n",
      "[2024-04-18 02:09:42] Features: 39/1 -- score: -0.15083352697184746[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:  1.0min finished\n",
      "\n",
      "[2024-04-18 02:10:43] Features: 38/1 -- score: -0.15083812006557792[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  38 | elapsed:   57.6s finished\n",
      "\n",
      "[2024-04-18 02:11:41] Features: 37/1 -- score: -0.1508124830435062[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  37 | elapsed:   54.1s finished\n",
      "\n",
      "[2024-04-18 02:12:36] Features: 36/1 -- score: -0.15085787602772963[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   50.2s finished\n",
      "\n",
      "[2024-04-18 02:13:26] Features: 35/1 -- score: -0.15080613259170497[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:   49.5s finished\n",
      "\n",
      "[2024-04-18 02:14:16] Features: 34/1 -- score: -0.15083055380017563[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:   46.8s finished\n",
      "\n",
      "[2024-04-18 02:15:03] Features: 33/1 -- score: -0.1508263313737164[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:   44.8s finished\n",
      "\n",
      "[2024-04-18 02:15:48] Features: 32/1 -- score: -0.15083194763536448[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:   41.6s finished\n",
      "\n",
      "[2024-04-18 02:16:30] Features: 31/1 -- score: -0.1508604812411852[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:   38.4s finished\n",
      "\n",
      "[2024-04-18 02:17:09] Features: 30/1 -- score: -0.1508212843630036[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   35.3s finished\n",
      "\n",
      "[2024-04-18 02:17:45] Features: 29/1 -- score: -0.15080110574071284[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:   34.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:   34.5s finished\n",
      "\n",
      "[2024-04-18 02:18:19] Features: 28/1 -- score: -0.15080633409434932[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:   33.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:   33.9s finished\n",
      "\n",
      "[2024-04-18 02:18:54] Features: 27/1 -- score: -0.1507439249242363[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:   31.7s finished\n",
      "\n",
      "[2024-04-18 02:19:26] Features: 26/1 -- score: -0.15071166661307908[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:   29.4s finished\n",
      "\n",
      "[2024-04-18 02:19:55] Features: 25/1 -- score: -0.15072455330782117[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:   24.6s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   27.2s finished\n",
      "\n",
      "[2024-04-18 02:20:23] Features: 24/1 -- score: -0.1507091453442101[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:   24.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   24.1s finished\n",
      "\n",
      "[2024-04-18 02:20:47] Features: 23/1 -- score: -0.15071882106844495[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  23 | elapsed:   22.2s remaining:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:   22.5s finished\n",
      "\n",
      "[2024-04-18 02:21:10] Features: 22/1 -- score: -0.15067441916525023[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  22 | elapsed:   20.5s remaining:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:   20.8s finished\n",
      "\n",
      "[2024-04-18 02:21:31] Features: 21/1 -- score: -0.1506436512938616[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  21 | elapsed:   18.8s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:   19.2s finished\n",
      "\n",
      "[2024-04-18 02:21:51] Features: 20/1 -- score: -0.150591409582113[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:   13.9s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   18.1s finished\n",
      "\n",
      "[2024-04-18 02:22:09] Features: 19/1 -- score: -0.1506608932238397[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  19 | elapsed:   13.3s remaining:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:   17.0s finished\n",
      "\n",
      "[2024-04-18 02:22:27] Features: 18/1 -- score: -0.15066898428794118[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:   12.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   15.7s finished\n",
      "\n",
      "[2024-04-18 02:22:43] Features: 17/1 -- score: -0.150630161092346[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  17 | elapsed:   12.0s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:   15.8s finished\n",
      "\n",
      "[2024-04-18 02:22:59] Features: 16/1 -- score: -0.1506452062217624[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  16 | elapsed:   11.7s remaining:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:   12.4s finished\n",
      "\n",
      "[2024-04-18 02:23:12] Features: 15/1 -- score: -0.15055866958543065[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    6.2s remaining:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   11.4s finished\n",
      "\n",
      "[2024-04-18 02:23:23] Features: 14/1 -- score: -0.150461739302431[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    5.8s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:   10.5s finished\n",
      "\n",
      "[2024-04-18 02:23:34] Features: 13/1 -- score: -0.1504140797763757[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  13 | elapsed:    5.2s remaining:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    8.9s finished\n",
      "\n",
      "[2024-04-18 02:23:43] Features: 12/1 -- score: -0.15042080801296984[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:    4.7s remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    8.1s finished\n",
      "\n",
      "[2024-04-18 02:23:51] Features: 11/1 -- score: -0.15041747462068192[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    4.4s remaining:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    4.7s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    7.2s finished\n",
      "\n",
      "[2024-04-18 02:23:59] Features: 10/1 -- score: -0.15041386453191066[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.4s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.7s finished\n",
      "\n",
      "[2024-04-18 02:24:06] Features: 9/1 -- score: -0.15046377129742464[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    3.9s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    5.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    5.6s finished\n",
      "\n",
      "[2024-04-18 02:24:11] Features: 8/1 -- score: -0.15037846370046468[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    3.5s remaining:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    3.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    3.7s finished\n",
      "\n",
      "[2024-04-18 02:24:15] Features: 7/1 -- score: -0.1505495410829636[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    3.2s remaining:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    3.3s finished\n",
      "\n",
      "[2024-04-18 02:24:19] Features: 6/1 -- score: -0.15109455256172757[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.8s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.9s finished\n",
      "\n",
      "[2024-04-18 02:24:22] Features: 5/1 -- score: -0.1515403297301904[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.2s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished\n",
      "\n",
      "[2024-04-18 02:24:24] Features: 4/1 -- score: -0.15237410917856603[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    2.0s finished\n",
      "\n",
      "[2024-04-18 02:24:27] Features: 3/1 -- score: -0.15347915833170148[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "\n",
      "[2024-04-18 02:24:29] Features: 2/1 -- score: -0.15555415703947445[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor\n",
      "\n",
      "CPU times: total: 29.3 s\n",
      "Wall time: 43min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    1.4s finished\n",
      "\n",
      "[2024-04-18 02:24:30] Features: 1/1 -- score: -0.17150298341367332"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize empty dictionary for SFS features\n",
    "sfs_features = {}\n",
    "\n",
    "for model in models:\n",
    "    # set name\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    try:\n",
    "\n",
    "        # features = perm_important_features[model_name]    \n",
    "        # features = kbest_features[model_name]\n",
    "        # features = feat_importance_features[MLA_name]\n",
    "        features = rfecv_features[MLA_name]\n",
    "        # features = baseline_features[model_name]\n",
    "\n",
    "        # incase there is no feature that had importance, go to the next model\n",
    "        if len(features) == 0:\n",
    "            continue\n",
    "        \n",
    "        X_sfs = X[features]\n",
    "\n",
    "        print(f'Running backward feature selection with {model_name}')\n",
    "\n",
    "        sfs = SFS(model,\n",
    "            k_features='best',\n",
    "            forward=False,\n",
    "            floating=False,\n",
    "            scoring=rmsle_scorer,\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "            cv=sk10)\n",
    "        \n",
    "        sfs = sfs.fit(X_sfs, y)\n",
    "\n",
    "        # Get the selected features index\n",
    "        selected_sfs_idx = list(sfs.k_feature_idx_)\n",
    "\n",
    "        # Get the feature names\n",
    "        selected_sfs_feats = X_sfs.columns[selected_sfs_idx]\n",
    "\n",
    "        selected_features = list(selected_sfs_feats)\n",
    "\n",
    "        # Reorder selected_features based on the predefined features_list\n",
    "        selected_features_ordered = [feat for feat in features_list if feat in selected_features]\n",
    "\n",
    "        sfs_features[model_name] = selected_features_ordered\n",
    "\n",
    "        print(f'Done with {model_name}', end='\\n\\n')\n",
    "\n",
    "    except KeyError:\n",
    "        print(f'{model_name} not in the dictionary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_features = {'CatBoostRegressor': ['Sex_I','Sex_M','Length','Diameter','Height','Whole_weight','Shucked_weight','Viscera_weight','Shell_weight'],\n",
    " 'ExtraTreesRegressor': ['Sex_I',  'Length',  'Diameter',  'Height',  'Whole_weight',  'Shucked_weight',  'Viscera_weight',  'Shell_weight'],\n",
    " 'HistGradientBoostingRegressor': ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'LGBMRegressor': ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'RandomForestRegressor': ['Sex_F', 'Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'XGBRegressor': ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'KNN': ['Sex_I', 'Length', 'Whole_weight', 'Shucked_weight', 'Shell_weight'],\n",
    " 'Nystroem Ridge': ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sfs_features_lgbm.txt', mode='w') as f:\n",
    "    pprint(sfs_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor.\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1.53 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC Std</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.145088</td>\n",
       "      <td>0.150577</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0 min 0.66 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MLA Name                                     MLA Parameters  \\\n",
       "0  LGBMRegressor  {'boosting_type': 'gbdt', 'class_weight': None...   \n",
       "\n",
       "   MLA Train ROC AUC  MLA Test ROC AUC  MLA Test ROC AUC Std        MLA Time  \n",
       "0           0.145088          0.150577              0.000155  0 min 0.66 sec  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "sfs_models = evaluate_models(models, X, y, sfs_features, sk10, f'{experiment_name}_sfs')\n",
    "sfs_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sfs_features['LGBMRegressor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_features = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMRegressor(n_jobs=-1, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_final = model1.fit(X[sfs_features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1_final.predict(test[sfs_features])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred, columns=['Rings'])\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], pred_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_lgbm_postprocess_asint_0.150577.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMRegressor(n_jobs=-1, random_state=5)\n",
    "model1b = LGBMRegressor(**lgbm_params_1)\n",
    "model1c = LGBMRegressor(**lgbm_params_2)\n",
    "model1d = LGBMRegressor(**lgbm_params_3)\n",
    "model1e = LGBMRegressor(**lgbm_params_4)\n",
    "model2 = XGBRegressor(random_state=5)\n",
    "model2b = XGBRegressor(**xgb_params_1)\n",
    "model2c = XGBRegressor(**xgb_params_2)\n",
    "model2d = XGBRegressor(**xgb_params_3)\n",
    "model2e = XGBRegressor(**xgb_params_4)\n",
    "model3 = RandomForestRegressor(random_state=5)\n",
    "model4 = ExtraTreesRegressor(random_state=5)\n",
    "model5 = HistGradientBoostingRegressor(random_state=5)\n",
    "model5b = HistGradientBoostingRegressor(**hist_params_1)\n",
    "model5c = HistGradientBoostingRegressor(**hist_params_2)\n",
    "model5d = HistGradientBoostingRegressor(**hist_params_3)\n",
    "model5e = HistGradientBoostingRegressor(**hist_params_4)\n",
    "model6 = CatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "model6b = CatBoostRegressor(**cat_params_1)\n",
    "model6c = CatBoostRegressor(**cat_params_2)\n",
    "model6d = CatBoostRegressor(**cat_params_3)\n",
    "model6e = CatBoostRegressor(**cat_params_4)\n",
    "model7 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=86, algorithm='kd_tree'))\n",
    "])\n",
    "model8 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nystroem', Nystroem(n_components=974, random_state=974)),\n",
    "    ('ridge', Ridge(alpha=0.1, max_iter=83157, random_state=1879, solver='svd'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features for Competition + Original dataset down to SFS for all models (Experiment Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_feats = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model2_feats = ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model3_feats = ['Sex_F', 'Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model4_feats = ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model5_feats = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model6_feats = ['Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model7_feats = ['Sex_I', 'Length', 'Whole_weight', 'Shucked_weight', 'Shell_weight']\n",
    "model8_feats = ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "\n",
    "X_lgbm = X[model1_feats]\n",
    "X_xgb = X[model2_feats]\n",
    "X_rf = X[model3_feats]\n",
    "X_extrat = X[model4_feats]\n",
    "X_hist = X[model5_feats]\n",
    "X_cat = X[model6_feats]\n",
    "X_knn = X[model7_feats]\n",
    "X_ridge = X[model8_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # class_weight_option = trial.suggest_categorical('class_weight', ['none', 'balanced', 'custom'])\n",
    "    # if class_weight_option == 'none':\n",
    "    #     class_weight = None\n",
    "    # elif class_weight_option == 'balanced':\n",
    "    #     class_weight = 'balanced'\n",
    "    # else:\n",
    "    #     # For multi-class, you could define a range or specific values to test\n",
    "    #     weight_for_class_0 = trial.suggest_float('weight_for_class_0', 0.1, 10.0)\n",
    "    #     weight_for_class_1 = trial.suggest_float('weight_for_class_1', 0.1, 10.0)\n",
    "    #     weight_for_class_2 = trial.suggest_float('weight_for_class_2', 0.1, 10.0)\n",
    "    #     weight_for_class_3 = trial.suggest_float('weight_for_class_3', 0.1, 10.0)\n",
    "    #     weight_for_class_4 = trial.suggest_float('weight_for_class_4', 0.1, 10.0)\n",
    "    #     weight_for_class_5 = trial.suggest_float('weight_for_class_5', 0.1, 10.0)\n",
    "    #     weight_for_class_6 = trial.suggest_float('weight_for_class_6', 0.1, 10.0)\n",
    "    #     class_weight = {0: weight_for_class_0, 1: weight_for_class_1, 2: weight_for_class_2, 3: weight_for_class_3, 4: weight_for_class_4, 5: weight_for_class_5, 6: weight_for_class_6}\n",
    "\n",
    "    # param = {\n",
    "    #     # 'objective': 'multiclass',\n",
    "    #     # 'num_class': 7,\n",
    "    #     # 'boosting_type': trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n",
    "    #     # 'class_weight': class_weight,\n",
    "    #     # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "    #     # 'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.5),\n",
    "    #     # 'max_depth': trial.suggest_int('max_depth', -1, 64),\n",
    "    #     # 'min_child_samples': trial.suggest_int('min_child_samples', 5, 500),\n",
    "    #     # 'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "    #     # 'min_split_gain': trial.suggest_float('min_split_gain', 0.5, 1.0),\n",
    "    #     # 'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "    #     # 'num_leaves': trial.suggest_int('num_leaves', 2, 1000),\n",
    "    #     # 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "    #     # 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "    #     # 'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "    #     'random_state': 5,\n",
    "    #     'n_jobs': -1,\n",
    "    #     }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'boosting_type': trial.suggest_categorical('boosting', ['gbdt', 'goss']),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.35, 0.8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 20, 64),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 50, 250),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.9, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1000),\n",
    "        'n_jobs': -1,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 40, 200),\n",
    "        'random_state': 5,\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        }\n",
    "    \n",
    "    rmsle_scores = []\n",
    "    \n",
    "    for train_index, test_index in sk10.split(X_lgbm, y):\n",
    "        X_train, X_test = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = LGBMRegressor(**param)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "    \n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# Using median pruner\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HistGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.15),\n",
    "    #     # 'max_iter': trial.suggest_int('max_iter', 50, 5000),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20, 1000),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 1000),\n",
    "    #     # 'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 1.0),\n",
    "    #     # 'max_bins': trial.suggest_int('max_bins', 10, 255),\n",
    "    #     'max_depth': trial.suggest_int('max_depth', 2, 64),\n",
    "    #     'random_state': 5,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.15),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 1000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 160),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 100, 300),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 1.0),\n",
    "        'max_bins': trial.suggest_int('max_bins', 200, 255),\n",
    "        'max_depth': trial.suggest_int('max_depth', 18, 64),\n",
    "        'random_state': 5,\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_hist, y)):\n",
    "        X_train, X_test = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = HistGradientBoostingRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(rmsle, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    param = {\n",
    "        # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "        # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "        # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']), # Classifier Problems\n",
    "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']), # Regression Problems\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 10, 3000),\n",
    "        # 'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "        # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 100, 3000, log=True) or None,\n",
    "        # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.1),\n",
    "        # 'min_samples_leaf': trial.suggest_int('min_samples_leaf',1, 500),\n",
    "        # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 500),\n",
    "        # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n",
    "        'random_state': 5,\n",
    "        # 'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    # # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    # param = {\n",
    "        # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "        # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "        # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 10, 3000),\n",
    "        # 'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "        # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 100, 3000, log=True) or None,\n",
    "        # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.1),\n",
    "        # 'min_samples_leaf': trial.suggest_int('min_samples_leaf',1, 500),\n",
    "        # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 500),\n",
    "        # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n",
    "        # 'random_state': 5,\n",
    "        # 'n_jobs': -1,\n",
    "    # }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_extrat, y)):\n",
    "        X_train, X_test = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = ExtraTreesRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(rmsle, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "    #     # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "    #     # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "    #     # 'max_depth': trial.suggest_int('max_depth', 10, 1000, log=True) or None,\n",
    "    #     # 'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 1000, log=True) or None,\n",
    "    #     # 'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "    #     # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "    #     # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 200),\n",
    "    #     # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.1),\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "    #     'random_state': 5,\n",
    "    #     'n_jobs': -1,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'ccp_alpha': 0.0,\n",
    "        'class_weight': None,\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 30, log=True),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 300, 700, log=True) or None,\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'log2',\n",
    "        'criterion': 'entropy',\n",
    "        'n_estimators': 1000,\n",
    "        'random_state': 5,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_rf, y)):\n",
    "        X_train, X_test = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(roc_auc, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if roc_auc < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    param = {\n",
    "        # 'iterations': trial.suggest_int('iterations', 50, 2000),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.3, log=True),\n",
    "        # 'depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.05, 1),\n",
    "        # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "        # 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        # 'l2_leaf_reg': trial.suggest_float('l2_reg', 1e-2, 10),\n",
    "        # 'random_strength': trial.suggest_float('random_strength', 1e-2, 10),\n",
    "        # 'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        # 'random_state': 5,\n",
    "        # 'verbose': False,\n",
    "        # 'early_stopping_rounds': 100,\n",
    "        # 'loss_function': 'RMSE',\n",
    "    }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        # 'iterations': trial.suggest_int('iterations', 1700, 2000),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 3),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-6, 0.3),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 2000),\n",
    "        # 'random_state': 5,\n",
    "        'verbose': False,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'loss_function': 'RMSE',\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_cat, y)):\n",
    "        X_train, X_test = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "        # 'verbosity': None,\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        # 'num_class': 3,  # Set this to the number of classes if using 'multi:softmax' objective\n",
    "        # 'random_state': 5,\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'tree_method': trial.suggest_categorical('tree_method', ['exact', 'approx', 'hist']),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 2, 64),\n",
    "        # 'min_child_weight': trial.suggest_int('min_child_weight', 1, 40),\n",
    "        # 'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.5),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "        # 'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        # 'max_bin': trial.suggest_int('max_bin', 256, 1024),\n",
    "        # 'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        # 'max_leaves': trial.suggest_int('max_leaves', 0, 255)\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'verbosity': None,\n",
    "        'objective': 'reg:squarederror',\n",
    "        # 'num_class': 3,  # Set this to the number of classes if using 'multi:softmax' objective\n",
    "        'random_state': trial.suggest_int('random_state', 1, 2000),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 250),\n",
    "        'tree_method': 'exact',\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 9, 30),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.8, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 0.9),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.45, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.45),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.6, 1.0),\n",
    "        'max_bin': trial.suggest_int('max_bin', 256, 1024),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 255)\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_xgb, y)):\n",
    "        X_train, X_test = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = XGBRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # # Suggest scaler type\n",
    "    # scaler_type = trial.suggest_categorical('scaler', ['standard', 'minmax', 'maxabs', 'robust', 'quantile'])\n",
    "    # if scaler_type == 'standard':\n",
    "    #     scaler = StandardScaler()\n",
    "    # elif scaler_type == 'minmax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif scaler_type == 'maxabs':\n",
    "    #     scaler = MaxAbsScaler()\n",
    "    # elif scaler_type == 'robust':\n",
    "    #     scaler = RobustScaler()\n",
    "    # else:\n",
    "    #     scaler = QuantileTransformer()\n",
    "\n",
    "    # # Suggest KNN params\n",
    "    # n_neighbors = trial.suggest_int('n_neighbors', 1, 100)\n",
    "    # weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    # algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "\n",
    "    # # Setup the pipeline\n",
    "    # pipeline = Pipeline([\n",
    "    #     # ('scaler', scaler),\n",
    "    #     ('scaler', StandardScaler()),\n",
    "    #     ('knn', KNeighborsRegressor(\n",
    "    #         n_neighbors=50,\n",
    "    #         # n_neighbors=n_neighbors,\n",
    "    #         # weights=weights,\n",
    "    #         algorithm=algorithm,\n",
    "    #         ))\n",
    "    # ])\n",
    "\n",
    "\n",
    "    # Final KNN params\n",
    "    # Suggest scaler type\n",
    "    scaler_type = trial.suggest_categorical('scaler', ['standard', 'minmax', 'maxabs'])\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "        \n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 40, 100)\n",
    "    weights = 'uniform'\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['auto', 'kd_tree'])\n",
    "    \n",
    "    # Setup the final pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('knn', KNeighborsRegressor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            algorithm=algorithm,\n",
    "            ))\n",
    "    ])\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_knn, y)):\n",
    "        X_train, X_test = X_knn.iloc[train_index], X_knn.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    # # Suggest scaler type\n",
    "    # scaler_type = trial.suggest_categorical('scaler', ['standard', 'robust', 'quantile'])\n",
    "    # if scaler_type == 'standard':\n",
    "    #     scaler = StandardScaler()\n",
    "    # elif scaler_type == 'minmax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif scaler_type == 'maxabs':\n",
    "    #     scaler = MaxAbsScaler()\n",
    "    # elif scaler_type == 'robust':\n",
    "    #     scaler = RobustScaler()\n",
    "    # else:\n",
    "    #     scaler = QuantileTransformer()\n",
    "\n",
    "    # # Suggest ridge params\n",
    "    # alpha = trial.suggest_float('alpha', 0.1, 1000)\n",
    "    # max_iter = trial.suggest_int('max_iter', 1, 200000)\n",
    "    # solver = trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])\n",
    "\n",
    "\n",
    "    # # Suggest Nystroem params\n",
    "#     components = trial.suggest_int('components', 1, 1000)\n",
    "    \n",
    "#     # Setup the pipeline\n",
    "#     pipeline = Pipeline([\n",
    "#     # ('scaler', scaler),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('nystroem', Nystroem(\n",
    "#         n_components=components, \n",
    "#         random_state=5,\n",
    "#         )),\n",
    "#     ('ridge', Ridge(\n",
    "#         # alpha=alpha,\n",
    "#         # max_iter=max_iter, \n",
    "#         # solver=solver, \n",
    "#         random_state=5,\n",
    "#         ))\n",
    "# ])\n",
    "\n",
    "    # Group Parameters after individual tunings\n",
    "    # Suggest scaler type\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Suggest ridge params\n",
    "    # alpha = trial.suggest_float('alpha', 0.1, 0.3)\n",
    "    max_iter = trial.suggest_int('max_iter', 1, 200000)\n",
    "    solver = trial.suggest_categorical('solver', ['auto', 'svd'])\n",
    "    random_state_ridge = trial.suggest_int('random_state_ridge', 1, 2000)\n",
    "\n",
    "\n",
    "    # Suggest Nystroem params\n",
    "    components = trial.suggest_int('components', 600, 3000)\n",
    "    random_state_nystroem = trial.suggest_int('random_state_nystroem', 1, 2000)\n",
    "    \n",
    "    # Setup the pipeline\n",
    "    pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('nystroem', Nystroem(\n",
    "        n_components=components, \n",
    "        random_state=random_state_nystroem\n",
    "        )),\n",
    "    ('ridge', Ridge(\n",
    "        alpha=0.1, \n",
    "        max_iter=max_iter, \n",
    "        solver=solver, \n",
    "        random_state=random_state_ridge\n",
    "        ))\n",
    "])\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_ridge, y)):\n",
    "        X_train, X_test = X_ridge.iloc[train_index], X_ridge.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Weights Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model1b_results, model1c_results, model1d_results, model1e_results, model2_results, model3_results, model3b_results, model3c_results, model4_results, model4b_results, model4c_results, model4d_results, model4e_results, model5_results, model5b_results, model5c_results, model5d_results, model5e_results, model5f_results, model6_results, model6b_results, model6c_results, model6d_results, model6e_results, model7_results, model8_results, y_test_list = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train_lgbm, X_test_lgbm = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "    X_train_xgb, X_test_xgb = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "    X_train_rf, X_test_rf = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "    X_train_extrat, X_test_extrat = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "    X_train_hist, X_test_hist = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "    X_train_cat, X_test_cat = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "    X_train_knn, X_test_knn = X_knn.iloc[train_index], X_knn.iloc[test_index]\n",
    "    X_train_ridge, X_test_ridge = X_ridge.iloc[train_index], X_ridge.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    print('Running LGBM')\n",
    "\n",
    "    model1.fit(X_train_lgbm, y_train)\n",
    "    model1_results.append(model1.predict(X_test_lgbm))\n",
    "\n",
    "    model1b.fit(X_train_lgbm, y_train)\n",
    "    model1b_results.append(model1b.predict(X_test_lgbm))\n",
    "\n",
    "    model1c.fit(X_train_lgbm, y_train)\n",
    "    model1c_results.append(model1c.predict(X_test_lgbm))\n",
    "\n",
    "    model1d.fit(X_train_lgbm, y_train)\n",
    "    model1d_results.append(model1d.predict(X_test_lgbm))\n",
    "\n",
    "    model1e.fit(X_train_lgbm, y_train)\n",
    "    model1e_results.append(model1e.predict(X_test_lgbm))\n",
    "\n",
    "    print('Running XGBoost')\n",
    "\n",
    "    model2.fit(X_train_xgb, y_train)\n",
    "    model2_results.append(model2.predict(X_test_xgb))\n",
    "\n",
    "    model2b.fit(X_train_xgb, y_train)\n",
    "    model2b_results.append(model2b.predict(X_test_xgb))\n",
    "\n",
    "    model2c.fit(X_train_xgb, y_train)\n",
    "    model2c_results.append(model2c.predict(X_test_xgb))\n",
    "\n",
    "    model2d.fit(X_train_xgb, y_train)\n",
    "    model2d_results.append(model2d.predict(X_test_xgb))\n",
    "\n",
    "    model2e.fit(X_train_xgb, y_train)\n",
    "    model2e_results.append(model2e.predict(X_test_xgb))\n",
    "\n",
    "    print('Running Random Forest')\n",
    "\n",
    "    model3.fit(X_train_rf, y_train)\n",
    "    model3_results.append(model3.predict(X_test_rf))\n",
    "\n",
    "    print('Running ExtraTrees')\n",
    "\n",
    "    model4.fit(X_train_extrat, y_train)\n",
    "    model4_results.append(model4.predict(X_test_extrat))\n",
    "\n",
    "    # model4b.fit(X_train_extrat, y_train)\n",
    "    # model4b_results.append(model4b.predict(X_test_extrat))\n",
    "\n",
    "    # model4c.fit(X_train_extrat, y_train)\n",
    "    # model4c_results.append(model4c.predict(X_test_extrat))\n",
    "\n",
    "    # model4d.fit(X_train_extrat, y_train)\n",
    "    # model4d_results.append(model4d.predict(X_test_extrat))\n",
    "\n",
    "    # model4e.fit(X_train_extrat, y_train)\n",
    "    # model4e_results.append(model4e.predict(X_test_extrat))\n",
    "\n",
    "    print('Running Hist Gradient')\n",
    "\n",
    "    model5.fit(X_train_hist, y_train)\n",
    "    model5_results.append(model5.predict(X_test_hist))\n",
    "\n",
    "    # model5b.fit(X_train_hist, y_train)\n",
    "    # model5b_results.append(model5b.predict(X_test_hist))\n",
    "\n",
    "    # model5c.fit(X_train_hist, y_train)\n",
    "    # model5c_results.append(model5c.predict(X_test_hist))\n",
    "\n",
    "    # model5d.fit(X_train_hist, y_train)\n",
    "    # model5d_results.append(model5d.predict(X_test_hist))\n",
    "\n",
    "    # model5e.fit(X_train_hist, y_train)\n",
    "    # model5e_results.append(model5e.predict(X_test_hist))\n",
    "\n",
    "    # model5f.fit(X_train_hist, y_train)\n",
    "    # model5f_results.append(model5f.predict(X_test_hist))\n",
    "\n",
    "    print('Running CatBoost')\n",
    "\n",
    "    model6.fit(X_train_cat, y_train)\n",
    "    model6_results.append(model6.predict(X_test_cat))\n",
    "\n",
    "    # model6b.fit(X_train_cat, y_train)\n",
    "    # model6b_results.append(model6b.predict(X_test_cat))\n",
    "\n",
    "    # model6c.fit(X_train_cat, y_train)\n",
    "    # model6c_results.append(model6c.predict(X_test_cat))\n",
    "\n",
    "    # model6d.fit(X_train_cat, y_train)\n",
    "    # model6d_results.append(model6d.predict(X_test_cat))\n",
    "\n",
    "    # model6e.fit(X_train_cat, y_train)\n",
    "    # model6e_results.append(model6e.predict(X_test_cat))\n",
    "\n",
    "    print('Running KNN')\n",
    "\n",
    "    model7.fit(X_train_knn, y_train)\n",
    "    model7_results.append(model7.predict(X_test_knn))    \n",
    "\n",
    "    print('Running Ridge')\n",
    "\n",
    "    model8.fit(X_train_ridge, y_train)\n",
    "    model8_results.append(model8.predict(X_test_ridge))   \n",
    "\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # model1_weights, model1b_weights, model1c_weights, model1d_weights, model1e_weights, model2_weights, model3_weights, model3b_weights, model3c_weights, model4_weights, model4b_weights, model4c_weights, model4d_weights, model4e_weights, model5_weights, model5b_weights, model5c_weights, model5d_weights, model5e_weights, model5f_weights, model6_weights, model6b_weights, model6c_weights, model6d_weights, model6e_weights, scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# model1_weights, model1b_weights, model1c_weights, model1d_weights, model1e_weights, model2_weights, model3_weights, model3b_weights, model3c_weights, model4_weights, model4b_weights, model4c_weights, model4d_weights, model4e_weights, model5_weights, model5b_weights, model5c_weights, model5d_weights, model5e_weights, model5f_weights, model6_weights, model6e_weights, scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# scores_in = []\n",
    "\n",
    "# for i in tqdm(range(20)):\n",
    "#     weight_1 = np.random.random_sample(size=1)[0]\n",
    "#     weight_1b = np.random.random_sample(size=1)[0]\n",
    "#     weight_1c = np.random.random_sample(size=1)[0]\n",
    "#     weight_1d = np.random.random_sample(size=1)[0]\n",
    "#     weight_1e = np.random.random_sample(size=1)[0]\n",
    "#     weight_2 = np.random.random_sample(size=1)[0]\n",
    "#     weight_3 = np.random.random_sample(size=1)[0]\n",
    "#     weight_3b = np.random.random_sample(size=1)[0]\n",
    "#     weight_3c = np.random.random_sample(size=1)[0]\n",
    "#     weight_4 = np.random.random_sample(size=1)[0]\n",
    "#     weight_4b = np.random.random_sample(size=1)[0]\n",
    "#     weight_4c = np.random.random_sample(size=1)[0]\n",
    "#     weight_4d = np.random.random_sample(size=1)[0]\n",
    "#     weight_4e = np.random.random_sample(size=1)[0]\n",
    "#     weight_5 = np.random.random_sample(size=1)[0]\n",
    "#     weight_5b = np.random.random_sample(size=1)[0]\n",
    "#     weight_5c = np.random.random_sample(size=1)[0]\n",
    "#     weight_5d = np.random.random_sample(size=1)[0]\n",
    "#     weight_5e = np.random.random_sample(size=1)[0]\n",
    "#     weight_5f = np.random.random_sample(size=1)[0]\n",
    "#     weight_6 = np.random.random_sample(size=1)[0]\n",
    "#     # weight_6b = np.random.random_sample(size=1)[0]\n",
    "#     # weight_6c = np.random.random_sample(size=1)[0]\n",
    "#     # weight_6d = np.random.random_sample(size=1)[0]\n",
    "#     weight_6e = np.random.random_sample(size=1)[0]\n",
    "\n",
    "#     model1_weights.append(weight_1)\n",
    "#     model1b_weights.append(weight_1b)\n",
    "#     model1c_weights.append(weight_1c)\n",
    "#     model1d_weights.append(weight_1d)\n",
    "#     model1e_weights.append(weight_1e)\n",
    "#     model2_weights.append(weight_2)\n",
    "#     model3_weights.append(weight_3)\n",
    "#     model3b_weights.append(weight_3b)\n",
    "#     model3c_weights.append(weight_3c)\n",
    "#     model4_weights.append(weight_4)\n",
    "#     model4b_weights.append(weight_4b)\n",
    "#     model4c_weights.append(weight_4c)\n",
    "#     model4d_weights.append(weight_4d)\n",
    "#     model4e_weights.append(weight_4e)\n",
    "#     model5_weights.append(weight_5)\n",
    "#     model5b_weights.append(weight_5b)\n",
    "#     model5c_weights.append(weight_5c)\n",
    "#     model5d_weights.append(weight_5d)\n",
    "#     model5e_weights.append(weight_5e)\n",
    "#     model5f_weights.append(weight_5f)\n",
    "#     model6_weights.append(weight_6)\n",
    "#     # model6b_weights.append(weight_6b)\n",
    "#     # model6c_weights.append(weight_6c)\n",
    "#     # model6d_weights.append(weight_6d)\n",
    "#     model6e_weights.append(weight_6e)\n",
    "\n",
    "#     # scores_in = []\n",
    "\n",
    "#     for j in range(n_splits):\n",
    "#         weighted_pred = (weight_1 * model1_results[j])\n",
    "#         + (weight_1b * model1b_results[j])\n",
    "#         + (weight_1c * model1c_results[j])\n",
    "#         + (weight_1d * model1d_results[j])\n",
    "#         + (weight_1e * model1e_results[j])\n",
    "#         + (weight_2 * model2_results[j])\n",
    "#         + (weight_3 * model3_results[j])\n",
    "#         + (weight_3b * model3b_results[j])\n",
    "#         + (weight_3c * model3c_results[j])\n",
    "#         + (weight_4 * model4_results[j])\n",
    "#         + (weight_4b * model4b_results[j])\n",
    "#         + (weight_4c * model4c_results[j])\n",
    "#         + (weight_4d * model4d_results[j])\n",
    "#         + (weight_4e * model4e_results[j])\n",
    "#         + (weight_5 * model5_results[j])\n",
    "#         + (weight_5b * model5b_results[j])\n",
    "#         + (weight_5c * model5c_results[j])\n",
    "#         + (weight_5d * model5d_results[j])\n",
    "#         + (weight_5e * model5e_results[j])\n",
    "#         + (weight_5f * model5f_results[j])\n",
    "#         + (weight_6 * model6_results[j])\n",
    "#         # + (weight_6b * model6b_results[j])\n",
    "#         # + (weight_6c * model6c_results[j])\n",
    "#         # + (weight_6d * model6d_results[j])\n",
    "#         + (weight_6e * model6e_results[j])\n",
    "\n",
    "#         weighted_pred_normalized = weighted_pred / np.sum(weighted_pred, axis=1, keepdims=True)\n",
    "\n",
    "#         scores_in.append(roc_auc_score(y_test_list[j], weighted_pred_normalized, multi_class='ovr'))\n",
    "        \n",
    "#     scores.append(np.mean(scores_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the predictions for each model and the true labels\n",
    "all_predictions = [np.concatenate(model_results) for model_results in [model1_results, model2_results, model3_results, model4_results, model5_results, model6_results, model7_results, model8_results]]\n",
    "all_true_labels = np.concatenate(y_test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def objective(trial):\n",
    "    # Generate weights for each model\n",
    "    weights = [trial.suggest_uniform(f'weight_{i}', -1, 1) for i in range(len(all_predictions))]\n",
    "    \n",
    "    # Calculate the weighted sum of predictions\n",
    "    weighted_predictions = np.zeros_like(all_predictions[0])\n",
    "    for weight, predictions in zip(weights, all_predictions):\n",
    "        weighted_predictions += weight * predictions\n",
    "    \n",
    "    # Compute RMSLE; you need to ensure there are no negative or zero predictions\n",
    "    # Ensuring no negative values\n",
    "    weighted_predictions = np.clip(weighted_predictions, a_min=0, a_max=None)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(all_true_labels, weighted_predictions))\n",
    "    \n",
    "    return rmsle\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=500)  # You can adjust the number of trials\n",
    "\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimal weights from Optuna study\n",
    "optimal_weights = study.best_params\n",
    "\n",
    "# Assuming `optimal_weights` is a dictionary with model identifiers as keys\n",
    "# and the optimized weight as values, you can directly use it to create a DataFrame\n",
    "# For the 'scores', you would use the best score achieved during the Optuna study\n",
    "optuna_results_df = pd.DataFrame([optimal_weights])\n",
    "optuna_results_df['score'] = study.best_value\n",
    "\n",
    "optuna_results_df.columns = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5', 'model_6', 'model_7', 'model_8', 'score']\n",
    "\n",
    "# Since you only have one row of data (the best combination of weights),\n",
    "# sorting by 'score' or getting the top rows doesn't apply as it's already the best\n",
    "optuna_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_0.14903.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('random_weights_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Submission (Random Weight Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Running LGBM')\n",
    "model1_final = model1.fit(X_lgbm, y)\n",
    "model1b_final = model1b.fit(X_lgbm, y)\n",
    "model1c_final = model1c.fit(X_lgbm, y)\n",
    "model1d_final = model1d.fit(X_lgbm, y)\n",
    "model1e_final = model1e.fit(X_lgbm, y)\n",
    "\n",
    "print('Running XGBoost')\n",
    "model2_final = model2.fit(X_xgb, y)\n",
    "\n",
    "print('Running Random Forest')\n",
    "model3_final = model3.fit(X_rf, y)\n",
    "model3b_final = model3b.fit(X_rf, y)\n",
    "model3c_final = model3c.fit(X_rf, y)\n",
    "\n",
    "print('Running ExtraTrees')\n",
    "model4_final = model4.fit(X_extrat, y)\n",
    "model4b_final = model4b.fit(X_extrat, y)\n",
    "model4c_final = model4c.fit(X_extrat, y)\n",
    "model4d_final = model4d.fit(X_extrat, y)\n",
    "model4e_final = model4e.fit(X_extrat, y)\n",
    "\n",
    "print('Running HistGradient')\n",
    "model5_final = model5.fit(X_hist, y)\n",
    "model5b_final = model5b.fit(X_hist, y)\n",
    "model5c_final = model5c.fit(X_hist, y)\n",
    "model5d_final = model5d.fit(X_hist, y)\n",
    "model5e_final = model5e.fit(X_hist, y)\n",
    "model5f_final = model5f.fit(X_hist, y)\n",
    "\n",
    "print('Running CatBoost')\n",
    "model6_final = model6.fit(X_cat, y)\n",
    "# model6b_final = model6b.fit(X_cat, y)\n",
    "# model6c_final = model6c.fit(X_cat, y)\n",
    "# model6d_final = model6d.fit(X_cat, y)\n",
    "model6e_final = model6e.fit(X_cat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ensemble_pred = (\n",
    "                results_df['model_1'][0] * model1_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1b'][0] * model1b_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1c'][0] * model1c_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1d'][0] * model1d_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1e'][0] * model1e_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_2'][0] * model2_final.predict_proba(test[model2_feats]) +\n",
    "                results_df['model_3'][0] * model3_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_3b'][0] * model3b_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_3c'][0] * model3c_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_4'][0] * model4_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4b'][0] * model4b_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4c'][0] * model4c_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4d'][0] * model4d_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4e'][0] * model4e_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_5'][0] * model5_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5b'][0] * model5b_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5c'][0] * model5c_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5d'][0] * model5d_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5e'][0] * model5e_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5f'][0] * model5f_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_6'][0] * model6_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6b'][0] * model6b_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6c'][0] * model6c_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6d'][0] * model6d_final.predict_proba(test[model6_feats]) +\n",
    "                results_df['model_6e'][0] * model6e_final.predict_proba(test[model6_feats])\n",
    "                 )\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_pred)\n",
    "\n",
    "# If all models predict 0, instead of getting NaN, fill in 0\n",
    "ensemble_df = ensemble_df.div(ensemble_df.sum(axis=1), axis=0).fillna(0)\n",
    "ensemble_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "optuna_ensemble_pred = (\n",
    "                optuna_results_df['model_1'][0] * model1_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1b'][0] * model1b_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1c'][0] * model1c_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1d'][0] * model1d_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1e'][0] * model1e_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_2'][0] * model2_final.predict_proba(test[model2_feats]) +\n",
    "                optuna_results_df['model_3'][0] * model3_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_3b'][0] * model3b_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_3c'][0] * model3c_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_4'][0] * model4_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4b'][0] * model4b_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4c'][0] * model4c_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4d'][0] * model4d_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4e'][0] * model4e_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_5'][0] * model5_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5b'][0] * model5b_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5c'][0] * model5c_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5d'][0] * model5d_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5e'][0] * model5e_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5f'][0] * model5f_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_6'][0] * model6_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6b'][0] * model6b_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6c'][0] * model6c_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6d'][0] * model6d_final.predict_proba(test[model6_feats]) +\n",
    "                optuna_results_df['model_6e'][0] * model6e_final.predict_proba(test[model6_feats])\n",
    "                 )\n",
    "\n",
    "optuna_ensemble_df = pd.DataFrame(optuna_ensemble_pred)\n",
    "\n",
    "# If all models predict 0, instead of getting NaN, fill in 0\n",
    "optuna_ensemble_df = optuna_ensemble_df.div(optuna_ensemble_df.sum(axis=1), axis=0).fillna(0)\n",
    "optuna_ensemble_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], optuna_ensemble_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_optuna_weights_ensemble_3fold_0.902197.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get submission (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('model1', model1_final),\n",
    "    ('model2', model2_final),\n",
    "    ('model3', model3_final),\n",
    "    ('model4', model4_final),\n",
    "    ('model5', model5_final),\n",
    "    ('model6', model6_final)\n",
    "]\n",
    "\n",
    "# Initialize the Stacking Classifier with LogisticRegression as the final estimator\n",
    "final_estimator = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "# final_estimator = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "# final_estimator = XGBClassifier(random_state=5)\n",
    "# final_estimator = RandomForestClassifier(random_state=5)\n",
    "# final_estimator = ExtraTreesClassifier(random_state=5)\n",
    "# final_estimator = HistGradientBoostingClassifier(random_state=5)\n",
    "# final_estimator = CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=final_estimator, passthrough=False, cv=3)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict_proba(X_test)\n",
    "\n",
    "    # Assuming your classes are 0, 1, 2, etc., adjust as necessary\n",
    "    y_test_binarized = label_binarize(y_test, classes=np.unique(y))\n",
    "    roc_auc = roc_auc_score(y_test_binarized, y_pred, multi_class='ovr')\n",
    "\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')\n",
    "    \n",
    "print(f'The average stacking score is {np.mean(roc_auc_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Reg - 0.886778\n",
    "- LGBM - 0.885863\n",
    "- XGB - 0.881636\n",
    "- RF - 0.883835\n",
    "- ET - 0.884523\n",
    "- Hist - 0.886572\n",
    "- Cat - 0.886183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on unseen test data\n",
    "y_test_pred = stacking_clf.predict_proba(test)\n",
    "\n",
    "stacking_df = pd.DataFrame(y_test_pred)\n",
    "\n",
    "ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model2_results, model3_results, model4_results, model5_results, model6_results, y_test_list = [], [], [], [], [], [], []\n",
    "\n",
    "# # Placeholder for OOF predictions for each model\n",
    "# # Assuming you have a dataset with N samples\n",
    "# N = len(y)  # y_train is your target variable array\n",
    "# oof_preds1 = np.zeros((N, 1))\n",
    "# oof_preds2 = np.zeros((N, 1))\n",
    "# oof_preds3 = np.zeros((N, 1))\n",
    "# oof_preds4 = np.zeros((N, 1))\n",
    "# oof_preds5 = np.zeros((N, 1))\n",
    "# oof_preds6 = np.zeros((N, 1))\n",
    "\n",
    "# # Similarly, for test predictions, accumulate them over folds\n",
    "# # Assuming you have a test set with M samples\n",
    "# M = len(test)  # x_test needs to be defined by you\n",
    "# test_preds1 = np.zeros((M, 1))\n",
    "# test_preds2 = np.zeros((M, 1))\n",
    "# test_preds3 = np.zeros((M, 1))\n",
    "# test_preds4 = np.zeros((M, 1))\n",
    "# test_preds5 = np.zeros((M, 1))\n",
    "# test_preds6 = np.zeros((M, 1))\n",
    "\n",
    "target_length = len(y)\n",
    "no_classes = len(np.unique(y))\n",
    "test_length = len(test)\n",
    "\n",
    "# Initialize arrays for OOF and test predictions with dimensions for multiclass for each model\n",
    "lgbm_oof_preds = np.zeros((target_length, no_classes))\n",
    "lgbm_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "xgb_oof_preds = np.zeros((target_length, no_classes))\n",
    "xgb_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "rf_oof_preds = np.zeros((target_length, no_classes))\n",
    "rf_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "extrat_oof_preds = np.zeros((target_length, no_classes))\n",
    "extrat_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "hist_oof_preds = np.zeros((target_length, no_classes))\n",
    "hist_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "cat_oof_preds = np.zeros((target_length, no_classes))\n",
    "cat_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "X_lgbm = X[model1_feats]\n",
    "X_xgb = X[model2_feats]\n",
    "X_rf = X[model3_feats]\n",
    "X_extrat = X[model4_feats]\n",
    "X_hist = X[model5_feats]\n",
    "X_cat = X[model6_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "\n",
    "    # Placeholder arrays for the fold's predicition\n",
    "    fold_oof_preds_lgbm = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_lgbm = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_xgb = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_xgb = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_rf = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_rf = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_extrat = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_extrat = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_hist = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_hist = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_cat = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_cat = np.zeros((test_length, no_classes))\n",
    "\n",
    "    # Get each models train and test for X and y\n",
    "    X_train_lgbm, X_test_lgbm = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "    X_train_xgb, X_test_xgb = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "    X_train_rf, X_test_rf = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "    X_train_extrat, X_test_extrat = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "    X_train_hist, X_test_hist = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "    X_train_cat, X_test_cat = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    ########\n",
    "    # LGBM #\n",
    "    ########\n",
    "    model1.fit(X_train_lgbm, y_train)\n",
    "    fold_oof_preds_lgbm = model1.predict_proba(X_test_lgbm)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    lgbm_oof_preds[test_index] = fold_oof_preds_lgbm\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_lgbm += model1.predict_proba(test.loc[:, model1_feats]) / sk10.n_splits\n",
    "\n",
    "    lgbm_test_preds += fold_test_preds_lgbm\n",
    "\n",
    "\n",
    "    ###########\n",
    "    # XGBOOST #\n",
    "    ###########\n",
    "    model2.fit(X_train_xgb, y_train)\n",
    "    fold_oof_preds_xgb = model2.predict_proba(X_test_xgb)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    xgb_oof_preds[test_index] = fold_oof_preds_xgb\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_xgb += model2.predict_proba(test.loc[:, model2_feats]) / sk10.n_splits\n",
    "\n",
    "    xgb_test_preds += fold_test_preds_xgb\n",
    "\n",
    "\n",
    "    #################\n",
    "    # RANDOM FOREST #\n",
    "    #################\n",
    "    model3.fit(X_train_rf, y_train)\n",
    "    fold_oof_preds_rf = model3.predict_proba(X_test_rf)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    rf_oof_preds[test_index] = fold_oof_preds_rf\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_rf += model3.predict_proba(test.loc[:, model3_feats]) / sk10.n_splits\n",
    "\n",
    "    rf_test_preds += fold_test_preds_rf\n",
    "\n",
    "    \n",
    "    ###############\n",
    "    # EXTRA TREES #\n",
    "    ###############\n",
    "    model4.fit(X_train_extrat, y_train)\n",
    "    fold_oof_preds_extrat = model4.predict_proba(X_test_extrat)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    extrat_oof_preds[test_index] = fold_oof_preds_extrat\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_extrat += model4.predict_proba(test.loc[:, model4_feats]) / sk10.n_splits\n",
    "\n",
    "    extrat_test_preds += fold_test_preds_extrat\n",
    "\n",
    "\n",
    "    #################\n",
    "    # HIST GRADIENT #\n",
    "    #################\n",
    "    model5.fit(X_train_hist, y_train)\n",
    "    fold_oof_preds_hist = model5.predict_proba(X_test_hist)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    hist_oof_preds[test_index] = fold_oof_preds_hist\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_hist += model5.predict_proba(test.loc[:, model5_feats]) / sk10.n_splits\n",
    "\n",
    "    hist_test_preds += fold_test_preds_hist\n",
    "\n",
    "\n",
    "    ############\n",
    "    # CATBOOST #\n",
    "    ############\n",
    "    model6.fit(X_train_cat, y_train)\n",
    "    fold_oof_preds_cat = model6.predict_proba(X_test_cat)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    cat_oof_preds[test_index] = fold_oof_preds_cat\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_cat += model6.predict_proba(test.loc[:, model6_feats]) / sk10.n_splits\n",
    "\n",
    "    cat_test_preds += fold_test_preds_cat\n",
    "    # y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_scores = [roc_auc_score((y == class_id).astype(int), oof_preds[:, class_id], multi_class='ovr') for class_id in range(no_classes)]\n",
    "lgbm_roc_auc = roc_auc_score(y, lgbm_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average LGBM ROC AUC Score:\", lgbm_roc_auc)\n",
    "\n",
    "xgb_roc_auc = roc_auc_score(y, xgb_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average XGBoost ROC AUC Score:\", xgb_roc_auc)\n",
    "\n",
    "rf_roc_auc = roc_auc_score(y, rf_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Random Forest ROC AUC Score:\", rf_roc_auc)\n",
    "\n",
    "extrat_roc_auc = roc_auc_score(y, extrat_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Extra Trees ROC AUC Score:\", extrat_roc_auc)\n",
    "\n",
    "hist_roc_auc = roc_auc_score(y, hist_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Hist Gradient ROC AUC Score:\", hist_roc_auc)\n",
    "\n",
    "cat_roc_auc = roc_auc_score(y, cat_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average CatBoost ROC AUC Score:\", cat_roc_auc)\n",
    "\n",
    "# 0.89369590207664\n",
    "# 0.00201442835387733\n",
    "# 0.886778 - StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# After running the fitting and prediction with the first level of machine learning models\n",
    "x_train = np.concatenate(( lgbm_oof_preds, xgb_oof_preds, rf_oof_preds, extrat_oof_preds, hist_oof_preds, cat_oof_preds), axis=1)\n",
    "test_stack = np.concatenate(( lgbm_test_preds, xgb_test_preds, rf_test_preds, extrat_test_preds, hist_test_preds, cat_test_preds), axis=1)\n",
    "\n",
    "# Assuming the second-level stacking is to be done with XGboost (pre-tuned). Yes! You can tune second-level stack\n",
    "\n",
    "stacking_estimator = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "xgb = stacking_estimator.fit(x_train, y)\n",
    "final_predictions = xgb.predict_proba(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = np.zeros((x_train.shape[0], no_classes))\n",
    "test_preds = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(x_train, y)):\n",
    "    X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model2.fit(X_train, y_train)\n",
    "    y_pred = model2.predict_proba(X_test)\n",
    "\n",
    "    # Assign predictions for this fold to the appropriate indices in oof_preds\n",
    "    oof_preds[test_index, :] = y_pred\n",
    "    \n",
    "    print(f'Done with fold {i+1}.')\n",
    "\n",
    "# Calculate ROC AUC on the OOF predictions\n",
    "roc_auc = roc_auc_score(y, oof_preds, multi_class='ovr', average='macro')\n",
    "print(f'The stacking score is {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Reg - 0.8883102077923056\n",
    "- LGBM - 0.8880225088607244\n",
    "- XGB - 0.8846028966376445\n",
    "- RF - \n",
    "- ET - \n",
    "- Hist - \n",
    "- Cat - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.DataFrame(final_predictions)\n",
    "final_predictions_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], final_predictions_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_3fold_0.88831.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the stack CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_scores = []\n",
    "meta_model = Ridge()\n",
    "\n",
    "for i, (train_idx, meta_idx) in enumerate(sk10.split(X)):\n",
    "    print(f'Fold {i + 1}')\n",
    "    X_train, X_meta = X.iloc[train_idx], X.iloc[meta_idx]\n",
    "    y_train, y_meta = y.iloc[train_idx], y.iloc[meta_idx]\n",
    "\n",
    "    meta_features_fold = np.zeros((X_meta.shape[0], len(models)))\n",
    "    # meta_test_features = np.zeros((y.shape[0], len(models)))\n",
    "    # meta_targets = np.zeros(y.shape[0])\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_name = model.__class__.__name__ if not hasattr(model, 'name') else model.name\n",
    "        print(f'Starting {model_name}')\n",
    "        model_features = sfs_features[model_name]\n",
    "\n",
    "        # Fit model on the selected features\n",
    "        model.fit(X_train[model_features], y_train)\n",
    "        preds = model.predict(X_meta[model_features])\n",
    "        meta_features_fold[:, i] = preds\n",
    "\n",
    "    # Train the meta-model on the predictions from the base models\n",
    "    meta_model.fit(meta_features_fold, y_meta)\n",
    "    \n",
    "    # Predict using the meta-model\n",
    "    final_preds = meta_model.predict(meta_features_fold)\n",
    "    \n",
    "    # Calculate RMSLE for the current fold\n",
    "    current_fold_rmsle = rmsle(y_meta, final_preds)\n",
    "    meta_scores.append(current_fold_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average RMSLE across all folds\n",
    "average_rmsle = np.mean(meta_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stacking submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain base models on all data\n",
    "all_base_model_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__ if not hasattr(model, 'name') else model.name\n",
    "    print(f'Starting {model_name}')\n",
    "    model_features = sfs_features[model_name]\n",
    "\n",
    "    model.fit(X[model_features], y)\n",
    "    preds = model.predict(test[model_features])\n",
    "    all_base_model_predictions.append(preds.reshape(-1, 1))\n",
    "\n",
    "# Stack predictions for the meta model\n",
    "X_new_meta = np.hstack(all_base_model_predictions)\n",
    "\n",
    "# Use the meta model to make final predictions\n",
    "final_predictions = meta_model.predict(X_new_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__ if not hasattr(model, 'name') else model.name\n",
    "    model_names.append(model_name)\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ensemble weights')\n",
    "weights = pd.Series(meta_model.coef_, index=model_names)\n",
    "print(weights)\n",
    "print(f'Weights total: {weights.sum()}')\n",
    "print(f'Intercept: {meta_model.intercept_}', end='\\n\\n')\n",
    "print(f\"Average Stacking RMSLE across all folds: {average_rmsle:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['Rings'])\n",
    "final_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], final_predictions_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_0.14848.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
