{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from helper_functions import brute_force_feat_engineering\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment_name = 'feat_eng_all_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate the engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('df_train.csv')\n",
    "test = pd.read_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90615, 11), (60411, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.2380</td>\n",
       "      <td>0.6185</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.4785</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>0.2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex_F  Sex_I  Sex_M  Length  Diameter  Height  Whole_weight  \\\n",
       "0    0.0    0.0    1.0   0.645     0.475   0.155        1.2380   \n",
       "1    0.0    0.0    1.0   0.580     0.460   0.160        0.9830   \n",
       "2    0.0    0.0    1.0   0.560     0.420   0.140        0.8395   \n",
       "3    0.0    0.0    1.0   0.570     0.490   0.145        0.8740   \n",
       "4    0.0    1.0    0.0   0.415     0.325   0.110        0.3580   \n",
       "\n",
       "   Shucked_weight  Viscera_weight  Shell_weight  \n",
       "0          0.6185          0.3125        0.3005  \n",
       "1          0.4785          0.2195        0.2750  \n",
       "2          0.3525          0.1845        0.2405  \n",
       "3          0.3525          0.1865        0.2350  \n",
       "4          0.1575          0.0670        0.1050  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.12 s\n",
      "Wall time: 5.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90615, 1145)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_df_train = brute_force_feat_engineering(train, numerical_features)\n",
    "\n",
    "new_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'significant_features_unique.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    new_feats = [line.strip() for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90615, 766)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_train = new_df_train[new_feats]\n",
    "final_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = list(final_df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'Rings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df_train\n",
    "y = train[TARGET]\n",
    "\n",
    "n_splits = 3\n",
    "sk10 = KFold(n_splits=n_splits, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_params_1 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.645831785537246, 'learning_rate': 0.13425614802120903, 'max_depth': 42, 'min_child_samples': 203, 'min_child_weight': 1.1732747258353804, 'min_split_gain': 0.9789391350146991, 'n_estimators': 639, 'num_leaves': 82, 'reg_alpha': 0.6380061711817115, 'reg_lambda': 0.6773770268796825}\n",
    "# lgbm_params_2 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.6293651061348586, 'learning_rate': 0.10096296495645429, 'max_depth': 31, 'min_child_samples': 225, 'min_child_weight': 3.3475628548945573, 'min_split_gain': 0.9997873682483012, 'n_estimators': 674, 'num_leaves': 48, 'reg_alpha': 0.005601881132355869, 'reg_lambda': 0.014315121505575882}\n",
    "# lgbm_params_3 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.5934335428151741, 'learning_rate': 0.13064405012196745, 'max_depth': 55, 'min_child_samples': 86, 'min_child_weight': 4.936258647279815, 'min_split_gain': 0.9987951237547827, 'n_estimators': 548, 'num_leaves': 49, 'reg_alpha': 0.7182759565064444, 'reg_lambda': 0.6464375985789672}\n",
    "# lgbm_params_4 = {'n_jobs': -1, 'random_state': 5, 'boosting': 'gbdt', 'colsample_bytree': 0.7783982819520713, 'learning_rate': 0.106849626777837, 'max_depth': 55, 'min_child_samples': 144, 'min_child_weight': 4.78221373340921, 'min_split_gain': 0.9962013661512532, 'n_estimators': 881, 'num_leaves': 73, 'reg_alpha': 0.6725415981122735, 'reg_lambda': 0.5691606484723241}\n",
    "# xgb_params_1 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 1910, 'n_estimators': 114, 'max_depth': 7, 'min_child_weight': 30, 'gamma': 0.07058151945617566, 'learning_rate': 0.11049946445523824, 'subsample': 0.8247120605397805, 'colsample_bytree': 0.8865698498028886, 'colsample_bylevel': 0.6702964552995809, 'colsample_bynode': 0.8408329349933361, 'reg_alpha': 0.10512246978624393, 'reg_lambda': 0.7492332818823758, 'max_bin': 470, 'grow_policy': 'depthwise', 'max_leaves': 168}\n",
    "# xgb_params_2 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 1847, 'n_estimators': 137, 'max_depth': 10, 'min_child_weight': 11, 'gamma': 0.009038694720046182, 'learning_rate': 0.03817953360871485, 'subsample': 0.8477644110167295, 'colsample_bytree': 0.9277554141706083, 'colsample_bylevel': 0.7237726407553582, 'colsample_bynode': 0.6770089667557697, 'reg_alpha': 0.24474247489256953, 'reg_lambda': 0.7195072858628302, 'max_bin': 378, 'grow_policy': 'depthwise', 'max_leaves': 254}\n",
    "# xgb_params_3 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 272, 'n_estimators': 230, 'max_depth': 9, 'min_child_weight': 11, 'gamma': 0.044843515145694, 'learning_rate': 0.019162331263840444, 'subsample': 0.931366685201835, 'colsample_bytree': 0.8799087346382896, 'colsample_bylevel': 0.6655591824860992, 'colsample_bynode': 0.8795942374460464, 'reg_alpha': 0.28610673472174625, 'reg_lambda': 0.8435492745641229, 'max_bin': 686, 'grow_policy': 'lossguide', 'max_leaves': 248}\n",
    "# xgb_params_4 = {'verbosity': None, 'objective': 'reg:squarederror', 'tree_method': 'exact', 'random_state': 865, 'n_estimators': 246, 'max_depth': 10, 'min_child_weight': 16, 'gamma': 0.07801675175603039, 'learning_rate': 0.0182283286494275, 'subsample': 0.9302927053213158, 'colsample_bytree': 0.9398793839324162, 'colsample_bylevel': 0.8842522189848564, 'colsample_bynode': 0.5975281202232402, 'reg_alpha': 0.25706875496430687, 'reg_lambda': 0.9594748896931249, 'max_bin': 990, 'grow_policy': 'depthwise', 'max_leaves': 75}\n",
    "# hist_params_1 = {'random_state': 5, 'learning_rate': 0.10725554777891158, 'max_iter': 770, 'max_leaf_nodes': 38, 'min_samples_leaf': 208, 'l2_regularization': 0.8537951071003338, 'max_bins': 245, 'max_depth': 23}\n",
    "# hist_params_2 = {'random_state': 5, 'learning_rate': 0.1458108188074952, 'max_iter': 314, 'max_leaf_nodes': 38, 'min_samples_leaf': 199, 'l2_regularization': 0.7905252458308586, 'max_bins': 255, 'max_depth': 53}\n",
    "# hist_params_3 = {'random_state': 5, 'learning_rate': 0.11235116550129322, 'max_iter': 941, 'max_leaf_nodes': 30, 'min_samples_leaf': 214, 'l2_regularization': 0.7327242793316453, 'max_bins': 253, 'max_depth': 64}\n",
    "# hist_params_4 = {'random_state': 5, 'learning_rate': 0.10273156315228334, 'max_iter': 841, 'max_leaf_nodes': 37, 'min_samples_leaf': 136, 'l2_regularization': 0.9094301806535087, 'max_bins': 248, 'max_depth': 33}\n",
    "# cat_params_1 = {'random_state': 5, 'verbose': False, 'n_estimators': 1000, 'learning_rate': 0.07845229262737155, 'max_depth': 7, 'subsample': 0.5027494707261151, 'colsample_bylevel': 0.8471975669265636, 'min_data_in_leaf': 10, 'l2_leaf_reg': 2.1925875095619385, 'random_strength': 0.1930409355787267, 'bagging_temperature': 0.9979677062970205}\n",
    "# cat_params_2 = {'random_state': 5, 'verbose': False, 'early_stopping_rounds': 100, 'loss_function': 'RMSE', 'n_estimators': 1349, 'learning_rate': 0.06263309535822247, 'max_depth': 6, 'subsample': 0.871730356394106, 'colsample_bylevel': 0.9511706719437459, 'min_data_in_leaf': 37, 'l2_leaf_reg': 2.5615538795939923, 'random_strength': 0.031857748396944496, 'bagging_temperature': 0.20981165087726864}\n",
    "# cat_params_3 = {'verbose': False, 'early_stopping_rounds': 100, 'loss_function': 'RMSE', 'n_estimators': 1884, 'learning_rate': 0.059602068583387596, 'max_depth': 6, 'subsample': 0.5002999540682693, 'colsample_bylevel': 0.7319888215923419, 'min_data_in_leaf': 19, 'l2_leaf_reg': 1.4226291560055486, 'random_strength': 0.026986589728532878, 'bagging_temperature': 0.8027411163556936, 'random_state': 832}\n",
    "# cat_params_4 = {'verbose': False, 'early_stopping_rounds': 100, 'loss_function': 'RMSE', 'n_estimators': 2435, 'learning_rate': 0.027540417908588785, 'max_depth': 7, 'subsample': 0.7284463831828892, 'colsample_bylevel': 0.8054488430220728, 'min_data_in_leaf': 13, 'l2_leaf_reg': 2.8222615876183643, 'random_strength': 0.04098122140187414, 'bagging_temperature': 0.8760690509958485, 'random_state': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=50))\n",
    "])\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nystroem', Nystroem(n_components=1000, random_state=5)),\n",
    "    ('ridge', Ridge(alpha=0.1, random_state=5))\n",
    "])\n",
    "\n",
    "# Manually set names\n",
    "knn_pipeline.name = 'KNN'  # Custom name\n",
    "ridge_pipeline.name = 'Nystroem Ridge'  # Custom name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LGBMRegressor(n_jobs=-1, random_state=5),\n",
    "    # LGBMRegressor(**lgbm_params_1),\n",
    "    # LGBMRegressor(**lgbm_params_2),\n",
    "    # LGBMRegressor(**lgbm_params_3),\n",
    "    # LGBMRegressor(**lgbm_params_4),\n",
    "    XGBRegressor(random_state=5),\n",
    "    # XGBRegressor(**xgb_params_1),\n",
    "    # XGBRegressor(**xgb_params_2),\n",
    "    # XGBRegressor(**xgb_params_3),\n",
    "    # XGBRegressor(**xgb_params_4),\n",
    "    RandomForestRegressor(random_state=5),\n",
    "    ExtraTreesRegressor(random_state=5),\n",
    "    HistGradientBoostingRegressor(random_state=5),\n",
    "    # HistGradientBoostingRegressor(**hist_params_1),\n",
    "    # HistGradientBoostingRegressor(**hist_params_2),\n",
    "    # HistGradientBoostingRegressor(**hist_params_3),\n",
    "    # HistGradientBoostingRegressor(**hist_params_4),\n",
    "    CatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100),\n",
    "    # CatBoostRegressor(**cat_params_1),\n",
    "    # CatBoostRegressor(**cat_params_2),\n",
    "    # CatBoostRegressor(**cat_params_3),\n",
    "    # CatBoostRegressor(**cat_params_4),\n",
    "    knn_pipeline,\n",
    "    ridge_pipeline,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error (RMSLE).\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    # # Add post processing step if required\n",
    "    # y_pred_processed = np.floor(y_pred)\n",
    "    \n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "    MLA_compare = pd.DataFrame(columns=['MLA Name', \n",
    "                                        'MLA Parameters', \n",
    "                                        'MLA Train ROC AUC', \n",
    "                                        'MLA Test ROC AUC', \n",
    "                                        'MLA Test ROC AUC Std', \n",
    "                                        'MLA Time'])\n",
    "    \n",
    "    def evaluate_model(alg, idx):\n",
    "        if hasattr(alg, 'name'):\n",
    "            MLA_name = alg.name\n",
    "        else:\n",
    "            MLA_name = alg.__class__.__name__\n",
    "        features = important_features.get(MLA_name, [])\n",
    "\n",
    "        # Check if the list of important features is empty\n",
    "        if len(features) == 0:\n",
    "            # If empty, return results with zero values\n",
    "            print(f'Skipping {MLA_name} due to no important features.')\n",
    "            return {\n",
    "                'MLA Name': MLA_name,\n",
    "                'MLA Parameters': str(alg.get_params()),\n",
    "                'MLA Train ROC': 0,\n",
    "                'MLA Test ROC': 0,\n",
    "                'MLA Test ROC Std': 0,\n",
    "                'MLA Time': \"0 min 0.00 sec\",\n",
    "            }\n",
    "        \n",
    "        cv_results = cross_validate(alg, \n",
    "                                    X[features], \n",
    "                                    y, cv=cv_split, \n",
    "                                    scoring=rmsle_scorer, \n",
    "                                    return_train_score=True, \n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Time formatting\n",
    "        mean_fit_time = cv_results['fit_time'].mean()\n",
    "        minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "        # Results population\n",
    "        result = {\n",
    "            'MLA Name': MLA_name,\n",
    "            'MLA Parameters': str(alg.get_params()),\n",
    "            'MLA Train ROC AUC': -cv_results['train_score'].mean(),\n",
    "            'MLA Test ROC AUC': -cv_results['test_score'].mean(),\n",
    "            'MLA Test ROC AUC Std': cv_results['test_score'].std(),\n",
    "            'MLA Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "        }\n",
    "\n",
    "        print(f'Done with {MLA_name}.')\n",
    "        return result\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(models)]\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results_list.append(result)\n",
    "\n",
    "    MLA_compare = pd.DataFrame(results_list)\n",
    "\n",
    "    MLA_compare.sort_values(by=['MLA Test ROC AUC'], ascending=True, inplace=True)\n",
    "    MLA_compare.to_csv(f'{experiment_name}_results.csv', index=False)\n",
    "\n",
    "    return MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = {}\n",
    "\n",
    "for model in models:\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    baseline_features[model_name] = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with HistGradientBoostingRegressor.\n",
      "Done with LGBMRegressor.\n",
      "Done with Nystroem Ridge.\n",
      "Done with XGBRegressor.\n",
      "Done with KNN.\n",
      "Done with CatBoostRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "CPU times: total: 1min 53s\n",
      "Wall time: 2h 28min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC Std</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'verbose': False, 'r...</td>\n",
       "      <td>0.134350</td>\n",
       "      <td>0.151143</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>9 min 58.23 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.141116</td>\n",
       "      <td>0.151268</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>1 min 35.41 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HistGradientBoostingRegressor</td>\n",
       "      <td>{'categorical_features': None, 'early_stopping...</td>\n",
       "      <td>0.143058</td>\n",
       "      <td>0.151474</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1 min 43.31 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nystroem Ridge</td>\n",
       "      <td>{'memory': None, 'steps': [('scaler', Standard...</td>\n",
       "      <td>0.150966</td>\n",
       "      <td>0.152246</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0 min 42.41 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'memory': None, 'steps': [('scaler', Standard...</td>\n",
       "      <td>0.149267</td>\n",
       "      <td>0.152512</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0 min 5.39 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>0.124118</td>\n",
       "      <td>0.154710</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>24 min 16.79 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>0.059579</td>\n",
       "      <td>0.155366</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>130 min 20.96 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'criter...</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.156090</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>70 min 34.28 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MLA Name  \\\n",
       "5              CatBoostRegressor   \n",
       "0                  LGBMRegressor   \n",
       "4  HistGradientBoostingRegressor   \n",
       "7                 Nystroem Ridge   \n",
       "6                            KNN   \n",
       "1                   XGBRegressor   \n",
       "2          RandomForestRegressor   \n",
       "3            ExtraTreesRegressor   \n",
       "\n",
       "                                      MLA Parameters  MLA Train ROC AUC  \\\n",
       "5  {'loss_function': 'RMSE', 'verbose': False, 'r...           0.134350   \n",
       "0  {'boosting_type': 'gbdt', 'class_weight': None...           0.141116   \n",
       "4  {'categorical_features': None, 'early_stopping...           0.143058   \n",
       "7  {'memory': None, 'steps': [('scaler', Standard...           0.150966   \n",
       "6  {'memory': None, 'steps': [('scaler', Standard...           0.149267   \n",
       "1  {'objective': 'reg:squarederror', 'base_score'...           0.124118   \n",
       "2  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...           0.059579   \n",
       "3  {'bootstrap': False, 'ccp_alpha': 0.0, 'criter...           0.000648   \n",
       "\n",
       "   MLA Test ROC AUC  MLA Test ROC AUC Std           MLA Time  \n",
       "5          0.151143              0.000197    9 min 58.23 sec  \n",
       "0          0.151268              0.000220    1 min 35.41 sec  \n",
       "4          0.151474              0.000224    1 min 43.31 sec  \n",
       "7          0.152246              0.000170    0 min 42.41 sec  \n",
       "6          0.152512              0.000161     0 min 5.39 sec  \n",
       "1          0.154710              0.000097   24 min 16.79 sec  \n",
       "2          0.155366              0.000408  130 min 20.96 sec  \n",
       "3          0.156090              0.000212   70 min 34.28 sec  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline_models = evaluate_models(models, X, y, baseline_features, sk10, f'{experiment_name}')\n",
    "baseline_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated features (leaving just 1 of each pair)\n",
    "# Leave features highly correlated with the target\n",
    "df_no_corr = X.copy()\n",
    "correlation_matrix_spear = df_no_corr.corr(method='spearman').abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_spear = correlation_matrix_spear.where(np.triu(np.ones(correlation_matrix_spear.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than a threshold (e.g., 0.9 in this case)\n",
    "to_drop_spear = [column for column in upper_spear.columns if any(upper_spear[column] >= 0.9)]\n",
    "\n",
    "# Drop features\n",
    "df_reduced_spear = df_no_corr.drop(to_drop_spear, axis=1)\n",
    "\n",
    "# Get list of low correlation features excluding TARGET\n",
    "low_corr_feats_spear = list(df_reduced_spear.columns)\n",
    "\n",
    "with open('low_corr_spear.txt', 'w') as f:\n",
    "    f.write(str(low_corr_feats_spear))\n",
    "    f.write('\\n')\n",
    "\n",
    "# Print the high correlation features effect\n",
    "# Both pre and post drop dfs contain the TARGET\n",
    "print(f\"Dropped {len(to_drop_spear)} highly correlated features.\\nOld Shape of the dataset was {df_no_corr.shape}\\nNew shape of the dataset is {df_reduced_spear.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "no_corr_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    no_corr_features[model_name] = list(df_reduced_spear.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "no_corr_models = evaluate_models(models, df_reduced_spear, y, no_corr_features, sk10, f'{experiment_name}_corr')\n",
    "no_corr_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_importance_features = {}\n",
    "\n",
    "# for model in models:\n",
    "#     model_name = model.__class__.__name__\n",
    "\n",
    "#     try:\n",
    "#         # Initialize array to store feature importances\n",
    "#         feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "#         # Loop through each fold and calculate the feature importances\n",
    "#         for train_index, test_index in sk10.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#             y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             # Get the feature importances and them to the total\n",
    "#             feature_importances += model.feature_importances_\n",
    "\n",
    "#         feature_importances /= n_splits\n",
    "\n",
    "#         feature_importances_dict = dict(zip(X.columns, feature_importances))\n",
    "\n",
    "#         df = pd.DataFrame.from_dict(feature_importances_dict, orient='index')\n",
    "\n",
    "#         # Resetting index with a name for the column\n",
    "#         df = df.reset_index().rename(columns={'index': 'Feature', 0: 'Avg_Feat_Importance'})\n",
    "#         df.sort_values(by='Avg_Feat_Importance', ascending=False, inplace=True)\n",
    "\n",
    "#         # Save to CSV\n",
    "#         df.to_csv(f'{model_name}_feature_importances.csv')\n",
    "\n",
    "#         fi_threshold = 0\n",
    "\n",
    "#         fi_feats = df[df['Avg_Feat_Importance'] > fi_threshold]['Feature'].tolist()\n",
    "\n",
    "#         feat_importance_features[model_name] = fi_feats\n",
    "#         print(f'Done with {model_name}')\n",
    "\n",
    "#     except AttributeError:\n",
    "#         feat_importance_features[model_name] = list(X.columns)\n",
    "#         print(f'{model_name} does not have feature_importances_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('featimp_features.txt', mode='w') as f:\n",
    "#     pprint(feat_importance_features, stream=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random feature for X\n",
    "np.random.seed(5)\n",
    "X['random_control_feature'] = np.round(np.random.uniform(-2, 2, X.shape[0]), 6)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "perm_cv = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "perm_importances = {model.__class__.__name__: [] for model in models}\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(perm_cv.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        model.fit(X_train, y_train)\n",
    "        # Calculate permutation importance\n",
    "        result = permutation_importance(model, X_test, y_test, n_repeats=5, random_state=5, n_jobs=-1, scoring=rmsle_scorer)\n",
    "        perm_importances[model_name].append(result.importances_mean)\n",
    "        print(f'Done with {model_name}.')\n",
    "    \n",
    "    print(f'Done with Fold {i+1}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Average importances across folds and export to CSV\n",
    "for model_name, importances in perm_importances.items():\n",
    "    avg_importance = np.mean(importances, axis=0)\n",
    "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': avg_importance})\n",
    "    importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    # Export to CSV\n",
    "    importance_df.to_csv(f'.\\permutation_importances\\{model_name}_permutation_importance.csv', index=False)\n",
    "\n",
    "print('Done with Permuation Importances', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'permutation_importances'\n",
    "\n",
    "# Initialize a dictionary for the features\n",
    "perm_important_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    csv_path = os.path.join(directory, f'{model_name}_permutation_importance.csv')\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Check for 'random_control_feature' and its importance\n",
    "        if 'random_control_feature' in df['Feature'].values:\n",
    "            random_feature_importance = df.loc[df['Feature'] == 'random_control_feature', 'Importance'].iloc[0]\n",
    "        else:\n",
    "            random_feature_importance = 0\n",
    "\n",
    "        # Determine the threshold\n",
    "        threshold = max(0, random_feature_importance)\n",
    "\n",
    "        # Filter features where importance is greater than 0\n",
    "        important_feats_filtered = df[df['Importance'] > threshold]['Feature'].tolist()\n",
    "\n",
    "        # Reorder important_feats based on the predefined features_list\n",
    "        important_feats_ordered = [feat for feat in features_list if feat in important_feats_filtered]\n",
    "\n",
    "        # Add to importance dictionary\n",
    "        perm_important_features[model_name] = important_feats_ordered\n",
    "\n",
    "    else:\n",
    "        print(f'CSV file for {model_name} not found.')\n",
    "\n",
    "print('Done getting important features dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perm_important_features.txt', mode='w') as f:\n",
    "    pprint(perm_important_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "perm_importance_models = evaluate_models(models, X, y, perm_important_features, sk10, f'{experiment_name}_permimp')\n",
    "perm_importance_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SelectKBest with f_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_list = []\n",
    "kbest_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # Select whichever one had a better CV score generally\n",
    "    # Also, consider computational expense and accuracy balance\n",
    "    \n",
    "    features = perm_important_features[model_name]\n",
    "    # features = list(df_reduced_spear.columns)\n",
    "\n",
    "    # incase there is no feature that had importance, go to the next model\n",
    "    if len(features) == 0:\n",
    "        continue\n",
    "\t\n",
    "    X_kbest = X[features]\n",
    "    best_score = 0\n",
    "    best_k = 0\n",
    "    best_features = []\n",
    "\n",
    "    # Iterate over k from 1 to number of features\n",
    "    for k in range(1, len(features) + 1):\n",
    "        print(f'currently running {k} features on {model_name}')\n",
    "        # Apply SelectKBest\n",
    "        selector = SelectKBest(f_regression, k=k)\n",
    "        X_new = selector.fit_transform(X_kbest, y)\n",
    "\n",
    "        # Get the selected feature names\n",
    "        selected_features = X_kbest.columns[selector.get_support()]\n",
    "\n",
    "        # Evaluate the model\n",
    "        # model = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "        rmsle_scores = cross_validate(model, X_new, y, cv=sk10, scoring=rmsle_scorer, n_jobs=-1)\n",
    "        mean_rmsle_scores = rmsle_scores['test_score'].mean()\n",
    "\n",
    "        if mean_rmsle_scores > best_score:\n",
    "            best_k = k\n",
    "            best_score = mean_rmsle_scores\n",
    "            best_features = list(selected_features)\n",
    "\n",
    "    best_features_list.append({'k': best_k,\n",
    "                    'Selected Features': best_features,\n",
    "                    'RMSLE Score': best_score,\n",
    "                    'Model Name': model_name})\n",
    "    \n",
    "    kbest_features[model_name] = best_features\n",
    "\n",
    "best_features_df = pd.DataFrame(best_features_list)\n",
    "\n",
    "best_features_df.sort_values(by='RMSLE Score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kbest_features.txt', mode='w') as f:\n",
    "    pprint(kbest_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize empty dictionary for RFECV features\n",
    "rfecv_features = {}\n",
    "\n",
    "for alg in models:\n",
    "    # set name\n",
    "    MLA_name = alg.__class__.__name__\n",
    "\t\t\n",
    "    features = perm_important_features[MLA_name]\n",
    "\n",
    "    # incase there is no feature that had importance, go to the next model\n",
    "    if len(features) == 0:\n",
    "        continue\n",
    "\t\n",
    "    X_rfecv = X[features]\n",
    "\n",
    "    try:\n",
    "        print(f'Starting with {MLA_name}')\n",
    "        # Create the RFECV object and rank each feature\n",
    "        selector = RFECV(alg, cv=sk10, step=1, scoring=rmsle_scorer, verbose=2)\n",
    "        selector = selector.fit(X_rfecv, y)\n",
    "\n",
    "        selected_features = list(X_rfecv.columns[selector.support_])\n",
    "\n",
    "        # Reorder selected_features based on the predefined features_list\n",
    "        selected_features_ordered = [feat for feat in features_list if feat in selected_features]\n",
    "\n",
    "        rfecv_features[MLA_name] = selected_features_ordered\n",
    "\n",
    "        print(f'Done with {MLA_name}', end='\\n\\n')\n",
    "    \n",
    "    except ValueError:\n",
    "        # In case of an error, keep the original order but filtered by features_list\n",
    "        features_filtered = [feat for feat in features_list if feat in features]\n",
    "        rfecv_features[MLA_name] = features_filtered\n",
    "        print(f'{MLA_name} does not have coef_ or feature_importances_', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rfecv_features.txt', mode='w') as f:\n",
    "    pprint(rfecv_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "rfecv_models = evaluate_models(models, X, y, rfecv_features, sk10, f'{experiment_name}_rfecv')\n",
    "rfecv_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize empty dictionary for SFS features\n",
    "sfs_features = {}\n",
    "\n",
    "for model in models:\n",
    "    # set name\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    try:\n",
    "\n",
    "        # features = perm_important_features[model_name]    \n",
    "        # features = kbest_features[model_name]\n",
    "        # features = feat_importance_features[MLA_name]\n",
    "        # features = rfecv_features[MLA_name]\n",
    "        features = baseline_features[model_name]\n",
    "\n",
    "        # incase there is no feature that had importance, go to the next model\n",
    "        if len(features) == 0:\n",
    "            continue\n",
    "        \n",
    "        X_sfs = X[features]\n",
    "\n",
    "        print(f'Running backward feature selection with {model_name}')\n",
    "\n",
    "        sfs = SFS(model,\n",
    "            k_features='best',\n",
    "            forward=False,\n",
    "            floating=False,\n",
    "            scoring=rmsle_scorer,\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "            cv=sk10)\n",
    "        \n",
    "        sfs = sfs.fit(X_sfs, y)\n",
    "\n",
    "        # Get the selected features index\n",
    "        selected_sfs_idx = list(sfs.k_feature_idx_)\n",
    "\n",
    "        # Get the feature names\n",
    "        selected_sfs_feats = X_sfs.columns[selected_sfs_idx]\n",
    "\n",
    "        selected_features = list(selected_sfs_feats)\n",
    "\n",
    "        # Reorder selected_features based on the predefined features_list\n",
    "        selected_features_ordered = [feat for feat in features_list if feat in selected_features]\n",
    "\n",
    "        sfs_features[model_name] = selected_features_ordered\n",
    "\n",
    "        print(f'Done with {model_name}', end='\\n\\n')\n",
    "\n",
    "    except KeyError:\n",
    "        print(f'{model_name} not in the dictionary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_features = {'CatBoostRegressor': ['Sex_I','Sex_M','Length','Diameter','Height','Whole_weight','Shucked_weight','Viscera_weight','Shell_weight'],\n",
    " 'ExtraTreesRegressor': ['Sex_I',  'Length',  'Diameter',  'Height',  'Whole_weight',  'Shucked_weight',  'Viscera_weight',  'Shell_weight'],\n",
    " 'HistGradientBoostingRegressor': ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'LGBMRegressor': ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'RandomForestRegressor': ['Sex_F', 'Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'XGBRegressor': ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight'],\n",
    " 'KNN': ['Sex_I', 'Length', 'Whole_weight', 'Shucked_weight', 'Shell_weight'],\n",
    " 'Nystroem Ridge': ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sfs_features_knn_ridge.txt', mode='w') as f:\n",
    "    pprint(sfs_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "sfs_models = evaluate_models(models, X, y, sfs_features, sk10, f'{experiment_name}_sfs_8models')\n",
    "sfs_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_features = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMRegressor(n_jobs=-1, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_final = model1.fit(X[sfs_features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1_final.predict(test[sfs_features])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred, columns=['Rings'])\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], pred_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_lgbm_postprocess_asint_0.150577.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMRegressor(n_jobs=-1, random_state=5)\n",
    "model1b = LGBMRegressor(**lgbm_params_1)\n",
    "model1c = LGBMRegressor(**lgbm_params_2)\n",
    "model1d = LGBMRegressor(**lgbm_params_3)\n",
    "model1e = LGBMRegressor(**lgbm_params_4)\n",
    "model2 = XGBRegressor(random_state=5)\n",
    "model2b = XGBRegressor(**xgb_params_1)\n",
    "model2c = XGBRegressor(**xgb_params_2)\n",
    "model2d = XGBRegressor(**xgb_params_3)\n",
    "model2e = XGBRegressor(**xgb_params_4)\n",
    "model3 = RandomForestRegressor(random_state=5)\n",
    "model4 = ExtraTreesRegressor(random_state=5)\n",
    "model5 = HistGradientBoostingRegressor(random_state=5)\n",
    "model5b = HistGradientBoostingRegressor(**hist_params_1)\n",
    "model5c = HistGradientBoostingRegressor(**hist_params_2)\n",
    "model5d = HistGradientBoostingRegressor(**hist_params_3)\n",
    "model5e = HistGradientBoostingRegressor(**hist_params_4)\n",
    "model6 = CatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "model6b = CatBoostRegressor(**cat_params_1)\n",
    "model6c = CatBoostRegressor(**cat_params_2)\n",
    "model6d = CatBoostRegressor(**cat_params_3)\n",
    "model6e = CatBoostRegressor(**cat_params_4)\n",
    "model7 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=86, algorithm='kd_tree'))\n",
    "])\n",
    "model8 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nystroem', Nystroem(n_components=974, random_state=974)),\n",
    "    ('ridge', Ridge(alpha=0.1, max_iter=83157, random_state=1879, solver='svd'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features for Competition + Original dataset down to SFS for all models (Experiment Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_feats = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model2_feats = ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model3_feats = ['Sex_F', 'Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model4_feats = ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model5_feats = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model6_feats = ['Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "model7_feats = ['Sex_I', 'Length', 'Whole_weight', 'Shucked_weight', 'Shell_weight']\n",
    "model8_feats = ['Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']\n",
    "\n",
    "X_lgbm = X[model1_feats]\n",
    "X_xgb = X[model2_feats]\n",
    "X_rf = X[model3_feats]\n",
    "X_extrat = X[model4_feats]\n",
    "X_hist = X[model5_feats]\n",
    "X_cat = X[model6_feats]\n",
    "X_knn = X[model7_feats]\n",
    "X_ridge = X[model8_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # class_weight_option = trial.suggest_categorical('class_weight', ['none', 'balanced', 'custom'])\n",
    "    # if class_weight_option == 'none':\n",
    "    #     class_weight = None\n",
    "    # elif class_weight_option == 'balanced':\n",
    "    #     class_weight = 'balanced'\n",
    "    # else:\n",
    "    #     # For multi-class, you could define a range or specific values to test\n",
    "    #     weight_for_class_0 = trial.suggest_float('weight_for_class_0', 0.1, 10.0)\n",
    "    #     weight_for_class_1 = trial.suggest_float('weight_for_class_1', 0.1, 10.0)\n",
    "    #     weight_for_class_2 = trial.suggest_float('weight_for_class_2', 0.1, 10.0)\n",
    "    #     weight_for_class_3 = trial.suggest_float('weight_for_class_3', 0.1, 10.0)\n",
    "    #     weight_for_class_4 = trial.suggest_float('weight_for_class_4', 0.1, 10.0)\n",
    "    #     weight_for_class_5 = trial.suggest_float('weight_for_class_5', 0.1, 10.0)\n",
    "    #     weight_for_class_6 = trial.suggest_float('weight_for_class_6', 0.1, 10.0)\n",
    "    #     class_weight = {0: weight_for_class_0, 1: weight_for_class_1, 2: weight_for_class_2, 3: weight_for_class_3, 4: weight_for_class_4, 5: weight_for_class_5, 6: weight_for_class_6}\n",
    "\n",
    "    # param = {\n",
    "    #     # 'objective': 'multiclass',\n",
    "    #     # 'num_class': 7,\n",
    "    #     # 'boosting_type': trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n",
    "    #     # 'class_weight': class_weight,\n",
    "    #     # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "    #     # 'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.5),\n",
    "    #     # 'max_depth': trial.suggest_int('max_depth', -1, 64),\n",
    "    #     # 'min_child_samples': trial.suggest_int('min_child_samples', 5, 500),\n",
    "    #     # 'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "    #     # 'min_split_gain': trial.suggest_float('min_split_gain', 0.5, 1.0),\n",
    "    #     # 'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "    #     # 'num_leaves': trial.suggest_int('num_leaves', 2, 1000),\n",
    "    #     # 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "    #     # 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "    #     # 'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "    #     'random_state': 5,\n",
    "    #     'n_jobs': -1,\n",
    "    #     }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'boosting_type': trial.suggest_categorical('boosting', ['gbdt', 'goss']),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.35, 0.8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 20, 64),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 50, 250),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.9, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1000),\n",
    "        'n_jobs': -1,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 40, 200),\n",
    "        'random_state': 5,\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        }\n",
    "    \n",
    "    rmsle_scores = []\n",
    "    \n",
    "    for train_index, test_index in sk10.split(X_lgbm, y):\n",
    "        X_train, X_test = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = LGBMRegressor(**param)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "    \n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# Using median pruner\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HistGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.15),\n",
    "    #     # 'max_iter': trial.suggest_int('max_iter', 50, 5000),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20, 1000),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 1000),\n",
    "    #     # 'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 1.0),\n",
    "    #     # 'max_bins': trial.suggest_int('max_bins', 10, 255),\n",
    "    #     'max_depth': trial.suggest_int('max_depth', 2, 64),\n",
    "    #     'random_state': 5,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.15),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 1000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 160),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 100, 300),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 1.0),\n",
    "        'max_bins': trial.suggest_int('max_bins', 200, 255),\n",
    "        'max_depth': trial.suggest_int('max_depth', 18, 64),\n",
    "        'random_state': 5,\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_hist, y)):\n",
    "        X_train, X_test = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = HistGradientBoostingRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(rmsle, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    param = {\n",
    "        # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "        # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "        # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']), # Classifier Problems\n",
    "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']), # Regression Problems\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 10, 3000),\n",
    "        # 'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "        # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 100, 3000, log=True) or None,\n",
    "        # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.1),\n",
    "        # 'min_samples_leaf': trial.suggest_int('min_samples_leaf',1, 500),\n",
    "        # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 500),\n",
    "        # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n",
    "        'random_state': 5,\n",
    "        # 'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    # # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    # param = {\n",
    "        # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "        # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "        # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 10, 3000),\n",
    "        # 'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "        # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 100, 3000, log=True) or None,\n",
    "        # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.1),\n",
    "        # 'min_samples_leaf': trial.suggest_int('min_samples_leaf',1, 500),\n",
    "        # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 500),\n",
    "        # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n",
    "        # 'random_state': 5,\n",
    "        # 'n_jobs': -1,\n",
    "    # }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_extrat, y)):\n",
    "        X_train, X_test = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = ExtraTreesRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(rmsle, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "    #     # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "    #     # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "    #     # 'max_depth': trial.suggest_int('max_depth', 10, 1000, log=True) or None,\n",
    "    #     # 'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 1000, log=True) or None,\n",
    "    #     # 'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "    #     # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "    #     # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 200),\n",
    "    #     # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.1),\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "    #     'random_state': 5,\n",
    "    #     'n_jobs': -1,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'ccp_alpha': 0.0,\n",
    "        'class_weight': None,\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 30, log=True),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 300, 700, log=True) or None,\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'log2',\n",
    "        'criterion': 'entropy',\n",
    "        'n_estimators': 1000,\n",
    "        'random_state': 5,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_rf, y)):\n",
    "        X_train, X_test = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(roc_auc, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if roc_auc < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    param = {\n",
    "        # 'iterations': trial.suggest_int('iterations', 50, 2000),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.3, log=True),\n",
    "        # 'depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.05, 1),\n",
    "        # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "        # 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        # 'l2_leaf_reg': trial.suggest_float('l2_reg', 1e-2, 10),\n",
    "        # 'random_strength': trial.suggest_float('random_strength', 1e-2, 10),\n",
    "        # 'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        # 'random_state': 5,\n",
    "        # 'verbose': False,\n",
    "        # 'early_stopping_rounds': 100,\n",
    "        # 'loss_function': 'RMSE',\n",
    "    }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        # 'iterations': trial.suggest_int('iterations', 1700, 2000),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 3),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-6, 0.3),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 2000),\n",
    "        # 'random_state': 5,\n",
    "        'verbose': False,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'loss_function': 'RMSE',\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_cat, y)):\n",
    "        X_train, X_test = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "        # 'verbosity': None,\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        # 'num_class': 3,  # Set this to the number of classes if using 'multi:softmax' objective\n",
    "        # 'random_state': 5,\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'tree_method': trial.suggest_categorical('tree_method', ['exact', 'approx', 'hist']),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 2, 64),\n",
    "        # 'min_child_weight': trial.suggest_int('min_child_weight', 1, 40),\n",
    "        # 'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.5),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "        # 'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        # 'max_bin': trial.suggest_int('max_bin', 256, 1024),\n",
    "        # 'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        # 'max_leaves': trial.suggest_int('max_leaves', 0, 255)\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'verbosity': None,\n",
    "        'objective': 'reg:squarederror',\n",
    "        # 'num_class': 3,  # Set this to the number of classes if using 'multi:softmax' objective\n",
    "        'random_state': trial.suggest_int('random_state', 1, 2000),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 250),\n",
    "        'tree_method': 'exact',\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 9, 30),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.8, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 0.9),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.45, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.45),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.6, 1.0),\n",
    "        'max_bin': trial.suggest_int('max_bin', 256, 1024),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 255)\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_xgb, y)):\n",
    "        X_train, X_test = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = XGBRegressor(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # # Suggest scaler type\n",
    "    # scaler_type = trial.suggest_categorical('scaler', ['standard', 'minmax', 'maxabs', 'robust', 'quantile'])\n",
    "    # if scaler_type == 'standard':\n",
    "    #     scaler = StandardScaler()\n",
    "    # elif scaler_type == 'minmax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif scaler_type == 'maxabs':\n",
    "    #     scaler = MaxAbsScaler()\n",
    "    # elif scaler_type == 'robust':\n",
    "    #     scaler = RobustScaler()\n",
    "    # else:\n",
    "    #     scaler = QuantileTransformer()\n",
    "\n",
    "    # # Suggest KNN params\n",
    "    # n_neighbors = trial.suggest_int('n_neighbors', 1, 100)\n",
    "    # weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    # algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "\n",
    "    # # Setup the pipeline\n",
    "    # pipeline = Pipeline([\n",
    "    #     # ('scaler', scaler),\n",
    "    #     ('scaler', StandardScaler()),\n",
    "    #     ('knn', KNeighborsRegressor(\n",
    "    #         n_neighbors=50,\n",
    "    #         # n_neighbors=n_neighbors,\n",
    "    #         # weights=weights,\n",
    "    #         algorithm=algorithm,\n",
    "    #         ))\n",
    "    # ])\n",
    "\n",
    "\n",
    "    # Final KNN params\n",
    "    # Suggest scaler type\n",
    "    scaler_type = trial.suggest_categorical('scaler', ['standard', 'minmax', 'maxabs'])\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "        \n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 40, 100)\n",
    "    weights = 'uniform'\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['auto', 'kd_tree'])\n",
    "    \n",
    "    # Setup the final pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('knn', KNeighborsRegressor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            algorithm=algorithm,\n",
    "            ))\n",
    "    ])\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_knn, y)):\n",
    "        X_train, X_test = X_knn.iloc[train_index], X_knn.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    # # Suggest scaler type\n",
    "    # scaler_type = trial.suggest_categorical('scaler', ['standard', 'robust', 'quantile'])\n",
    "    # if scaler_type == 'standard':\n",
    "    #     scaler = StandardScaler()\n",
    "    # elif scaler_type == 'minmax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif scaler_type == 'maxabs':\n",
    "    #     scaler = MaxAbsScaler()\n",
    "    # elif scaler_type == 'robust':\n",
    "    #     scaler = RobustScaler()\n",
    "    # else:\n",
    "    #     scaler = QuantileTransformer()\n",
    "\n",
    "    # # Suggest ridge params\n",
    "    # alpha = trial.suggest_float('alpha', 0.1, 1000)\n",
    "    # max_iter = trial.suggest_int('max_iter', 1, 200000)\n",
    "    # solver = trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])\n",
    "\n",
    "\n",
    "    # # Suggest Nystroem params\n",
    "#     components = trial.suggest_int('components', 1, 1000)\n",
    "    \n",
    "#     # Setup the pipeline\n",
    "#     pipeline = Pipeline([\n",
    "#     # ('scaler', scaler),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('nystroem', Nystroem(\n",
    "#         n_components=components, \n",
    "#         random_state=5,\n",
    "#         )),\n",
    "#     ('ridge', Ridge(\n",
    "#         # alpha=alpha,\n",
    "#         # max_iter=max_iter, \n",
    "#         # solver=solver, \n",
    "#         random_state=5,\n",
    "#         ))\n",
    "# ])\n",
    "\n",
    "    # Group Parameters after individual tunings\n",
    "    # Suggest scaler type\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Suggest ridge params\n",
    "    # alpha = trial.suggest_float('alpha', 0.1, 0.3)\n",
    "    max_iter = trial.suggest_int('max_iter', 1, 200000)\n",
    "    solver = trial.suggest_categorical('solver', ['auto', 'svd'])\n",
    "    random_state_ridge = trial.suggest_int('random_state_ridge', 1, 2000)\n",
    "\n",
    "\n",
    "    # Suggest Nystroem params\n",
    "    components = trial.suggest_int('components', 600, 3000)\n",
    "    random_state_nystroem = trial.suggest_int('random_state_nystroem', 1, 2000)\n",
    "    \n",
    "    # Setup the pipeline\n",
    "    pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('nystroem', Nystroem(\n",
    "        n_components=components, \n",
    "        random_state=random_state_nystroem\n",
    "        )),\n",
    "    ('ridge', Ridge(\n",
    "        alpha=0.1, \n",
    "        max_iter=max_iter, \n",
    "        solver=solver, \n",
    "        random_state=random_state_ridge\n",
    "        ))\n",
    "])\n",
    "\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_ridge, y)):\n",
    "        X_train, X_test = X_ridge.iloc[train_index], X_ridge.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict(X_test)\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_test, preds))\n",
    "        rmsle_scores.append(rmsle)\n",
    "\n",
    "        # # Report intermediate objective value\n",
    "        # trial.report(rmsle, i)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned('RMSLE score higher than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if rmsle < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Weights Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model1b_results, model1c_results, model1d_results, model1e_results, model2_results, model3_results, model3b_results, model3c_results, model4_results, model4b_results, model4c_results, model4d_results, model4e_results, model5_results, model5b_results, model5c_results, model5d_results, model5e_results, model5f_results, model6_results, model6b_results, model6c_results, model6d_results, model6e_results, model7_results, model8_results, y_test_list = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train_lgbm, X_test_lgbm = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "    X_train_xgb, X_test_xgb = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "    X_train_rf, X_test_rf = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "    X_train_extrat, X_test_extrat = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "    X_train_hist, X_test_hist = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "    X_train_cat, X_test_cat = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "    X_train_knn, X_test_knn = X_knn.iloc[train_index], X_knn.iloc[test_index]\n",
    "    X_train_ridge, X_test_ridge = X_ridge.iloc[train_index], X_ridge.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    print('Running LGBM')\n",
    "\n",
    "    model1.fit(X_train_lgbm, y_train)\n",
    "    model1_results.append(model1.predict(X_test_lgbm))\n",
    "\n",
    "    model1b.fit(X_train_lgbm, y_train)\n",
    "    model1b_results.append(model1b.predict(X_test_lgbm))\n",
    "\n",
    "    model1c.fit(X_train_lgbm, y_train)\n",
    "    model1c_results.append(model1c.predict(X_test_lgbm))\n",
    "\n",
    "    model1d.fit(X_train_lgbm, y_train)\n",
    "    model1d_results.append(model1d.predict(X_test_lgbm))\n",
    "\n",
    "    model1e.fit(X_train_lgbm, y_train)\n",
    "    model1e_results.append(model1e.predict(X_test_lgbm))\n",
    "\n",
    "    print('Running XGBoost')\n",
    "\n",
    "    model2.fit(X_train_xgb, y_train)\n",
    "    model2_results.append(model2.predict(X_test_xgb))\n",
    "\n",
    "    model2b.fit(X_train_xgb, y_train)\n",
    "    model2b_results.append(model2b.predict(X_test_xgb))\n",
    "\n",
    "    model2c.fit(X_train_xgb, y_train)\n",
    "    model2c_results.append(model2c.predict(X_test_xgb))\n",
    "\n",
    "    model2d.fit(X_train_xgb, y_train)\n",
    "    model2d_results.append(model2d.predict(X_test_xgb))\n",
    "\n",
    "    model2e.fit(X_train_xgb, y_train)\n",
    "    model2e_results.append(model2e.predict(X_test_xgb))\n",
    "\n",
    "    print('Running Random Forest')\n",
    "\n",
    "    model3.fit(X_train_rf, y_train)\n",
    "    model3_results.append(model3.predict(X_test_rf))\n",
    "\n",
    "    print('Running ExtraTrees')\n",
    "\n",
    "    model4.fit(X_train_extrat, y_train)\n",
    "    model4_results.append(model4.predict(X_test_extrat))\n",
    "\n",
    "    # model4b.fit(X_train_extrat, y_train)\n",
    "    # model4b_results.append(model4b.predict(X_test_extrat))\n",
    "\n",
    "    # model4c.fit(X_train_extrat, y_train)\n",
    "    # model4c_results.append(model4c.predict(X_test_extrat))\n",
    "\n",
    "    # model4d.fit(X_train_extrat, y_train)\n",
    "    # model4d_results.append(model4d.predict(X_test_extrat))\n",
    "\n",
    "    # model4e.fit(X_train_extrat, y_train)\n",
    "    # model4e_results.append(model4e.predict(X_test_extrat))\n",
    "\n",
    "    print('Running Hist Gradient')\n",
    "\n",
    "    model5.fit(X_train_hist, y_train)\n",
    "    model5_results.append(model5.predict(X_test_hist))\n",
    "\n",
    "    # model5b.fit(X_train_hist, y_train)\n",
    "    # model5b_results.append(model5b.predict(X_test_hist))\n",
    "\n",
    "    # model5c.fit(X_train_hist, y_train)\n",
    "    # model5c_results.append(model5c.predict(X_test_hist))\n",
    "\n",
    "    # model5d.fit(X_train_hist, y_train)\n",
    "    # model5d_results.append(model5d.predict(X_test_hist))\n",
    "\n",
    "    # model5e.fit(X_train_hist, y_train)\n",
    "    # model5e_results.append(model5e.predict(X_test_hist))\n",
    "\n",
    "    # model5f.fit(X_train_hist, y_train)\n",
    "    # model5f_results.append(model5f.predict(X_test_hist))\n",
    "\n",
    "    print('Running CatBoost')\n",
    "\n",
    "    model6.fit(X_train_cat, y_train)\n",
    "    model6_results.append(model6.predict(X_test_cat))\n",
    "\n",
    "    # model6b.fit(X_train_cat, y_train)\n",
    "    # model6b_results.append(model6b.predict(X_test_cat))\n",
    "\n",
    "    # model6c.fit(X_train_cat, y_train)\n",
    "    # model6c_results.append(model6c.predict(X_test_cat))\n",
    "\n",
    "    # model6d.fit(X_train_cat, y_train)\n",
    "    # model6d_results.append(model6d.predict(X_test_cat))\n",
    "\n",
    "    # model6e.fit(X_train_cat, y_train)\n",
    "    # model6e_results.append(model6e.predict(X_test_cat))\n",
    "\n",
    "    print('Running KNN')\n",
    "\n",
    "    model7.fit(X_train_knn, y_train)\n",
    "    model7_results.append(model7.predict(X_test_knn))    \n",
    "\n",
    "    print('Running Ridge')\n",
    "\n",
    "    model8.fit(X_train_ridge, y_train)\n",
    "    model8_results.append(model8.predict(X_test_ridge))   \n",
    "\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # model1_weights, model1b_weights, model1c_weights, model1d_weights, model1e_weights, model2_weights, model3_weights, model3b_weights, model3c_weights, model4_weights, model4b_weights, model4c_weights, model4d_weights, model4e_weights, model5_weights, model5b_weights, model5c_weights, model5d_weights, model5e_weights, model5f_weights, model6_weights, model6b_weights, model6c_weights, model6d_weights, model6e_weights, scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# model1_weights, model1b_weights, model1c_weights, model1d_weights, model1e_weights, model2_weights, model3_weights, model3b_weights, model3c_weights, model4_weights, model4b_weights, model4c_weights, model4d_weights, model4e_weights, model5_weights, model5b_weights, model5c_weights, model5d_weights, model5e_weights, model5f_weights, model6_weights, model6e_weights, scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# scores_in = []\n",
    "\n",
    "# for i in tqdm(range(20)):\n",
    "#     weight_1 = np.random.random_sample(size=1)[0]\n",
    "#     weight_1b = np.random.random_sample(size=1)[0]\n",
    "#     weight_1c = np.random.random_sample(size=1)[0]\n",
    "#     weight_1d = np.random.random_sample(size=1)[0]\n",
    "#     weight_1e = np.random.random_sample(size=1)[0]\n",
    "#     weight_2 = np.random.random_sample(size=1)[0]\n",
    "#     weight_3 = np.random.random_sample(size=1)[0]\n",
    "#     weight_3b = np.random.random_sample(size=1)[0]\n",
    "#     weight_3c = np.random.random_sample(size=1)[0]\n",
    "#     weight_4 = np.random.random_sample(size=1)[0]\n",
    "#     weight_4b = np.random.random_sample(size=1)[0]\n",
    "#     weight_4c = np.random.random_sample(size=1)[0]\n",
    "#     weight_4d = np.random.random_sample(size=1)[0]\n",
    "#     weight_4e = np.random.random_sample(size=1)[0]\n",
    "#     weight_5 = np.random.random_sample(size=1)[0]\n",
    "#     weight_5b = np.random.random_sample(size=1)[0]\n",
    "#     weight_5c = np.random.random_sample(size=1)[0]\n",
    "#     weight_5d = np.random.random_sample(size=1)[0]\n",
    "#     weight_5e = np.random.random_sample(size=1)[0]\n",
    "#     weight_5f = np.random.random_sample(size=1)[0]\n",
    "#     weight_6 = np.random.random_sample(size=1)[0]\n",
    "#     # weight_6b = np.random.random_sample(size=1)[0]\n",
    "#     # weight_6c = np.random.random_sample(size=1)[0]\n",
    "#     # weight_6d = np.random.random_sample(size=1)[0]\n",
    "#     weight_6e = np.random.random_sample(size=1)[0]\n",
    "\n",
    "#     model1_weights.append(weight_1)\n",
    "#     model1b_weights.append(weight_1b)\n",
    "#     model1c_weights.append(weight_1c)\n",
    "#     model1d_weights.append(weight_1d)\n",
    "#     model1e_weights.append(weight_1e)\n",
    "#     model2_weights.append(weight_2)\n",
    "#     model3_weights.append(weight_3)\n",
    "#     model3b_weights.append(weight_3b)\n",
    "#     model3c_weights.append(weight_3c)\n",
    "#     model4_weights.append(weight_4)\n",
    "#     model4b_weights.append(weight_4b)\n",
    "#     model4c_weights.append(weight_4c)\n",
    "#     model4d_weights.append(weight_4d)\n",
    "#     model4e_weights.append(weight_4e)\n",
    "#     model5_weights.append(weight_5)\n",
    "#     model5b_weights.append(weight_5b)\n",
    "#     model5c_weights.append(weight_5c)\n",
    "#     model5d_weights.append(weight_5d)\n",
    "#     model5e_weights.append(weight_5e)\n",
    "#     model5f_weights.append(weight_5f)\n",
    "#     model6_weights.append(weight_6)\n",
    "#     # model6b_weights.append(weight_6b)\n",
    "#     # model6c_weights.append(weight_6c)\n",
    "#     # model6d_weights.append(weight_6d)\n",
    "#     model6e_weights.append(weight_6e)\n",
    "\n",
    "#     # scores_in = []\n",
    "\n",
    "#     for j in range(n_splits):\n",
    "#         weighted_pred = (weight_1 * model1_results[j])\n",
    "#         + (weight_1b * model1b_results[j])\n",
    "#         + (weight_1c * model1c_results[j])\n",
    "#         + (weight_1d * model1d_results[j])\n",
    "#         + (weight_1e * model1e_results[j])\n",
    "#         + (weight_2 * model2_results[j])\n",
    "#         + (weight_3 * model3_results[j])\n",
    "#         + (weight_3b * model3b_results[j])\n",
    "#         + (weight_3c * model3c_results[j])\n",
    "#         + (weight_4 * model4_results[j])\n",
    "#         + (weight_4b * model4b_results[j])\n",
    "#         + (weight_4c * model4c_results[j])\n",
    "#         + (weight_4d * model4d_results[j])\n",
    "#         + (weight_4e * model4e_results[j])\n",
    "#         + (weight_5 * model5_results[j])\n",
    "#         + (weight_5b * model5b_results[j])\n",
    "#         + (weight_5c * model5c_results[j])\n",
    "#         + (weight_5d * model5d_results[j])\n",
    "#         + (weight_5e * model5e_results[j])\n",
    "#         + (weight_5f * model5f_results[j])\n",
    "#         + (weight_6 * model6_results[j])\n",
    "#         # + (weight_6b * model6b_results[j])\n",
    "#         # + (weight_6c * model6c_results[j])\n",
    "#         # + (weight_6d * model6d_results[j])\n",
    "#         + (weight_6e * model6e_results[j])\n",
    "\n",
    "#         weighted_pred_normalized = weighted_pred / np.sum(weighted_pred, axis=1, keepdims=True)\n",
    "\n",
    "#         scores_in.append(roc_auc_score(y_test_list[j], weighted_pred_normalized, multi_class='ovr'))\n",
    "        \n",
    "#     scores.append(np.mean(scores_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the predictions for each model and the true labels\n",
    "all_predictions = [np.concatenate(model_results) for model_results in [model1_results, model2_results, model3_results, model4_results, model5_results, model6_results, model7_results, model8_results]]\n",
    "all_true_labels = np.concatenate(y_test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def objective(trial):\n",
    "    # Generate weights for each model\n",
    "    weights = [trial.suggest_uniform(f'weight_{i}', -1, 1) for i in range(len(all_predictions))]\n",
    "    \n",
    "    # Calculate the weighted sum of predictions\n",
    "    weighted_predictions = np.zeros_like(all_predictions[0])\n",
    "    for weight, predictions in zip(weights, all_predictions):\n",
    "        weighted_predictions += weight * predictions\n",
    "    \n",
    "    # Compute RMSLE; you need to ensure there are no negative or zero predictions\n",
    "    # Ensuring no negative values\n",
    "    weighted_predictions = np.clip(weighted_predictions, a_min=0, a_max=None)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(all_true_labels, weighted_predictions))\n",
    "    \n",
    "    return rmsle\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=500)  # You can adjust the number of trials\n",
    "\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimal weights from Optuna study\n",
    "optimal_weights = study.best_params\n",
    "\n",
    "# Assuming `optimal_weights` is a dictionary with model identifiers as keys\n",
    "# and the optimized weight as values, you can directly use it to create a DataFrame\n",
    "# For the 'scores', you would use the best score achieved during the Optuna study\n",
    "optuna_results_df = pd.DataFrame([optimal_weights])\n",
    "optuna_results_df['score'] = study.best_value\n",
    "\n",
    "optuna_results_df.columns = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5', 'model_6', 'model_7', 'model_8', 'score']\n",
    "\n",
    "# Since you only have one row of data (the best combination of weights),\n",
    "# sorting by 'score' or getting the top rows doesn't apply as it's already the best\n",
    "optuna_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_0.14903.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('random_weights_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Submission (Random Weight Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Running LGBM')\n",
    "model1_final = model1.fit(X_lgbm, y)\n",
    "model1b_final = model1b.fit(X_lgbm, y)\n",
    "model1c_final = model1c.fit(X_lgbm, y)\n",
    "model1d_final = model1d.fit(X_lgbm, y)\n",
    "model1e_final = model1e.fit(X_lgbm, y)\n",
    "\n",
    "print('Running XGBoost')\n",
    "model2_final = model2.fit(X_xgb, y)\n",
    "\n",
    "print('Running Random Forest')\n",
    "model3_final = model3.fit(X_rf, y)\n",
    "model3b_final = model3b.fit(X_rf, y)\n",
    "model3c_final = model3c.fit(X_rf, y)\n",
    "\n",
    "print('Running ExtraTrees')\n",
    "model4_final = model4.fit(X_extrat, y)\n",
    "model4b_final = model4b.fit(X_extrat, y)\n",
    "model4c_final = model4c.fit(X_extrat, y)\n",
    "model4d_final = model4d.fit(X_extrat, y)\n",
    "model4e_final = model4e.fit(X_extrat, y)\n",
    "\n",
    "print('Running HistGradient')\n",
    "model5_final = model5.fit(X_hist, y)\n",
    "model5b_final = model5b.fit(X_hist, y)\n",
    "model5c_final = model5c.fit(X_hist, y)\n",
    "model5d_final = model5d.fit(X_hist, y)\n",
    "model5e_final = model5e.fit(X_hist, y)\n",
    "model5f_final = model5f.fit(X_hist, y)\n",
    "\n",
    "print('Running CatBoost')\n",
    "model6_final = model6.fit(X_cat, y)\n",
    "# model6b_final = model6b.fit(X_cat, y)\n",
    "# model6c_final = model6c.fit(X_cat, y)\n",
    "# model6d_final = model6d.fit(X_cat, y)\n",
    "model6e_final = model6e.fit(X_cat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ensemble_pred = (\n",
    "                results_df['model_1'][0] * model1_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1b'][0] * model1b_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1c'][0] * model1c_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1d'][0] * model1d_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1e'][0] * model1e_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_2'][0] * model2_final.predict_proba(test[model2_feats]) +\n",
    "                results_df['model_3'][0] * model3_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_3b'][0] * model3b_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_3c'][0] * model3c_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_4'][0] * model4_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4b'][0] * model4b_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4c'][0] * model4c_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4d'][0] * model4d_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4e'][0] * model4e_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_5'][0] * model5_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5b'][0] * model5b_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5c'][0] * model5c_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5d'][0] * model5d_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5e'][0] * model5e_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5f'][0] * model5f_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_6'][0] * model6_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6b'][0] * model6b_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6c'][0] * model6c_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6d'][0] * model6d_final.predict_proba(test[model6_feats]) +\n",
    "                results_df['model_6e'][0] * model6e_final.predict_proba(test[model6_feats])\n",
    "                 )\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_pred)\n",
    "\n",
    "# If all models predict 0, instead of getting NaN, fill in 0\n",
    "ensemble_df = ensemble_df.div(ensemble_df.sum(axis=1), axis=0).fillna(0)\n",
    "ensemble_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "optuna_ensemble_pred = (\n",
    "                optuna_results_df['model_1'][0] * model1_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1b'][0] * model1b_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1c'][0] * model1c_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1d'][0] * model1d_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1e'][0] * model1e_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_2'][0] * model2_final.predict_proba(test[model2_feats]) +\n",
    "                optuna_results_df['model_3'][0] * model3_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_3b'][0] * model3b_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_3c'][0] * model3c_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_4'][0] * model4_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4b'][0] * model4b_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4c'][0] * model4c_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4d'][0] * model4d_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4e'][0] * model4e_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_5'][0] * model5_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5b'][0] * model5b_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5c'][0] * model5c_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5d'][0] * model5d_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5e'][0] * model5e_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5f'][0] * model5f_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_6'][0] * model6_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6b'][0] * model6b_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6c'][0] * model6c_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6d'][0] * model6d_final.predict_proba(test[model6_feats]) +\n",
    "                optuna_results_df['model_6e'][0] * model6e_final.predict_proba(test[model6_feats])\n",
    "                 )\n",
    "\n",
    "optuna_ensemble_df = pd.DataFrame(optuna_ensemble_pred)\n",
    "\n",
    "# If all models predict 0, instead of getting NaN, fill in 0\n",
    "optuna_ensemble_df = optuna_ensemble_df.div(optuna_ensemble_df.sum(axis=1), axis=0).fillna(0)\n",
    "optuna_ensemble_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], optuna_ensemble_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_optuna_weights_ensemble_3fold_0.902197.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get submission (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('model1', model1_final),\n",
    "    ('model2', model2_final),\n",
    "    ('model3', model3_final),\n",
    "    ('model4', model4_final),\n",
    "    ('model5', model5_final),\n",
    "    ('model6', model6_final)\n",
    "]\n",
    "\n",
    "# Initialize the Stacking Classifier with LogisticRegression as the final estimator\n",
    "final_estimator = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "# final_estimator = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "# final_estimator = XGBClassifier(random_state=5)\n",
    "# final_estimator = RandomForestClassifier(random_state=5)\n",
    "# final_estimator = ExtraTreesClassifier(random_state=5)\n",
    "# final_estimator = HistGradientBoostingClassifier(random_state=5)\n",
    "# final_estimator = CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=final_estimator, passthrough=False, cv=3)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict_proba(X_test)\n",
    "\n",
    "    # Assuming your classes are 0, 1, 2, etc., adjust as necessary\n",
    "    y_test_binarized = label_binarize(y_test, classes=np.unique(y))\n",
    "    roc_auc = roc_auc_score(y_test_binarized, y_pred, multi_class='ovr')\n",
    "\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')\n",
    "    \n",
    "print(f'The average stacking score is {np.mean(roc_auc_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Reg - 0.886778\n",
    "- LGBM - 0.885863\n",
    "- XGB - 0.881636\n",
    "- RF - 0.883835\n",
    "- ET - 0.884523\n",
    "- Hist - 0.886572\n",
    "- Cat - 0.886183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on unseen test data\n",
    "y_test_pred = stacking_clf.predict_proba(test)\n",
    "\n",
    "stacking_df = pd.DataFrame(y_test_pred)\n",
    "\n",
    "ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model2_results, model3_results, model4_results, model5_results, model6_results, y_test_list = [], [], [], [], [], [], []\n",
    "\n",
    "# # Placeholder for OOF predictions for each model\n",
    "# # Assuming you have a dataset with N samples\n",
    "# N = len(y)  # y_train is your target variable array\n",
    "# oof_preds1 = np.zeros((N, 1))\n",
    "# oof_preds2 = np.zeros((N, 1))\n",
    "# oof_preds3 = np.zeros((N, 1))\n",
    "# oof_preds4 = np.zeros((N, 1))\n",
    "# oof_preds5 = np.zeros((N, 1))\n",
    "# oof_preds6 = np.zeros((N, 1))\n",
    "\n",
    "# # Similarly, for test predictions, accumulate them over folds\n",
    "# # Assuming you have a test set with M samples\n",
    "# M = len(test)  # x_test needs to be defined by you\n",
    "# test_preds1 = np.zeros((M, 1))\n",
    "# test_preds2 = np.zeros((M, 1))\n",
    "# test_preds3 = np.zeros((M, 1))\n",
    "# test_preds4 = np.zeros((M, 1))\n",
    "# test_preds5 = np.zeros((M, 1))\n",
    "# test_preds6 = np.zeros((M, 1))\n",
    "\n",
    "target_length = len(y)\n",
    "no_classes = len(np.unique(y))\n",
    "test_length = len(test)\n",
    "\n",
    "# Initialize arrays for OOF and test predictions with dimensions for multiclass for each model\n",
    "lgbm_oof_preds = np.zeros((target_length, no_classes))\n",
    "lgbm_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "xgb_oof_preds = np.zeros((target_length, no_classes))\n",
    "xgb_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "rf_oof_preds = np.zeros((target_length, no_classes))\n",
    "rf_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "extrat_oof_preds = np.zeros((target_length, no_classes))\n",
    "extrat_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "hist_oof_preds = np.zeros((target_length, no_classes))\n",
    "hist_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "cat_oof_preds = np.zeros((target_length, no_classes))\n",
    "cat_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "X_lgbm = X[model1_feats]\n",
    "X_xgb = X[model2_feats]\n",
    "X_rf = X[model3_feats]\n",
    "X_extrat = X[model4_feats]\n",
    "X_hist = X[model5_feats]\n",
    "X_cat = X[model6_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "\n",
    "    # Placeholder arrays for the fold's predicition\n",
    "    fold_oof_preds_lgbm = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_lgbm = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_xgb = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_xgb = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_rf = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_rf = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_extrat = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_extrat = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_hist = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_hist = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_cat = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_cat = np.zeros((test_length, no_classes))\n",
    "\n",
    "    # Get each models train and test for X and y\n",
    "    X_train_lgbm, X_test_lgbm = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "    X_train_xgb, X_test_xgb = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "    X_train_rf, X_test_rf = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "    X_train_extrat, X_test_extrat = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "    X_train_hist, X_test_hist = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "    X_train_cat, X_test_cat = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    ########\n",
    "    # LGBM #\n",
    "    ########\n",
    "    model1.fit(X_train_lgbm, y_train)\n",
    "    fold_oof_preds_lgbm = model1.predict_proba(X_test_lgbm)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    lgbm_oof_preds[test_index] = fold_oof_preds_lgbm\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_lgbm += model1.predict_proba(test.loc[:, model1_feats]) / sk10.n_splits\n",
    "\n",
    "    lgbm_test_preds += fold_test_preds_lgbm\n",
    "\n",
    "\n",
    "    ###########\n",
    "    # XGBOOST #\n",
    "    ###########\n",
    "    model2.fit(X_train_xgb, y_train)\n",
    "    fold_oof_preds_xgb = model2.predict_proba(X_test_xgb)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    xgb_oof_preds[test_index] = fold_oof_preds_xgb\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_xgb += model2.predict_proba(test.loc[:, model2_feats]) / sk10.n_splits\n",
    "\n",
    "    xgb_test_preds += fold_test_preds_xgb\n",
    "\n",
    "\n",
    "    #################\n",
    "    # RANDOM FOREST #\n",
    "    #################\n",
    "    model3.fit(X_train_rf, y_train)\n",
    "    fold_oof_preds_rf = model3.predict_proba(X_test_rf)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    rf_oof_preds[test_index] = fold_oof_preds_rf\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_rf += model3.predict_proba(test.loc[:, model3_feats]) / sk10.n_splits\n",
    "\n",
    "    rf_test_preds += fold_test_preds_rf\n",
    "\n",
    "    \n",
    "    ###############\n",
    "    # EXTRA TREES #\n",
    "    ###############\n",
    "    model4.fit(X_train_extrat, y_train)\n",
    "    fold_oof_preds_extrat = model4.predict_proba(X_test_extrat)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    extrat_oof_preds[test_index] = fold_oof_preds_extrat\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_extrat += model4.predict_proba(test.loc[:, model4_feats]) / sk10.n_splits\n",
    "\n",
    "    extrat_test_preds += fold_test_preds_extrat\n",
    "\n",
    "\n",
    "    #################\n",
    "    # HIST GRADIENT #\n",
    "    #################\n",
    "    model5.fit(X_train_hist, y_train)\n",
    "    fold_oof_preds_hist = model5.predict_proba(X_test_hist)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    hist_oof_preds[test_index] = fold_oof_preds_hist\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_hist += model5.predict_proba(test.loc[:, model5_feats]) / sk10.n_splits\n",
    "\n",
    "    hist_test_preds += fold_test_preds_hist\n",
    "\n",
    "\n",
    "    ############\n",
    "    # CATBOOST #\n",
    "    ############\n",
    "    model6.fit(X_train_cat, y_train)\n",
    "    fold_oof_preds_cat = model6.predict_proba(X_test_cat)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    cat_oof_preds[test_index] = fold_oof_preds_cat\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_cat += model6.predict_proba(test.loc[:, model6_feats]) / sk10.n_splits\n",
    "\n",
    "    cat_test_preds += fold_test_preds_cat\n",
    "    # y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_scores = [roc_auc_score((y == class_id).astype(int), oof_preds[:, class_id], multi_class='ovr') for class_id in range(no_classes)]\n",
    "lgbm_roc_auc = roc_auc_score(y, lgbm_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average LGBM ROC AUC Score:\", lgbm_roc_auc)\n",
    "\n",
    "xgb_roc_auc = roc_auc_score(y, xgb_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average XGBoost ROC AUC Score:\", xgb_roc_auc)\n",
    "\n",
    "rf_roc_auc = roc_auc_score(y, rf_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Random Forest ROC AUC Score:\", rf_roc_auc)\n",
    "\n",
    "extrat_roc_auc = roc_auc_score(y, extrat_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Extra Trees ROC AUC Score:\", extrat_roc_auc)\n",
    "\n",
    "hist_roc_auc = roc_auc_score(y, hist_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Hist Gradient ROC AUC Score:\", hist_roc_auc)\n",
    "\n",
    "cat_roc_auc = roc_auc_score(y, cat_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average CatBoost ROC AUC Score:\", cat_roc_auc)\n",
    "\n",
    "# 0.89369590207664\n",
    "# 0.00201442835387733\n",
    "# 0.886778 - StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# After running the fitting and prediction with the first level of machine learning models\n",
    "x_train = np.concatenate(( lgbm_oof_preds, xgb_oof_preds, rf_oof_preds, extrat_oof_preds, hist_oof_preds, cat_oof_preds), axis=1)\n",
    "test_stack = np.concatenate(( lgbm_test_preds, xgb_test_preds, rf_test_preds, extrat_test_preds, hist_test_preds, cat_test_preds), axis=1)\n",
    "\n",
    "# Assuming the second-level stacking is to be done with XGboost (pre-tuned). Yes! You can tune second-level stack\n",
    "\n",
    "stacking_estimator = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "xgb = stacking_estimator.fit(x_train, y)\n",
    "final_predictions = xgb.predict_proba(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = np.zeros((x_train.shape[0], no_classes))\n",
    "test_preds = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(x_train, y)):\n",
    "    X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model2.fit(X_train, y_train)\n",
    "    y_pred = model2.predict_proba(X_test)\n",
    "\n",
    "    # Assign predictions for this fold to the appropriate indices in oof_preds\n",
    "    oof_preds[test_index, :] = y_pred\n",
    "    \n",
    "    print(f'Done with fold {i+1}.')\n",
    "\n",
    "# Calculate ROC AUC on the OOF predictions\n",
    "roc_auc = roc_auc_score(y, oof_preds, multi_class='ovr', average='macro')\n",
    "print(f'The stacking score is {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Reg - 0.8883102077923056\n",
    "- LGBM - 0.8880225088607244\n",
    "- XGB - 0.8846028966376445\n",
    "- RF - \n",
    "- ET - \n",
    "- Hist - \n",
    "- Cat - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.DataFrame(final_predictions)\n",
    "final_predictions_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], final_predictions_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_3fold_0.88831.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the stack CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_scores = []\n",
    "meta_model = Ridge()\n",
    "\n",
    "for i, (train_idx, meta_idx) in enumerate(sk10.split(X)):\n",
    "    print(f'Fold {i + 1}')\n",
    "    X_train, X_meta = X.iloc[train_idx], X.iloc[meta_idx]\n",
    "    y_train, y_meta = y.iloc[train_idx], y.iloc[meta_idx]\n",
    "\n",
    "    meta_features_fold = np.zeros((X_meta.shape[0], len(models)))\n",
    "    # meta_test_features = np.zeros((y.shape[0], len(models)))\n",
    "    # meta_targets = np.zeros(y.shape[0])\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_name = model.__class__.__name__ if not hasattr(model, 'name') else model.name\n",
    "        print(f'Starting {model_name}')\n",
    "        model_features = sfs_features[model_name]\n",
    "\n",
    "        # Fit model on the selected features\n",
    "        model.fit(X_train[model_features], y_train)\n",
    "        preds = model.predict(X_meta[model_features])\n",
    "        meta_features_fold[:, i] = preds\n",
    "\n",
    "    # Train the meta-model on the predictions from the base models\n",
    "    meta_model.fit(meta_features_fold, y_meta)\n",
    "    \n",
    "    # Predict using the meta-model\n",
    "    final_preds = meta_model.predict(meta_features_fold)\n",
    "    \n",
    "    # Calculate RMSLE for the current fold\n",
    "    current_fold_rmsle = rmsle(y_meta, final_preds)\n",
    "    meta_scores.append(current_fold_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average RMSLE across all folds\n",
    "average_rmsle = np.mean(meta_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stacking submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain base models on all data\n",
    "all_base_model_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__ if not hasattr(model, 'name') else model.name\n",
    "    print(f'Starting {model_name}')\n",
    "    model_features = sfs_features[model_name]\n",
    "\n",
    "    model.fit(X[model_features], y)\n",
    "    preds = model.predict(test[model_features])\n",
    "    all_base_model_predictions.append(preds.reshape(-1, 1))\n",
    "\n",
    "# Stack predictions for the meta model\n",
    "X_new_meta = np.hstack(all_base_model_predictions)\n",
    "\n",
    "# Use the meta model to make final predictions\n",
    "final_predictions = meta_model.predict(X_new_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__ if not hasattr(model, 'name') else model.name\n",
    "    model_names.append(model_name)\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ensemble weights')\n",
    "weights = pd.Series(meta_model.coef_, index=model_names)\n",
    "print(weights)\n",
    "print(f'Weights total: {weights.sum()}')\n",
    "print(f'Intercept: {meta_model.intercept_}', end='\\n\\n')\n",
    "print(f\"Average Stacking RMSLE across all folds: {average_rmsle:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['Rings'])\n",
    "final_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], final_predictions_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_0.14848.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
