{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment_name = 'all_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('df_train.csv')\n",
    "original = pd.read_csv('original_df.csv')\n",
    "test = pd.read_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90615, 11), (4177, 11), (60411, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, original.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.2380</td>\n",
       "      <td>0.6185</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.4785</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>0.2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex_F  Sex_I  Sex_M  Length  Diameter  Height  Whole_weight  \\\n",
       "0    0.0    0.0    1.0   0.645     0.475   0.155        1.2380   \n",
       "1    0.0    0.0    1.0   0.580     0.460   0.160        0.9830   \n",
       "2    0.0    0.0    1.0   0.560     0.420   0.140        0.8395   \n",
       "3    0.0    0.0    1.0   0.570     0.490   0.145        0.8740   \n",
       "4    0.0    1.0    0.0   0.415     0.325   0.110        0.3580   \n",
       "\n",
       "   Shucked_weight  Viscera_weight  Shell_weight  \n",
       "0          0.6185          0.3125        0.3005  \n",
       "1          0.4785          0.2195        0.2750  \n",
       "2          0.3525          0.1845        0.2405  \n",
       "3          0.3525          0.1865        0.2350  \n",
       "4          0.1575          0.0670        0.1050  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['Sex_F', 'Sex_I', 'Sex_M', 'Length', 'Diameter', 'Height',\n",
    "       'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight',\n",
    "       'Rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concat the train and original dataset\n",
    "# combined_df = pd.concat([train, original], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'Rings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop([TARGET], axis=1)\n",
    "y = train[TARGET]\n",
    "\n",
    "n_splits = 3\n",
    "sk10 = KFold(n_splits=n_splits, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_params_1 = \n",
    "# lgbm_params_2 = \n",
    "# lgbm_params_3 = \n",
    "# lgbm_params_4 = \n",
    "# hist_params_1 = \n",
    "# hist_params_2 = \n",
    "# hist_params_3 = \n",
    "# hist_params_4 = \n",
    "# hist_params_5 = \n",
    "# extrat_params_1 = \n",
    "# extrat_params_2 = \n",
    "# extrat_params_3 = \n",
    "# extrat_params_4 = \n",
    "# rf_params_1 = \n",
    "# rf_params_2 = \n",
    "# rf_params_3 = \n",
    "# cat_params_1 =\n",
    "# cat_params_2 = \n",
    "# cat_params_3 = \n",
    "# cat_params_4 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LGBMRegressor(n_jobs=-1, random_state=5),\n",
    "    # LGBMRegressor(**lgbm_params_1),\n",
    "    # LGBMRegressor(**lgbm_params_2),\n",
    "    # LGBMRegressor(**lgbm_params_3),\n",
    "    # LGBMRegressor(**lgbm_params_4),\n",
    "    XGBRegressor(random_state=5),\n",
    "    RandomForestRegressor(random_state=5),\n",
    "    # RandomForestRegressor(**rf_params_1),\n",
    "    # RandomForestRegressor(**rf_params_2),\n",
    "    ExtraTreesRegressor(random_state=5),\n",
    "    # ExtraTreesRegressor(**extrat_params_1),\n",
    "    # ExtraTreesRegressor(**extrat_params_2),\n",
    "    # ExtraTreesRegressor(**extrat_params_3),\n",
    "    # ExtraTreesRegressor(**extrat_params_4),\n",
    "    HistGradientBoostingRegressor(random_state=5),\n",
    "    # HistGradientBoostingRegressor(**hist_params_1),\n",
    "    # HistGradientBoostingRegressor(**hist_params_2),\n",
    "    # HistGradientBoostingRegressor(**hist_params_3),\n",
    "    # HistGradientBoostingRegressor(**hist_params_4),\n",
    "    # HistGradientBoostingRegressor(**hist_params_5),\n",
    "    CatBoostRegressor(random_state=5, verbose=False, early_stopping_rounds=100),\n",
    "    # CatBoostRegressor(**cat_params_1),\n",
    "    # CatBoostRegressor(**cat_params_2),\n",
    "    # CatBoostRegressor(**cat_params_3),\n",
    "    # CatBoostRegressor(**cat_params_4),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error (RMSLE).\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    # # Add post processing step if required\n",
    "    # y_pred_processed = np.floor(y_pred)\n",
    "    \n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "    MLA_compare = pd.DataFrame(columns=['MLA Name', \n",
    "                                        'MLA Parameters', \n",
    "                                        'MLA Train ROC AUC', \n",
    "                                        'MLA Test ROC AUC', \n",
    "                                        'MLA Test ROC AUC Std', \n",
    "                                        'MLA Time'])\n",
    "    \n",
    "    def evaluate_model(alg, idx):\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        features = important_features.get(MLA_name, [])\n",
    "\n",
    "        # Check if the list of important features is empty\n",
    "        if len(features) == 0:\n",
    "            # If empty, return results with zero values\n",
    "            print(f'Skipping {MLA_name} due to no important features.')\n",
    "            return {\n",
    "                'MLA Name': MLA_name,\n",
    "                'MLA Parameters': str(alg.get_params()),\n",
    "                'MLA Train ROC': 0,\n",
    "                'MLA Test ROC': 0,\n",
    "                'MLA Test ROC Std': 0,\n",
    "                'MLA Time': \"0 min 0.00 sec\",\n",
    "            }\n",
    "        \n",
    "        cv_results = cross_validate(alg, \n",
    "                                    X[features], \n",
    "                                    y, cv=cv_split, \n",
    "                                    scoring=rmsle_scorer, \n",
    "                                    return_train_score=True, \n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Time formatting\n",
    "        mean_fit_time = cv_results['fit_time'].mean()\n",
    "        minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "        # Results population\n",
    "        result = {\n",
    "            'MLA Name': MLA_name,\n",
    "            'MLA Parameters': str(alg.get_params()),\n",
    "            'MLA Train ROC AUC': -cv_results['train_score'].mean(),\n",
    "            'MLA Test ROC AUC': -cv_results['test_score'].mean(),\n",
    "            'MLA Test ROC AUC Std': cv_results['test_score'].std(),\n",
    "            'MLA Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "        }\n",
    "\n",
    "        print(f'Done with {MLA_name}.')\n",
    "        return result\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(models)]\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results_list.append(result)\n",
    "\n",
    "    MLA_compare = pd.DataFrame(results_list)\n",
    "\n",
    "    MLA_compare.sort_values(by=['MLA Test ROC AUC'], ascending=True, inplace=True)\n",
    "    MLA_compare.to_csv(f'{experiment_name}_results.csv', index=False)\n",
    "\n",
    "    return MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    baseline_features[model_name] = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 1min 47s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC Std</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'criter...</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.155476</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0 min 47.63 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>0.059210</td>\n",
       "      <td>0.154395</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>1 min 19.49 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>0.134005</td>\n",
       "      <td>0.151059</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0 min 12.80 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HistGradientBoostingRegressor</td>\n",
       "      <td>{'categorical_features': None, 'early_stopping...</td>\n",
       "      <td>0.145466</td>\n",
       "      <td>0.151014</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0 min 2.46 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.145066</td>\n",
       "      <td>0.150627</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0 min 0.90 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'verbose': False, 'r...</td>\n",
       "      <td>0.140177</td>\n",
       "      <td>0.149725</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>1 min 2.73 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MLA Name  \\\n",
       "3            ExtraTreesRegressor   \n",
       "2          RandomForestRegressor   \n",
       "1                   XGBRegressor   \n",
       "4  HistGradientBoostingRegressor   \n",
       "0                  LGBMRegressor   \n",
       "5              CatBoostRegressor   \n",
       "\n",
       "                                      MLA Parameters  MLA Train ROC AUC  \\\n",
       "3  {'bootstrap': False, 'ccp_alpha': 0.0, 'criter...           0.000648   \n",
       "2  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...           0.059210   \n",
       "1  {'objective': 'reg:squarederror', 'base_score'...           0.134005   \n",
       "4  {'categorical_features': None, 'early_stopping...           0.145466   \n",
       "0  {'boosting_type': 'gbdt', 'class_weight': None...           0.145066   \n",
       "5  {'loss_function': 'RMSE', 'verbose': False, 'r...           0.140177   \n",
       "\n",
       "   MLA Test ROC AUC  MLA Test ROC AUC Std         MLA Time  \n",
       "3          0.155476              0.000299  0 min 47.63 sec  \n",
       "2          0.154395              0.000395  1 min 19.49 sec  \n",
       "1          0.151059              0.000530  0 min 12.80 sec  \n",
       "4          0.151014              0.000097   0 min 2.46 sec  \n",
       "0          0.150627              0.000205   0 min 0.90 sec  \n",
       "5          0.149725              0.000251   1 min 2.73 sec  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline_models = evaluate_models(models, X, y, baseline_features, sk10, f'{experiment_name}')\n",
    "baseline_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated features (leaving just 1 of each pair)\n",
    "# Leave features highly correlated with the target\n",
    "df_no_corr = X.copy()\n",
    "correlation_matrix_spear = df_no_corr.corr(method='spearman').abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_spear = correlation_matrix_spear.where(np.triu(np.ones(correlation_matrix_spear.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than a threshold (e.g., 0.9 in this case)\n",
    "to_drop_spear = [column for column in upper_spear.columns if any(upper_spear[column] >= 0.9)]\n",
    "\n",
    "# Drop features\n",
    "df_reduced_spear = df_no_corr.drop(to_drop_spear, axis=1)\n",
    "\n",
    "# Get list of low correlation features excluding TARGET\n",
    "low_corr_feats_spear = list(df_reduced_spear.columns)\n",
    "\n",
    "with open('low_corr_spear.txt', 'w') as f:\n",
    "    f.write(str(low_corr_feats_spear))\n",
    "    f.write('\\n')\n",
    "\n",
    "# Print the high correlation features effect\n",
    "# Both pre and post drop dfs contain the TARGET\n",
    "print(f\"Dropped {len(to_drop_spear)} highly correlated features.\\nOld Shape of the dataset was {df_no_corr.shape}\\nNew shape of the dataset is {df_reduced_spear.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "no_corr_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    no_corr_features[model_name] = list(df_reduced_spear.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "no_corr_models = evaluate_models(models, df_reduced_spear, y, no_corr_features, sk10, f'{experiment_name}_corr')\n",
    "no_corr_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_importance_features = {}\n",
    "\n",
    "# for model in models:\n",
    "#     model_name = model.__class__.__name__\n",
    "\n",
    "#     try:\n",
    "#         # Initialize array to store feature importances\n",
    "#         feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "#         # Loop through each fold and calculate the feature importances\n",
    "#         for train_index, test_index in sk10.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#             y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             # Get the feature importances and them to the total\n",
    "#             feature_importances += model.feature_importances_\n",
    "\n",
    "#         feature_importances /= n_splits\n",
    "\n",
    "#         feature_importances_dict = dict(zip(X.columns, feature_importances))\n",
    "\n",
    "#         df = pd.DataFrame.from_dict(feature_importances_dict, orient='index')\n",
    "\n",
    "#         # Resetting index with a name for the column\n",
    "#         df = df.reset_index().rename(columns={'index': 'Feature', 0: 'Avg_Feat_Importance'})\n",
    "#         df.sort_values(by='Avg_Feat_Importance', ascending=False, inplace=True)\n",
    "\n",
    "#         # Save to CSV\n",
    "#         df.to_csv(f'{model_name}_feature_importances.csv')\n",
    "\n",
    "#         fi_threshold = 0\n",
    "\n",
    "#         fi_feats = df[df['Avg_Feat_Importance'] > fi_threshold]['Feature'].tolist()\n",
    "\n",
    "#         feat_importance_features[model_name] = fi_feats\n",
    "#         print(f'Done with {model_name}')\n",
    "\n",
    "#     except AttributeError:\n",
    "#         feat_importance_features[model_name] = list(X.columns)\n",
    "#         print(f'{model_name} does not have feature_importances_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('featimp_features.txt', mode='w') as f:\n",
    "#     pprint(feat_importance_features, stream=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90615, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a random feature for X\n",
    "np.random.seed(5)\n",
    "X['random_control_feature'] = np.round(np.random.uniform(-2, 2, X.shape[0]), 6)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with Fold 1\n",
      "\n",
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with Fold 2\n",
      "\n",
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with Fold 3\n",
      "\n",
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with Fold 4\n",
      "\n",
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with Fold 5\n",
      "\n",
      "CPU times: total: 10min 30s\n",
      "Wall time: 14min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "perm_cv = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "perm_importances = {model.__class__.__name__: [] for model in models}\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(perm_cv.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        model.fit(X_train, y_train)\n",
    "        # Calculate permutation importance\n",
    "        result = permutation_importance(model, X_test, y_test, n_repeats=5, random_state=5, n_jobs=-1, scoring=rmsle_scorer)\n",
    "        perm_importances[model_name].append(result.importances_mean)\n",
    "        print(f'Done with {model_name}.')\n",
    "    \n",
    "    print(f'Done with Fold {i+1}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Permuation Importances\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 84.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Average importances across folds and export to CSV\n",
    "for model_name, importances in perm_importances.items():\n",
    "    avg_importance = np.mean(importances, axis=0)\n",
    "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': avg_importance})\n",
    "    importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    # Export to CSV\n",
    "    importance_df.to_csv(f'.\\permutation_importances\\{model_name}_permutation_importance.csv', index=False)\n",
    "\n",
    "print('Done with Permuation Importances', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting important features dictionary\n"
     ]
    }
   ],
   "source": [
    "directory = 'permutation_importances'\n",
    "\n",
    "# Initialize a dictionary for the features\n",
    "perm_important_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    csv_path = os.path.join(directory, f'{model_name}_permutation_importance.csv')\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Check for 'random_control_feature' and its importance\n",
    "        if 'random_control_feature' in df['Feature'].values:\n",
    "            random_feature_importance = df.loc[df['Feature'] == 'random_control_feature', 'Importance'].iloc[0]\n",
    "        else:\n",
    "            random_feature_importance = 0\n",
    "\n",
    "        # Determine the threshold\n",
    "        threshold = max(0, random_feature_importance)\n",
    "\n",
    "        # Filter features where importance is greater than 0\n",
    "        important_feats_filtered = df[df['Importance'] > threshold]['Feature'].tolist()\n",
    "\n",
    "        # Reorder important_feats based on the predefined features_list\n",
    "        important_feats_ordered = [feat for feat in features_list if feat in important_feats_filtered]\n",
    "\n",
    "        # Add to importance dictionary\n",
    "        perm_important_features[model_name] = important_feats_ordered\n",
    "\n",
    "    else:\n",
    "        print(f'CSV file for {model_name} not found.')\n",
    "\n",
    "print('Done getting important features dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perm_important_features.txt', mode='w') as f:\n",
    "    pprint(perm_important_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC Std</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'verbose': False, 'r...</td>\n",
       "      <td>0.140177</td>\n",
       "      <td>0.149725</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0 min 55.66 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.145066</td>\n",
       "      <td>0.150627</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0 min 1.42 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HistGradientBoostingRegressor</td>\n",
       "      <td>{'categorical_features': None, 'early_stopping...</td>\n",
       "      <td>0.145527</td>\n",
       "      <td>0.150935</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0 min 3.23 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>0.134005</td>\n",
       "      <td>0.151059</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0 min 15.93 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>0.059210</td>\n",
       "      <td>0.154395</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>1 min 6.42 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'criter...</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.155432</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0 min 49.31 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MLA Name  \\\n",
       "5              CatBoostRegressor   \n",
       "0                  LGBMRegressor   \n",
       "4  HistGradientBoostingRegressor   \n",
       "1                   XGBRegressor   \n",
       "2          RandomForestRegressor   \n",
       "3            ExtraTreesRegressor   \n",
       "\n",
       "                                      MLA Parameters  MLA Train ROC AUC  \\\n",
       "5  {'loss_function': 'RMSE', 'verbose': False, 'r...           0.140177   \n",
       "0  {'boosting_type': 'gbdt', 'class_weight': None...           0.145066   \n",
       "4  {'categorical_features': None, 'early_stopping...           0.145527   \n",
       "1  {'objective': 'reg:squarederror', 'base_score'...           0.134005   \n",
       "2  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...           0.059210   \n",
       "3  {'bootstrap': False, 'ccp_alpha': 0.0, 'criter...           0.001807   \n",
       "\n",
       "   MLA Test ROC AUC  MLA Test ROC AUC Std         MLA Time  \n",
       "5          0.149725              0.000251  0 min 55.66 sec  \n",
       "0          0.150627              0.000205   0 min 1.42 sec  \n",
       "4          0.150935              0.000064   0 min 3.23 sec  \n",
       "1          0.151059              0.000530  0 min 15.93 sec  \n",
       "2          0.154395              0.000395   1 min 6.42 sec  \n",
       "3          0.155432              0.000298  0 min 49.31 sec  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "perm_importance_models = evaluate_models(models, X, y, perm_important_features, sk10, f'{experiment_name}_permimp')\n",
    "perm_importance_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SelectKBest with f_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_list = []\n",
    "kbest_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # Select whichever one had a better CV score generally\n",
    "    # Also, consider computational expense and accuracy balance\n",
    "    \n",
    "    features = perm_important_features[model_name]\n",
    "    # features = list(df_reduced_spear.columns)\n",
    "\n",
    "    # incase there is no feature that had importance, go to the next model\n",
    "    if len(features) == 0:\n",
    "        continue\n",
    "\t\n",
    "    X_kbest = X[features]\n",
    "    best_score = 0\n",
    "    best_k = 0\n",
    "    best_features = []\n",
    "\n",
    "    # Iterate over k from 1 to number of features\n",
    "    for k in range(1, len(features) + 1):\n",
    "        print(f'currently running {k} features on {model_name}')\n",
    "        # Apply SelectKBest\n",
    "        selector = SelectKBest(f_regression, k=k)\n",
    "        X_new = selector.fit_transform(X_kbest, y)\n",
    "\n",
    "        # Get the selected feature names\n",
    "        selected_features = X_kbest.columns[selector.get_support()]\n",
    "\n",
    "        # Evaluate the model\n",
    "        # model = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "        rmsle_scores = cross_validate(model, X_new, y, cv=sk10, scoring=rmsle_scorer, n_jobs=-1)\n",
    "        mean_rmsle_scores = rmsle_scores['test_score'].mean()\n",
    "\n",
    "        if mean_rmsle_scores > best_score:\n",
    "            best_k = k\n",
    "            best_score = mean_rmsle_scores\n",
    "            best_features = list(selected_features)\n",
    "\n",
    "    best_features_list.append({'k': best_k,\n",
    "                    'Selected Features': best_features,\n",
    "                    'RMSLE Score': best_score,\n",
    "                    'Model Name': model_name})\n",
    "    \n",
    "    kbest_features[model_name] = best_features\n",
    "\n",
    "best_features_df = pd.DataFrame(best_features_list)\n",
    "\n",
    "best_features_df.sort_values(by='RMSLE Score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kbest_features.txt', mode='w') as f:\n",
    "    pprint(kbest_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize empty dictionary for RFECV features\n",
    "rfecv_features = {}\n",
    "\n",
    "for alg in models:\n",
    "    # set name\n",
    "    MLA_name = alg.__class__.__name__\n",
    "\t\t\n",
    "    features = perm_important_features[MLA_name]\n",
    "\n",
    "    # incase there is no feature that had importance, go to the next model\n",
    "    if len(features) == 0:\n",
    "        continue\n",
    "\t\n",
    "    X_rfecv = X[features]\n",
    "\n",
    "    try:\n",
    "        print(f'Starting with {MLA_name}')\n",
    "        # Create the RFECV object and rank each feature\n",
    "        selector = RFECV(alg, cv=sk10, step=1, scoring=rmsle_scorer, verbose=2)\n",
    "        selector = selector.fit(X_rfecv, y)\n",
    "\n",
    "        selected_features = list(X_rfecv.columns[selector.support_])\n",
    "\n",
    "        # Reorder selected_features based on the predefined features_list\n",
    "        selected_features_ordered = [feat for feat in features_list if feat in selected_features]\n",
    "\n",
    "        rfecv_features[MLA_name] = selected_features_ordered\n",
    "\n",
    "        print(f'Done with {MLA_name}', end='\\n\\n')\n",
    "    \n",
    "    except ValueError:\n",
    "        # In case of an error, keep the original order but filtered by features_list\n",
    "        features_filtered = [feat for feat in features_list if feat in features]\n",
    "        rfecv_features[MLA_name] = features_filtered\n",
    "        print(f'{MLA_name} does not have coef_ or feature_importances_', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rfecv_features.txt', mode='w') as f:\n",
    "    pprint(rfecv_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "rfecv_models = evaluate_models(models, X, y, rfecv_features, sk10, f'{experiment_name}_rfecv')\n",
    "rfecv_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running backward feature selection with LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    5.0s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.1s finished\n",
      "\n",
      "[2024-04-02 19:33:41] Features: 9/1 -- score: -0.1505774857817751[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    3.6s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    5.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    5.4s finished\n",
      "\n",
      "[2024-04-02 19:33:46] Features: 8/1 -- score: -0.1506809363794596[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    3.7s remaining:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    4.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    4.4s finished\n",
      "\n",
      "[2024-04-02 19:33:51] Features: 7/1 -- score: -0.15074125321219398[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    4.2s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    4.2s finished\n",
      "\n",
      "[2024-04-02 19:33:55] Features: 6/1 -- score: -0.15126736127789217[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    4.0s remaining:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    4.1s finished\n",
      "\n",
      "[2024-04-02 19:34:00] Features: 5/1 -- score: -0.15187335119645654[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.5s remaining:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.6s finished\n",
      "\n",
      "[2024-04-02 19:34:04] Features: 4/1 -- score: -0.15274483004871656[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    2.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    2.7s finished\n",
      "\n",
      "[2024-04-02 19:34:06] Features: 3/1 -- score: -0.1538086818863269[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    2.1s finished\n",
      "\n",
      "[2024-04-02 19:34:09] Features: 2/1 -- score: -0.15555415703947445[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    1.9s finished\n",
      "\n",
      "[2024-04-02 19:34:11] Features: 1/1 -- score: -0.17150298341367332"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor\n",
      "\n",
      "Running backward feature selection with XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   43.4s remaining:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   59.9s finished\n",
      "\n",
      "[2024-04-02 19:35:20] Features: 9/1 -- score: -0.15089004363661665[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:   39.4s remaining:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   53.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   53.9s finished\n",
      "\n",
      "[2024-04-02 19:36:14] Features: 8/1 -- score: -0.1507842735894188[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:   36.5s remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   38.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   38.6s finished\n",
      "\n",
      "[2024-04-02 19:36:53] Features: 7/1 -- score: -0.150984044086362[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:   31.4s remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:   32.6s finished\n",
      "\n",
      "[2024-04-02 19:37:26] Features: 6/1 -- score: -0.15140901440467305[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:   24.3s remaining:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   25.1s finished\n",
      "\n",
      "[2024-04-02 19:37:51] Features: 5/1 -- score: -0.1518475808231867[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   17.7s remaining:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   18.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   18.6s finished\n",
      "\n",
      "[2024-04-02 19:38:10] Features: 4/1 -- score: -0.15230974292148614[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   12.9s finished\n",
      "\n",
      "[2024-04-02 19:38:23] Features: 3/1 -- score: -0.15345381375194103[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    9.1s finished\n",
      "\n",
      "[2024-04-02 19:38:32] Features: 2/1 -- score: -0.15525930921229025[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.9s finished\n",
      "\n",
      "[2024-04-02 19:38:39] Features: 1/1 -- score: -0.16960682827289628"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with XGBRegressor\n",
      "\n",
      "Running backward feature selection with RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  4.1min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  5.8min finished\n",
      "\n",
      "[2024-04-02 19:46:10] Features: 9/1 -- score: -0.15441935024262463[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  3.5min remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  5.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  5.4min finished\n",
      "\n",
      "[2024-04-02 19:51:37] Features: 8/1 -- score: -0.15459442393040407[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:  3.3min remaining:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  3.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  3.6min finished\n",
      "\n",
      "[2024-04-02 19:55:16] Features: 7/1 -- score: -0.15522434202644766[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:  2.8min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:  3.0min finished\n",
      "\n",
      "[2024-04-02 19:58:19] Features: 6/1 -- score: -0.15638994731772468[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:  3.5min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  3.9min finished\n",
      "\n",
      "[2024-04-02 20:02:16] Features: 5/1 -- score: -0.15802098642064552[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  3.2min remaining:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.4min finished\n",
      "\n",
      "[2024-04-02 20:05:39] Features: 4/1 -- score: -0.1607837771545416[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.7min finished\n",
      "\n",
      "[2024-04-02 20:07:22] Features: 3/1 -- score: -0.16494659728201658[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.1min finished\n",
      "\n",
      "[2024-04-02 20:08:26] Features: 2/1 -- score: -0.17353900269711406[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.9s finished\n",
      "\n",
      "[2024-04-02 20:08:48] Features: 1/1 -- score: -0.17007255319516448"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with RandomForestRegressor\n",
      "\n",
      "Running backward feature selection with ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:  2.7min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  3.0min finished\n",
      "\n",
      "[2024-04-02 20:12:34] Features: 7/1 -- score: -0.15628512569663558[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:  2.4min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:  2.5min finished\n",
      "\n",
      "[2024-04-02 20:15:03] Features: 6/1 -- score: -0.15737156331777188[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:  1.8min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  1.9min finished\n",
      "\n",
      "[2024-04-02 20:16:58] Features: 5/1 -- score: -0.15942566068120625[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.4min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min finished\n",
      "\n",
      "[2024-04-02 20:18:29] Features: 4/1 -- score: -0.16368473796701613[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.2min finished\n",
      "\n",
      "[2024-04-02 20:19:40] Features: 3/1 -- score: -0.17141060152470203[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   44.5s finished\n",
      "\n",
      "[2024-04-02 20:20:25] Features: 2/1 -- score: -0.18284355673436206[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   11.4s finished\n",
      "\n",
      "[2024-04-02 20:20:37] Features: 1/1 -- score: -0.17004622231109434"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with ExtraTreesRegressor\n",
      "\n",
      "Running backward feature selection with HistGradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    9.7s remaining:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   13.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   13.5s finished\n",
      "\n",
      "[2024-04-02 20:20:56] Features: 8/1 -- score: -0.15102402758904832[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    8.8s remaining:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    9.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    9.8s finished\n",
      "\n",
      "[2024-04-02 20:21:06] Features: 7/1 -- score: -0.15108858565278072[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    7.7s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    7.9s finished\n",
      "\n",
      "[2024-04-02 20:21:14] Features: 6/1 -- score: -0.15156735462507664[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    6.4s remaining:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    6.8s finished\n",
      "\n",
      "[2024-04-02 20:21:21] Features: 5/1 -- score: -0.1523886059008028[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.3s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.8s finished\n",
      "\n",
      "[2024-04-02 20:21:27] Features: 4/1 -- score: -0.1533268748035792[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    4.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    4.8s finished\n",
      "\n",
      "[2024-04-02 20:21:32] Features: 3/1 -- score: -0.15458010198979386[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    3.9s finished\n",
      "\n",
      "[2024-04-02 20:21:36] Features: 2/1 -- score: -0.1565769796127672[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    3.6s finished\n",
      "\n",
      "[2024-04-02 20:21:40] Features: 1/1 -- score: -0.17484439419285883"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with HistGradientBoostingRegressor\n",
      "\n",
      "Running backward feature selection with CatBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  2.9min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.7min finished\n",
      "\n",
      "[2024-04-02 20:25:54] Features: 9/1 -- score: -0.1496777058106973[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  2.7min remaining:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.2min finished\n",
      "\n",
      "[2024-04-02 20:29:09] Features: 8/1 -- score: -0.14978546383553085[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:  2.7min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  2.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  2.8min finished\n",
      "\n",
      "[2024-04-02 20:31:54] Features: 7/1 -- score: -0.14985923073581345[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:  2.3min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:  2.3min finished\n",
      "\n",
      "[2024-04-02 20:34:13] Features: 6/1 -- score: -0.1504264767235269[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:  1.8min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  1.9min finished\n",
      "\n",
      "[2024-04-02 20:36:06] Features: 5/1 -- score: -0.15108504401826106[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.5min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min finished\n",
      "\n",
      "[2024-04-02 20:37:38] Features: 4/1 -- score: -0.15186364615130574[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.2min finished\n",
      "\n",
      "[2024-04-02 20:38:51] Features: 3/1 -- score: -0.15300417038759673[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   56.1s finished\n",
      "\n",
      "[2024-04-02 20:39:48] Features: 2/1 -- score: -0.15510793997435982[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with CatBoostRegressor\n",
      "\n",
      "CPU times: total: 3min 19s\n",
      "Wall time: 1h 6min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   38.8s finished\n",
      "\n",
      "[2024-04-02 20:40:27] Features: 1/1 -- score: -0.17149652085226685"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize empty dictionary for SFS features\n",
    "sfs_features = {}\n",
    "\n",
    "for alg in models:\n",
    "    # set name\n",
    "    MLA_name = alg.__class__.__name__\n",
    "\n",
    "    try:\n",
    "\n",
    "        features = perm_important_features[MLA_name]    \n",
    "        # features = kbest_features[MLA_name]\n",
    "        # features = feat_importance_features[MLA_name]\n",
    "        # features = rfecv_features[MLA_name]\n",
    "\n",
    "        # incase there is no feature that had importance, go to the next model\n",
    "        if len(features) == 0:\n",
    "            continue\n",
    "        \n",
    "        X_sfs = X[features]\n",
    "\n",
    "        print(f'Running backward feature selection with {MLA_name}')\n",
    "\n",
    "        sfs = SFS(alg,\n",
    "            k_features='best',\n",
    "            forward=False,\n",
    "            floating=False,\n",
    "            scoring=rmsle_scorer,\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "            cv=sk10)\n",
    "        \n",
    "        sfs = sfs.fit(X_sfs, y)\n",
    "\n",
    "        # Get the selected features index\n",
    "        selected_sfs_idx = list(sfs.k_feature_idx_)\n",
    "\n",
    "        # Get the feature names\n",
    "        selected_sfs_feats = X_sfs.columns[selected_sfs_idx]\n",
    "\n",
    "        selected_features = list(selected_sfs_feats)\n",
    "\n",
    "        # Reorder selected_features based on the predefined features_list\n",
    "        selected_features_ordered = [feat for feat in features_list if feat in selected_features]\n",
    "\n",
    "        sfs_features[MLA_name] = selected_features_ordered\n",
    "\n",
    "        print(f'Done with {MLA_name}', end='\\n\\n')\n",
    "\n",
    "    except KeyError:\n",
    "        print(f'{MLA_name} not in the dictionary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sfs_features.txt', mode='w') as f:\n",
    "    pprint(sfs_features, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMRegressor.\n",
      "Done with XGBRegressor.\n",
      "Done with HistGradientBoostingRegressor.\n",
      "Done with ExtraTreesRegressor.\n",
      "Done with CatBoostRegressor.\n",
      "Done with RandomForestRegressor.\n",
      "CPU times: total: 625 ms\n",
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC</th>\n",
       "      <th>MLA Test ROC AUC Std</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'verbose': False, 'r...</td>\n",
       "      <td>0.140225</td>\n",
       "      <td>0.149678</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0 min 41.68 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.145088</td>\n",
       "      <td>0.150577</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0 min 1.85 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>0.134222</td>\n",
       "      <td>0.150784</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0 min 13.62 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HistGradientBoostingRegressor</td>\n",
       "      <td>{'categorical_features': None, 'early_stopping...</td>\n",
       "      <td>0.145527</td>\n",
       "      <td>0.150935</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0 min 2.97 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>0.059210</td>\n",
       "      <td>0.154395</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>1 min 25.46 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'criter...</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.155432</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0 min 49.18 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MLA Name  \\\n",
       "5              CatBoostRegressor   \n",
       "0                  LGBMRegressor   \n",
       "1                   XGBRegressor   \n",
       "4  HistGradientBoostingRegressor   \n",
       "2          RandomForestRegressor   \n",
       "3            ExtraTreesRegressor   \n",
       "\n",
       "                                      MLA Parameters  MLA Train ROC AUC  \\\n",
       "5  {'loss_function': 'RMSE', 'verbose': False, 'r...           0.140225   \n",
       "0  {'boosting_type': 'gbdt', 'class_weight': None...           0.145088   \n",
       "1  {'objective': 'reg:squarederror', 'base_score'...           0.134222   \n",
       "4  {'categorical_features': None, 'early_stopping...           0.145527   \n",
       "2  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...           0.059210   \n",
       "3  {'bootstrap': False, 'ccp_alpha': 0.0, 'criter...           0.001807   \n",
       "\n",
       "   MLA Test ROC AUC  MLA Test ROC AUC Std         MLA Time  \n",
       "5          0.149678              0.000216  0 min 41.68 sec  \n",
       "0          0.150577              0.000155   0 min 1.85 sec  \n",
       "1          0.150784              0.000279  0 min 13.62 sec  \n",
       "4          0.150935              0.000064   0 min 2.97 sec  \n",
       "2          0.154395              0.000395  1 min 25.46 sec  \n",
       "3          0.155432              0.000298  0 min 49.18 sec  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "sfs_models = evaluate_models(models, X, y, sfs_features, sk10, f'{experiment_name}_sfs')\n",
    "sfs_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_features = ['Sex_F', 'Sex_I', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMRegressor(n_jobs=-1, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_final = model1.fit(X[sfs_features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1_final.predict(test[sfs_features])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred, columns=['Rings'])\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], pred_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_lgbm_postprocess_asint_0.150577.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "# model2 = XGBClassifier(random_state=5)\n",
    "# model3 = RandomForestClassifier(random_state=5)\n",
    "# model4 = ExtraTreesClassifier(random_state=5)\n",
    "# model5 = HistGradientBoostingClassifier(random_state=5)\n",
    "# model6 = CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "model1b = LGBMClassifier(**lgbm_params_1)\n",
    "model1c = LGBMClassifier(**lgbm_params_2)\n",
    "model1d = LGBMClassifier(**lgbm_params_3)\n",
    "model1e = LGBMClassifier(**lgbm_params_4)\n",
    "model2 = XGBClassifier(random_state=5)\n",
    "model3 = RandomForestClassifier(random_state=5)\n",
    "model3b = RandomForestClassifier(**rf_params_1)\n",
    "model3c = RandomForestClassifier(**rf_params_2)\n",
    "model4 = ExtraTreesClassifier(random_state=5)\n",
    "model4b = ExtraTreesClassifier(**extrat_params_1)\n",
    "model4c = ExtraTreesClassifier(**extrat_params_2)\n",
    "model4d = ExtraTreesClassifier(**extrat_params_3)\n",
    "model4e = ExtraTreesClassifier(**extrat_params_4)\n",
    "model5 = HistGradientBoostingClassifier(random_state=5)\n",
    "model5b = HistGradientBoostingClassifier(**hist_params_1)\n",
    "model5c = HistGradientBoostingClassifier(**hist_params_2)\n",
    "model5d = HistGradientBoostingClassifier(**hist_params_3)\n",
    "model5e = HistGradientBoostingClassifier(**hist_params_4)\n",
    "model5f = HistGradientBoostingClassifier(**hist_params_5)\n",
    "model6 = CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "# model6b = CatBoostClassifier(**cat_params_1)\n",
    "# model6c = CatBoostClassifier(**cat_params_2)\n",
    "# model6d = CatBoostClassifier(**cat_params_3)\n",
    "model6e = CatBoostClassifier(**cat_params_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features for Competition + Original dataset down to SFS for all models (Experiment Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_feats = ['X_Minimum', 'Y_Minimum', 'Pixels_Areas', 'X_Perimeter', 'Minimum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Orientation_Index', 'Luminosity_Index']\n",
    "model2_feats = ['X_Minimum', 'Pixels_Areas', 'X_Perimeter', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Outside_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'Orientation_Index', 'Luminosity_Index']\n",
    "model3_feats = ['X_Minimum', 'Y_Minimum', 'Pixels_Areas', 'X_Perimeter', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Outside_X_Index', 'Edges_Y_Index', 'Orientation_Index', 'Luminosity_Index']\n",
    "model4_feats = ['X_Minimum', 'Y_Minimum', 'Pixels_Areas', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Outside_X_Index', 'Edges_Y_Index', 'Orientation_Index']\n",
    "model5_feats = ['X_Minimum', 'Y_Minimum', 'Pixels_Areas', 'Minimum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_Y_Index', 'Orientation_Index', 'Luminosity_Index']\n",
    "model6_feats = ['X_Minimum', 'Pixels_Areas', 'X_Perimeter', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Outside_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'Orientation_Index', 'Luminosity_Index']\n",
    "\n",
    "X_lgbm = X[model1_feats]\n",
    "X_xgb = X[model2_feats]\n",
    "X_rf = X[model3_feats]\n",
    "X_extrat = X[model4_feats]\n",
    "X_hist = X[model5_feats]\n",
    "X_cat = X[model6_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    class_weight_option = trial.suggest_categorical('class_weight', ['none', 'balanced', 'custom'])\n",
    "    if class_weight_option == 'none':\n",
    "        class_weight = None\n",
    "    elif class_weight_option == 'balanced':\n",
    "        class_weight = 'balanced'\n",
    "    else:\n",
    "        # For multi-class, you could define a range or specific values to test\n",
    "        weight_for_class_0 = trial.suggest_float('weight_for_class_0', 0.1, 10.0)\n",
    "        weight_for_class_1 = trial.suggest_float('weight_for_class_1', 0.1, 10.0)\n",
    "        weight_for_class_2 = trial.suggest_float('weight_for_class_2', 0.1, 10.0)\n",
    "        weight_for_class_3 = trial.suggest_float('weight_for_class_3', 0.1, 10.0)\n",
    "        weight_for_class_4 = trial.suggest_float('weight_for_class_4', 0.1, 10.0)\n",
    "        weight_for_class_5 = trial.suggest_float('weight_for_class_5', 0.1, 10.0)\n",
    "        weight_for_class_6 = trial.suggest_float('weight_for_class_6', 0.1, 10.0)\n",
    "        class_weight = {0: weight_for_class_0, 1: weight_for_class_1, 2: weight_for_class_2, 3: weight_for_class_3, 4: weight_for_class_4, 5: weight_for_class_5, 6: weight_for_class_6}\n",
    "\n",
    "    param = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 7,\n",
    "        'boosting_type': trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n",
    "        # 'class_weight': class_weight,\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.5),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', -1, 64),\n",
    "        # 'min_child_samples': trial.suggest_int('min_child_samples', 5, 500),\n",
    "        # 'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "        # 'min_split_gain': trial.suggest_float('min_split_gain', 0.5, 1.0),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        # 'n_jobs': -1,\n",
    "        # 'num_leaves': trial.suggest_int('num_leaves', 2, 1000),\n",
    "        # 'random_state': 5,\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        }\n",
    "\n",
    "    # # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    # param = {\n",
    "    #     'objective': 'multiclass',\n",
    "    #     'num_class': 7,\n",
    "    #     'boosting_type': trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n",
    "    #     'class_weight': class_weight,\n",
    "    #     'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "    #     'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.5),\n",
    "    #     'max_depth': trial.suggest_int('max_depth', -1, 64),\n",
    "    #     'min_child_samples': trial.suggest_int('min_child_samples', 5, 500),\n",
    "    #     'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "    #     'min_split_gain': trial.suggest_float('min_split_gain', 0.5, 1.0),\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "    #     'n_jobs': -1,\n",
    "    #     'num_leaves': trial.suggest_int('num_leaves', 2, 1000),\n",
    "    #     'random_state': 5,\n",
    "    #     'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "    #     'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "    #     'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "    #     }\n",
    "    \n",
    "    roc_auc_scores = []\n",
    "    \n",
    "    for train_index, test_index in sk10.split(X_lgbm, y):\n",
    "        X_train, X_test = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = LGBMClassifier(**param)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "# Using median pruner\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n",
    "# 0.893695902"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HistGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'learning_rate': trial.suggest_float('learning_rate', 0.000001, 0.15),\n",
    "    #     # 'max_iter': trial.suggest_int('max_iter', 50, 5000),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20, 1000),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 1000),\n",
    "    #     # 'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 1.0),\n",
    "    #     # 'max_bins': trial.suggest_int('max_bins', 10, 255),\n",
    "    #     'max_depth': trial.suggest_int('max_depth', 2, 64),\n",
    "    #     'random_state': 5,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.06),\n",
    "        'max_iter': 100,\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 50),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 300, 800),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.8, 1.0),\n",
    "        'max_bins': trial.suggest_int('max_bins', 50, 150),\n",
    "        'max_depth': 25,\n",
    "        'random_state': 5,\n",
    "    }\n",
    "\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_hist, y)):\n",
    "        X_train, X_test = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = HistGradientBoostingClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(roc_auc, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if roc_auc < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "    #     # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "    #     # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "    #     # 'max_depth': trial.suggest_int('max_depth', 10, 3000),\n",
    "    #     # 'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 100, 3000, log=True) or None,\n",
    "    #     # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.1),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf',1, 500),\n",
    "    #     # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 500),\n",
    "    #     # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n",
    "    #     'random_state': 5,\n",
    "    #     'n_jobs': -1,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'ccp_alpha': 0.0,\n",
    "        'class_weight': None,\n",
    "        'criterion': 'gini',\n",
    "        'max_depth': None,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 1000, 3000, log=True) or None,\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf',1, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n",
    "        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.002),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2500, 3000),\n",
    "        'random_state': 5,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_extrat, y)):\n",
    "        X_train, X_test = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = ExtraTreesClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(roc_auc, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if roc_auc < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=0, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # # # Raw Parameters for individual tunings\n",
    "    # param = {\n",
    "    #     # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "    #     # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "    #     # 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "    #     # 'max_depth': trial.suggest_int('max_depth', 10, 1000, log=True) or None,\n",
    "    #     # 'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "    #     # 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 1000, log=True) or None,\n",
    "    #     # 'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "    #     # 'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05),\n",
    "    #     # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "    #     # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 200),\n",
    "    #     # 'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.1),\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "    #     'random_state': 5,\n",
    "    #     'n_jobs': -1,\n",
    "    # }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    param = {\n",
    "        'ccp_alpha': 0.0,\n",
    "        'class_weight': None,\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 30, log=True),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 300, 700, log=True) or None,\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'log2',\n",
    "        'criterion': 'entropy',\n",
    "        'n_estimators': 1000,\n",
    "        'random_state': 5,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_rf, y)):\n",
    "        X_train, X_test = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(roc_auc, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if roc_auc < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Raw Parameters for individual tunings\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 2000),\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.3, log=True),\n",
    "        # 'depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.05, 1),\n",
    "        # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "        # 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        # 'l2_leaf_reg': trial.suggest_float('l2_reg', 1e-2, 10),\n",
    "        # 'random_strength': trial.suggest_float('random_strength', 1e-2, 10),\n",
    "        # 'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_state': 5,\n",
    "        'verbose': False,\n",
    "    }\n",
    "\n",
    "    # Group Parameters after individual tunings (change after testing the individual params)\n",
    "    # param = {\n",
    "    #     'iterations': trial.suggest_int('iterations', 50, 2000),\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "    #     'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.3, log=True),\n",
    "    #     'depth': trial.suggest_int('max_depth', 1, 10),\n",
    "    #     'subsample': trial.suggest_float('subsample', 0.05, 1),\n",
    "    #     'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "    #     'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "    #     'l2_leaf_reg': trial.suggest_float('l2_reg', 1e-2, 10),\n",
    "    #     'random_strength': trial.suggest_float('random_strength', 1e-2, 10),\n",
    "    #     'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "    #     'random_state': 5,\n",
    "    #     'verbose': False,\n",
    "    # }\n",
    "\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sk10.split(X_rf, y)):\n",
    "        X_train, X_test = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = CatBoostClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, preds, multi_class='ovr', average='macro')\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(roc_auc, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "        # # Check if performance is below threshold\n",
    "        # if roc_auc < performance_threshold:\n",
    "        #     raise optuna.exceptions.TrialPruned('ROC score lower than threshold.')\n",
    "\n",
    "    return np.mean(roc_auc_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model1b_results, model1c_results, model1d_results, model1e_results, model2_results, model3_results, model3b_results, model3c_results, model4_results, model4b_results, model4c_results, model4d_results, model4e_results, model5_results, model5b_results, model5c_results, model5d_results, model5e_results, model5f_results, model6_results, model6b_results, model6c_results, model6d_results, model6e_results, y_test_list = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train_lgbm, X_test_lgbm = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "    X_train_xgb, X_test_xgb = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "    X_train_rf, X_test_rf = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "    X_train_extrat, X_test_extrat = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "    X_train_hist, X_test_hist = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "    X_train_cat, X_test_cat = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model1.fit(X_train_lgbm, y_train)\n",
    "    model1_results.append(model1.predict_proba(X_test_lgbm))\n",
    "\n",
    "    model1b.fit(X_train_lgbm, y_train)\n",
    "    model1b_results.append(model1b.predict_proba(X_test_lgbm))\n",
    "\n",
    "    model1c.fit(X_train_lgbm, y_train)\n",
    "    model1c_results.append(model1c.predict_proba(X_test_lgbm))\n",
    "\n",
    "    model1d.fit(X_train_lgbm, y_train)\n",
    "    model1d_results.append(model1d.predict_proba(X_test_lgbm))\n",
    "\n",
    "    model1e.fit(X_train_lgbm, y_train)\n",
    "    model1e_results.append(model1e.predict_proba(X_test_lgbm))\n",
    "\n",
    "    print('Running XGBoost')\n",
    "\n",
    "    model2.fit(X_train_xgb, y_train)\n",
    "    model2_results.append(model2.predict_proba(X_test_xgb))\n",
    "\n",
    "    print('Running Random Forest')\n",
    "\n",
    "    model3.fit(X_train_rf, y_train)\n",
    "    model3_results.append(model3.predict_proba(X_test_rf))\n",
    "\n",
    "    model3b.fit(X_train_rf, y_train)\n",
    "    model3b_results.append(model3b.predict_proba(X_test_rf))\n",
    "\n",
    "    model3c.fit(X_train_rf, y_train)\n",
    "    model3c_results.append(model3c.predict_proba(X_test_rf))\n",
    "\n",
    "    print('Running ExtraTrees')\n",
    "\n",
    "    model4.fit(X_train_extrat, y_train)\n",
    "    model4_results.append(model4.predict_proba(X_test_extrat))\n",
    "\n",
    "    model4b.fit(X_train_extrat, y_train)\n",
    "    model4b_results.append(model4b.predict_proba(X_test_extrat))\n",
    "\n",
    "    model4c.fit(X_train_extrat, y_train)\n",
    "    model4c_results.append(model4c.predict_proba(X_test_extrat))\n",
    "\n",
    "    model4d.fit(X_train_extrat, y_train)\n",
    "    model4d_results.append(model4d.predict_proba(X_test_extrat))\n",
    "\n",
    "    model4e.fit(X_train_extrat, y_train)\n",
    "    model4e_results.append(model4e.predict_proba(X_test_extrat))\n",
    "\n",
    "    print('Running Hist Gradient')\n",
    "\n",
    "    model5.fit(X_train_hist, y_train)\n",
    "    model5_results.append(model5.predict_proba(X_test_hist))\n",
    "\n",
    "    model5b.fit(X_train_hist, y_train)\n",
    "    model5b_results.append(model5b.predict_proba(X_test_hist))\n",
    "\n",
    "    model5c.fit(X_train_hist, y_train)\n",
    "    model5c_results.append(model5c.predict_proba(X_test_hist))\n",
    "\n",
    "    model5d.fit(X_train_hist, y_train)\n",
    "    model5d_results.append(model5d.predict_proba(X_test_hist))\n",
    "\n",
    "    model5e.fit(X_train_hist, y_train)\n",
    "    model5e_results.append(model5e.predict_proba(X_test_hist))\n",
    "\n",
    "    model5f.fit(X_train_hist, y_train)\n",
    "    model5f_results.append(model5f.predict_proba(X_test_hist))\n",
    "\n",
    "    print('Running CatBoost')\n",
    "\n",
    "    model6.fit(X_train_cat, y_train)\n",
    "    model6_results.append(model6.predict_proba(X_test_cat))\n",
    "\n",
    "    # model6b.fit(X_train_cat, y_train)\n",
    "    # model6b_results.append(model6b.predict_proba(X_test_cat))\n",
    "\n",
    "    # model6c.fit(X_train_cat, y_train)\n",
    "    # model6c_results.append(model6c.predict_proba(X_test_cat))\n",
    "\n",
    "    # model6d.fit(X_train_cat, y_train)\n",
    "    # model6d_results.append(model6d.predict_proba(X_test_cat))\n",
    "\n",
    "    model6e.fit(X_train_cat, y_train)\n",
    "    model6e_results.append(model6e.predict_proba(X_test_cat))\n",
    "\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# model1_weights, model1b_weights, model1c_weights, model1d_weights, model1e_weights, model2_weights, model3_weights, model3b_weights, model3c_weights, model4_weights, model4b_weights, model4c_weights, model4d_weights, model4e_weights, model5_weights, model5b_weights, model5c_weights, model5d_weights, model5e_weights, model5f_weights, model6_weights, model6b_weights, model6c_weights, model6d_weights, model6e_weights, scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "model1_weights, model1b_weights, model1c_weights, model1d_weights, model1e_weights, model2_weights, model3_weights, model3b_weights, model3c_weights, model4_weights, model4b_weights, model4c_weights, model4d_weights, model4e_weights, model5_weights, model5b_weights, model5c_weights, model5d_weights, model5e_weights, model5f_weights, model6_weights, model6e_weights, scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "scores_in = []\n",
    "\n",
    "for i in tqdm(range(20)):\n",
    "    weight_1 = np.random.random_sample(size=1)[0]\n",
    "    weight_1b = np.random.random_sample(size=1)[0]\n",
    "    weight_1c = np.random.random_sample(size=1)[0]\n",
    "    weight_1d = np.random.random_sample(size=1)[0]\n",
    "    weight_1e = np.random.random_sample(size=1)[0]\n",
    "    weight_2 = np.random.random_sample(size=1)[0]\n",
    "    weight_3 = np.random.random_sample(size=1)[0]\n",
    "    weight_3b = np.random.random_sample(size=1)[0]\n",
    "    weight_3c = np.random.random_sample(size=1)[0]\n",
    "    weight_4 = np.random.random_sample(size=1)[0]\n",
    "    weight_4b = np.random.random_sample(size=1)[0]\n",
    "    weight_4c = np.random.random_sample(size=1)[0]\n",
    "    weight_4d = np.random.random_sample(size=1)[0]\n",
    "    weight_4e = np.random.random_sample(size=1)[0]\n",
    "    weight_5 = np.random.random_sample(size=1)[0]\n",
    "    weight_5b = np.random.random_sample(size=1)[0]\n",
    "    weight_5c = np.random.random_sample(size=1)[0]\n",
    "    weight_5d = np.random.random_sample(size=1)[0]\n",
    "    weight_5e = np.random.random_sample(size=1)[0]\n",
    "    weight_5f = np.random.random_sample(size=1)[0]\n",
    "    weight_6 = np.random.random_sample(size=1)[0]\n",
    "    # weight_6b = np.random.random_sample(size=1)[0]\n",
    "    # weight_6c = np.random.random_sample(size=1)[0]\n",
    "    # weight_6d = np.random.random_sample(size=1)[0]\n",
    "    weight_6e = np.random.random_sample(size=1)[0]\n",
    "\n",
    "    model1_weights.append(weight_1)\n",
    "    model1b_weights.append(weight_1b)\n",
    "    model1c_weights.append(weight_1c)\n",
    "    model1d_weights.append(weight_1d)\n",
    "    model1e_weights.append(weight_1e)\n",
    "    model2_weights.append(weight_2)\n",
    "    model3_weights.append(weight_3)\n",
    "    model3b_weights.append(weight_3b)\n",
    "    model3c_weights.append(weight_3c)\n",
    "    model4_weights.append(weight_4)\n",
    "    model4b_weights.append(weight_4b)\n",
    "    model4c_weights.append(weight_4c)\n",
    "    model4d_weights.append(weight_4d)\n",
    "    model4e_weights.append(weight_4e)\n",
    "    model5_weights.append(weight_5)\n",
    "    model5b_weights.append(weight_5b)\n",
    "    model5c_weights.append(weight_5c)\n",
    "    model5d_weights.append(weight_5d)\n",
    "    model5e_weights.append(weight_5e)\n",
    "    model5f_weights.append(weight_5f)\n",
    "    model6_weights.append(weight_6)\n",
    "    # model6b_weights.append(weight_6b)\n",
    "    # model6c_weights.append(weight_6c)\n",
    "    # model6d_weights.append(weight_6d)\n",
    "    model6e_weights.append(weight_6e)\n",
    "\n",
    "    # scores_in = []\n",
    "\n",
    "    for j in range(n_splits):\n",
    "        weighted_pred = (weight_1 * model1_results[j])\n",
    "        + (weight_1b * model1b_results[j])\n",
    "        + (weight_1c * model1c_results[j])\n",
    "        + (weight_1d * model1d_results[j])\n",
    "        + (weight_1e * model1e_results[j])\n",
    "        + (weight_2 * model2_results[j])\n",
    "        + (weight_3 * model3_results[j])\n",
    "        + (weight_3b * model3b_results[j])\n",
    "        + (weight_3c * model3c_results[j])\n",
    "        + (weight_4 * model4_results[j])\n",
    "        + (weight_4b * model4b_results[j])\n",
    "        + (weight_4c * model4c_results[j])\n",
    "        + (weight_4d * model4d_results[j])\n",
    "        + (weight_4e * model4e_results[j])\n",
    "        + (weight_5 * model5_results[j])\n",
    "        + (weight_5b * model5b_results[j])\n",
    "        + (weight_5c * model5c_results[j])\n",
    "        + (weight_5d * model5d_results[j])\n",
    "        + (weight_5e * model5e_results[j])\n",
    "        + (weight_5f * model5f_results[j])\n",
    "        + (weight_6 * model6_results[j])\n",
    "        # + (weight_6b * model6b_results[j])\n",
    "        # + (weight_6c * model6c_results[j])\n",
    "        # + (weight_6d * model6d_results[j])\n",
    "        + (weight_6e * model6e_results[j])\n",
    "\n",
    "        weighted_pred_normalized = weighted_pred / np.sum(weighted_pred, axis=1, keepdims=True)\n",
    "\n",
    "        scores_in.append(roc_auc_score(y_test_list[j], weighted_pred_normalized, multi_class='ovr'))\n",
    "        \n",
    "    scores.append(np.mean(scores_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the predictions for each model and the true labels\n",
    "all_predictions = [np.concatenate(model_results) for model_results in [model1_results, model1b_results, model1c_results, model1d_results, model1e_results, model2_results, model3_results, model3b_results, model3c_results, model4_results, model4b_results, model4c_results, model4d_results, model4e_results, model5_results, model5b_results, model5c_results, model5d_results, model5e_results, model5f_results, model6_results, model6e_results]]\n",
    "all_true_labels = np.concatenate(y_test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Generate weights for each model's prediction\n",
    "    weights = [trial.suggest_float(f'w{i}', -1, 1) for i in range(len(all_predictions))]\n",
    "\n",
    "    # Compute weighted sum of predictions\n",
    "    weighted_sum = np.zeros_like(all_predictions[0])\n",
    "    for weight, prediction in zip(weights, all_predictions):\n",
    "        weighted_sum += weight * prediction\n",
    "\n",
    "    # Normalize the weighted sum to ensure it forms a proper probability distribution\n",
    "    weighted_sum_normalized = np.divide(weighted_sum, np.sum(weighted_sum, axis=1, keepdims=True))\n",
    "    \n",
    "    # Compute and return the multi-class ROC AUC score\n",
    "    # Note: You might need to adjust the `average` parameter based on how you want to average the AUCs\n",
    "    score = roc_auc_score(all_true_labels, weighted_sum_normalized, multi_class='ovr', average='macro')\n",
    "    return score\n",
    "\n",
    "# def switch_sampler(study, trials):\n",
    "#     if len(study.trials) == 250:\n",
    "#         study.sampler = TPESampler(seed=5)\n",
    "#     # elif len(study.trials) == 50:\n",
    "#     #     study.sampler = TPESampler()\n",
    "#     # elif len(study.trials) == 100:\n",
    "#     #     study.sampler = TPESampler()\n",
    "#     # elif len(study.trials) == 150:\n",
    "#     #     study.sampler = TPESampler()\n",
    "\n",
    "# sampler = RandomSampler(seed=5)\n",
    "\n",
    "# # Create an Optuna study and optimize the objective\n",
    "# study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=500, callbacks=[switch_sampler])\n",
    "\n",
    "# Create an Optuna study and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Print the optimal weights found\n",
    "print(\"Optimal weights:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimal weights from Optuna study\n",
    "optimal_weights = study.best_params\n",
    "\n",
    "# Assuming `optimal_weights` is a dictionary with model identifiers as keys\n",
    "# and the optimized weight as values, you can directly use it to create a DataFrame\n",
    "# For the 'scores', you would use the best score achieved during the Optuna study\n",
    "optuna_results_df = pd.DataFrame([optimal_weights])\n",
    "optuna_results_df['score'] = study.best_value\n",
    "\n",
    "optuna_results_df.columns = ['model_1', 'model_1b', 'model_1c', 'model_1d', 'model_1e', 'model_2', 'model_3', 'model_3b', 'model_3c', 'model_4', 'model_4b', 'model_4c', 'model_4d', 'model_4e', 'model_5', 'model_5b', 'model_5c', 'model_5d', 'model_5e', 'model_5f', 'model_6', 'model_6e', 'score']\n",
    "\n",
    "# Since you only have one row of data (the best combination of weights),\n",
    "# sorting by 'score' or getting the top rows doesn't apply as it's already the best\n",
    "optuna_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df['model_1'] = model1_weights\n",
    "results_df['model_1b'] = model1b_weights\n",
    "results_df['model_1c'] = model1c_weights\n",
    "results_df['model_1d'] = model1d_weights\n",
    "results_df['model_1e'] = model1e_weights\n",
    "results_df['model_2'] = model2_weights\n",
    "results_df['model_3'] = model3_weights\n",
    "results_df['model_3b'] = model3b_weights\n",
    "results_df['model_3c'] = model3c_weights\n",
    "results_df['model_4'] = model4_weights\n",
    "results_df['model_4b'] = model4b_weights\n",
    "results_df['model_4c'] = model4c_weights\n",
    "results_df['model_4d'] = model4d_weights\n",
    "results_df['model_4e'] = model4e_weights\n",
    "results_df['model_5'] = model5_weights\n",
    "results_df['model_5b'] = model5b_weights\n",
    "results_df['model_5c'] = model5c_weights\n",
    "results_df['model_5d'] = model5d_weights\n",
    "results_df['model_5e'] = model5e_weights\n",
    "results_df['model_5f'] = model5f_weights\n",
    "results_df['model_6'] = model6_weights\n",
    "# results_df['model_6b'] = model6b_weights\n",
    "# results_df['model_6c'] = model6c_weights\n",
    "# results_df['model_6d'] = model6d_weights\n",
    "results_df['model_6e'] = model6e_weights\n",
    "results_df['score'] = scores\n",
    "\n",
    "results_df = results_df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('random_weights_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Submission (Random Weight Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Running LGBM')\n",
    "model1_final = model1.fit(X_lgbm, y)\n",
    "model1b_final = model1b.fit(X_lgbm, y)\n",
    "model1c_final = model1c.fit(X_lgbm, y)\n",
    "model1d_final = model1d.fit(X_lgbm, y)\n",
    "model1e_final = model1e.fit(X_lgbm, y)\n",
    "\n",
    "print('Running XGBoost')\n",
    "model2_final = model2.fit(X_xgb, y)\n",
    "\n",
    "print('Running Random Forest')\n",
    "model3_final = model3.fit(X_rf, y)\n",
    "model3b_final = model3b.fit(X_rf, y)\n",
    "model3c_final = model3c.fit(X_rf, y)\n",
    "\n",
    "print('Running ExtraTrees')\n",
    "model4_final = model4.fit(X_extrat, y)\n",
    "model4b_final = model4b.fit(X_extrat, y)\n",
    "model4c_final = model4c.fit(X_extrat, y)\n",
    "model4d_final = model4d.fit(X_extrat, y)\n",
    "model4e_final = model4e.fit(X_extrat, y)\n",
    "\n",
    "print('Running HistGradient')\n",
    "model5_final = model5.fit(X_hist, y)\n",
    "model5b_final = model5b.fit(X_hist, y)\n",
    "model5c_final = model5c.fit(X_hist, y)\n",
    "model5d_final = model5d.fit(X_hist, y)\n",
    "model5e_final = model5e.fit(X_hist, y)\n",
    "model5f_final = model5f.fit(X_hist, y)\n",
    "\n",
    "print('Running CatBoost')\n",
    "model6_final = model6.fit(X_cat, y)\n",
    "# model6b_final = model6b.fit(X_cat, y)\n",
    "# model6c_final = model6c.fit(X_cat, y)\n",
    "# model6d_final = model6d.fit(X_cat, y)\n",
    "model6e_final = model6e.fit(X_cat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ensemble_pred = (\n",
    "                results_df['model_1'][0] * model1_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1b'][0] * model1b_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1c'][0] * model1c_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1d'][0] * model1d_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_1e'][0] * model1e_final.predict_proba(test[model1_feats]) +\n",
    "                results_df['model_2'][0] * model2_final.predict_proba(test[model2_feats]) +\n",
    "                results_df['model_3'][0] * model3_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_3b'][0] * model3b_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_3c'][0] * model3c_final.predict_proba(test[model3_feats]) +\n",
    "                results_df['model_4'][0] * model4_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4b'][0] * model4b_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4c'][0] * model4c_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4d'][0] * model4d_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_4e'][0] * model4e_final.predict_proba(test[model4_feats]) +\n",
    "                results_df['model_5'][0] * model5_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5b'][0] * model5b_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5c'][0] * model5c_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5d'][0] * model5d_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5e'][0] * model5e_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_5f'][0] * model5f_final.predict_proba(test[model5_feats]) +\n",
    "                results_df['model_6'][0] * model6_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6b'][0] * model6b_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6c'][0] * model6c_final.predict_proba(test[model6_feats]) +\n",
    "                # results_df['model_6d'][0] * model6d_final.predict_proba(test[model6_feats]) +\n",
    "                results_df['model_6e'][0] * model6e_final.predict_proba(test[model6_feats])\n",
    "                 )\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_pred)\n",
    "\n",
    "# If all models predict 0, instead of getting NaN, fill in 0\n",
    "ensemble_df = ensemble_df.div(ensemble_df.sum(axis=1), axis=0).fillna(0)\n",
    "ensemble_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "optuna_ensemble_pred = (\n",
    "                optuna_results_df['model_1'][0] * model1_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1b'][0] * model1b_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1c'][0] * model1c_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1d'][0] * model1d_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_1e'][0] * model1e_final.predict_proba(test[model1_feats]) +\n",
    "                optuna_results_df['model_2'][0] * model2_final.predict_proba(test[model2_feats]) +\n",
    "                optuna_results_df['model_3'][0] * model3_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_3b'][0] * model3b_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_3c'][0] * model3c_final.predict_proba(test[model3_feats]) +\n",
    "                optuna_results_df['model_4'][0] * model4_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4b'][0] * model4b_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4c'][0] * model4c_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4d'][0] * model4d_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_4e'][0] * model4e_final.predict_proba(test[model4_feats]) +\n",
    "                optuna_results_df['model_5'][0] * model5_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5b'][0] * model5b_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5c'][0] * model5c_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5d'][0] * model5d_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5e'][0] * model5e_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_5f'][0] * model5f_final.predict_proba(test[model5_feats]) +\n",
    "                optuna_results_df['model_6'][0] * model6_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6b'][0] * model6b_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6c'][0] * model6c_final.predict_proba(test[model6_feats]) +\n",
    "                # optuna_results_df['model_6d'][0] * model6d_final.predict_proba(test[model6_feats]) +\n",
    "                optuna_results_df['model_6e'][0] * model6e_final.predict_proba(test[model6_feats])\n",
    "                 )\n",
    "\n",
    "optuna_ensemble_df = pd.DataFrame(optuna_ensemble_pred)\n",
    "\n",
    "# If all models predict 0, instead of getting NaN, fill in 0\n",
    "optuna_ensemble_df = optuna_ensemble_df.div(optuna_ensemble_df.sum(axis=1), axis=0).fillna(0)\n",
    "optuna_ensemble_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], optuna_ensemble_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_optuna_weights_ensemble_3fold_0.902197.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get submission (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('model1', model1_final),\n",
    "    ('model2', model2_final),\n",
    "    ('model3', model3_final),\n",
    "    ('model4', model4_final),\n",
    "    ('model5', model5_final),\n",
    "    ('model6', model6_final)\n",
    "]\n",
    "\n",
    "# Initialize the Stacking Classifier with LogisticRegression as the final estimator\n",
    "final_estimator = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "# final_estimator = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "# final_estimator = XGBClassifier(random_state=5)\n",
    "# final_estimator = RandomForestClassifier(random_state=5)\n",
    "# final_estimator = ExtraTreesClassifier(random_state=5)\n",
    "# final_estimator = HistGradientBoostingClassifier(random_state=5)\n",
    "# final_estimator = CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=final_estimator, passthrough=False, cv=3)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict_proba(X_test)\n",
    "\n",
    "    # Assuming your classes are 0, 1, 2, etc., adjust as necessary\n",
    "    y_test_binarized = label_binarize(y_test, classes=np.unique(y))\n",
    "    roc_auc = roc_auc_score(y_test_binarized, y_pred, multi_class='ovr')\n",
    "\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')\n",
    "    \n",
    "print(f'The average stacking score is {np.mean(roc_auc_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Reg - 0.886778\n",
    "- LGBM - 0.885863\n",
    "- XGB - 0.881636\n",
    "- RF - 0.883835\n",
    "- ET - 0.884523\n",
    "- Hist - 0.886572\n",
    "- Cat - 0.886183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on unseen test data\n",
    "y_test_pred = stacking_clf.predict_proba(test)\n",
    "\n",
    "stacking_df = pd.DataFrame(y_test_pred)\n",
    "\n",
    "ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model2_results, model3_results, model4_results, model5_results, model6_results, y_test_list = [], [], [], [], [], [], []\n",
    "\n",
    "# # Placeholder for OOF predictions for each model\n",
    "# # Assuming you have a dataset with N samples\n",
    "# N = len(y)  # y_train is your target variable array\n",
    "# oof_preds1 = np.zeros((N, 1))\n",
    "# oof_preds2 = np.zeros((N, 1))\n",
    "# oof_preds3 = np.zeros((N, 1))\n",
    "# oof_preds4 = np.zeros((N, 1))\n",
    "# oof_preds5 = np.zeros((N, 1))\n",
    "# oof_preds6 = np.zeros((N, 1))\n",
    "\n",
    "# # Similarly, for test predictions, accumulate them over folds\n",
    "# # Assuming you have a test set with M samples\n",
    "# M = len(test)  # x_test needs to be defined by you\n",
    "# test_preds1 = np.zeros((M, 1))\n",
    "# test_preds2 = np.zeros((M, 1))\n",
    "# test_preds3 = np.zeros((M, 1))\n",
    "# test_preds4 = np.zeros((M, 1))\n",
    "# test_preds5 = np.zeros((M, 1))\n",
    "# test_preds6 = np.zeros((M, 1))\n",
    "\n",
    "target_length = len(y)\n",
    "no_classes = len(np.unique(y))\n",
    "test_length = len(test)\n",
    "\n",
    "# Initialize arrays for OOF and test predictions with dimensions for multiclass for each model\n",
    "lgbm_oof_preds = np.zeros((target_length, no_classes))\n",
    "lgbm_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "xgb_oof_preds = np.zeros((target_length, no_classes))\n",
    "xgb_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "rf_oof_preds = np.zeros((target_length, no_classes))\n",
    "rf_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "extrat_oof_preds = np.zeros((target_length, no_classes))\n",
    "extrat_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "hist_oof_preds = np.zeros((target_length, no_classes))\n",
    "hist_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "cat_oof_preds = np.zeros((target_length, no_classes))\n",
    "cat_test_preds = np.zeros((test_length, no_classes))\n",
    "\n",
    "X_lgbm = X[model1_feats]\n",
    "X_xgb = X[model2_feats]\n",
    "X_rf = X[model3_feats]\n",
    "X_extrat = X[model4_feats]\n",
    "X_hist = X[model5_feats]\n",
    "X_cat = X[model6_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "\n",
    "    # Placeholder arrays for the fold's predicition\n",
    "    fold_oof_preds_lgbm = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_lgbm = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_xgb = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_xgb = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_rf = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_rf = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_extrat = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_extrat = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_hist = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_hist = np.zeros((test_length, no_classes))\n",
    "\n",
    "    fold_oof_preds_cat = np.zeros((len(test_index), no_classes))\n",
    "    fold_test_preds_cat = np.zeros((test_length, no_classes))\n",
    "\n",
    "    # Get each models train and test for X and y\n",
    "    X_train_lgbm, X_test_lgbm = X_lgbm.iloc[train_index], X_lgbm.iloc[test_index]\n",
    "    X_train_xgb, X_test_xgb = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
    "    X_train_rf, X_test_rf = X_rf.iloc[train_index], X_rf.iloc[test_index]\n",
    "    X_train_extrat, X_test_extrat = X_extrat.iloc[train_index], X_extrat.iloc[test_index]\n",
    "    X_train_hist, X_test_hist = X_hist.iloc[train_index], X_hist.iloc[test_index]\n",
    "    X_train_cat, X_test_cat = X_cat.iloc[train_index], X_cat.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    ########\n",
    "    # LGBM #\n",
    "    ########\n",
    "    model1.fit(X_train_lgbm, y_train)\n",
    "    fold_oof_preds_lgbm = model1.predict_proba(X_test_lgbm)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    lgbm_oof_preds[test_index] = fold_oof_preds_lgbm\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_lgbm += model1.predict_proba(test.loc[:, model1_feats]) / sk10.n_splits\n",
    "\n",
    "    lgbm_test_preds += fold_test_preds_lgbm\n",
    "\n",
    "\n",
    "    ###########\n",
    "    # XGBOOST #\n",
    "    ###########\n",
    "    model2.fit(X_train_xgb, y_train)\n",
    "    fold_oof_preds_xgb = model2.predict_proba(X_test_xgb)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    xgb_oof_preds[test_index] = fold_oof_preds_xgb\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_xgb += model2.predict_proba(test.loc[:, model2_feats]) / sk10.n_splits\n",
    "\n",
    "    xgb_test_preds += fold_test_preds_xgb\n",
    "\n",
    "\n",
    "    #################\n",
    "    # RANDOM FOREST #\n",
    "    #################\n",
    "    model3.fit(X_train_rf, y_train)\n",
    "    fold_oof_preds_rf = model3.predict_proba(X_test_rf)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    rf_oof_preds[test_index] = fold_oof_preds_rf\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_rf += model3.predict_proba(test.loc[:, model3_feats]) / sk10.n_splits\n",
    "\n",
    "    rf_test_preds += fold_test_preds_rf\n",
    "\n",
    "    \n",
    "    ###############\n",
    "    # EXTRA TREES #\n",
    "    ###############\n",
    "    model4.fit(X_train_extrat, y_train)\n",
    "    fold_oof_preds_extrat = model4.predict_proba(X_test_extrat)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    extrat_oof_preds[test_index] = fold_oof_preds_extrat\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_extrat += model4.predict_proba(test.loc[:, model4_feats]) / sk10.n_splits\n",
    "\n",
    "    extrat_test_preds += fold_test_preds_extrat\n",
    "\n",
    "\n",
    "    #################\n",
    "    # HIST GRADIENT #\n",
    "    #################\n",
    "    model5.fit(X_train_hist, y_train)\n",
    "    fold_oof_preds_hist = model5.predict_proba(X_test_hist)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    hist_oof_preds[test_index] = fold_oof_preds_hist\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_hist += model5.predict_proba(test.loc[:, model5_feats]) / sk10.n_splits\n",
    "\n",
    "    hist_test_preds += fold_test_preds_hist\n",
    "\n",
    "\n",
    "    ############\n",
    "    # CATBOOST #\n",
    "    ############\n",
    "    model6.fit(X_train_cat, y_train)\n",
    "    fold_oof_preds_cat = model6.predict_proba(X_test_cat)\n",
    "\n",
    "    # Update the OOF prediction for this fold\n",
    "    cat_oof_preds[test_index] = fold_oof_preds_cat\n",
    "\n",
    "    # Predict on the test set and accumulate predictions\n",
    "    fold_test_preds_cat += model6.predict_proba(test.loc[:, model6_feats]) / sk10.n_splits\n",
    "\n",
    "    cat_test_preds += fold_test_preds_cat\n",
    "    # y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_scores = [roc_auc_score((y == class_id).astype(int), oof_preds[:, class_id], multi_class='ovr') for class_id in range(no_classes)]\n",
    "lgbm_roc_auc = roc_auc_score(y, lgbm_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average LGBM ROC AUC Score:\", lgbm_roc_auc)\n",
    "\n",
    "xgb_roc_auc = roc_auc_score(y, xgb_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average XGBoost ROC AUC Score:\", xgb_roc_auc)\n",
    "\n",
    "rf_roc_auc = roc_auc_score(y, rf_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Random Forest ROC AUC Score:\", rf_roc_auc)\n",
    "\n",
    "extrat_roc_auc = roc_auc_score(y, extrat_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Extra Trees ROC AUC Score:\", extrat_roc_auc)\n",
    "\n",
    "hist_roc_auc = roc_auc_score(y, hist_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average Hist Gradient ROC AUC Score:\", hist_roc_auc)\n",
    "\n",
    "cat_roc_auc = roc_auc_score(y, cat_oof_preds, multi_class='ovr', average='macro')\n",
    "print(\"Average CatBoost ROC AUC Score:\", cat_roc_auc)\n",
    "\n",
    "# 0.89369590207664\n",
    "# 0.00201442835387733\n",
    "# 0.886778 - StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# After running the fitting and prediction with the first level of machine learning models\n",
    "x_train = np.concatenate(( lgbm_oof_preds, xgb_oof_preds, rf_oof_preds, extrat_oof_preds, hist_oof_preds, cat_oof_preds), axis=1)\n",
    "test_stack = np.concatenate(( lgbm_test_preds, xgb_test_preds, rf_test_preds, extrat_test_preds, hist_test_preds, cat_test_preds), axis=1)\n",
    "\n",
    "# Assuming the second-level stacking is to be done with XGboost (pre-tuned). Yes! You can tune second-level stack\n",
    "\n",
    "stacking_estimator = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "xgb = stacking_estimator.fit(x_train, y)\n",
    "final_predictions = xgb.predict_proba(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = np.zeros((x_train.shape[0], no_classes))\n",
    "test_preds = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(x_train, y)):\n",
    "    X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model2.fit(X_train, y_train)\n",
    "    y_pred = model2.predict_proba(X_test)\n",
    "\n",
    "    # Assign predictions for this fold to the appropriate indices in oof_preds\n",
    "    oof_preds[test_index, :] = y_pred\n",
    "    \n",
    "    print(f'Done with fold {i+1}.')\n",
    "\n",
    "# Calculate ROC AUC on the OOF predictions\n",
    "roc_auc = roc_auc_score(y, oof_preds, multi_class='ovr', average='macro')\n",
    "print(f'The stacking score is {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Reg - 0.8883102077923056\n",
    "- LGBM - 0.8880225088607244\n",
    "- XGB - 0.8846028966376445\n",
    "- RF - \n",
    "- ET - \n",
    "- Hist - \n",
    "- Cat - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.DataFrame(final_predictions)\n",
    "final_predictions_df.columns = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], final_predictions_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_stacking_3fold_0.88831.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
