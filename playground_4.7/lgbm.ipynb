{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ast\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "experiment_name = 'synthetic_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11504798, 12), (7669866, 11), (381109, 12))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(r'data\\train.csv')\n",
    "test = pd.read_csv(r'data\\test.csv')\n",
    "original = pd.read_csv(r'data\\original_train.csv')\n",
    "\n",
    "train.shape, test.shape, original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different feature types and target\n",
    "cat_cols = ['Vehicle_Age']\n",
    "num_cols = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
    "ord_cols = []\n",
    "bin_cols = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Damage']\n",
    "drop_cols = ['id']\n",
    "TARGET = 'Response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train, original], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = pd.get_dummies(combined_df, columns= bin_cols+cat_cols, drop_first=True, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11885907, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11885907, 11), (11885907,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign X and y\n",
    "X = ohe.drop(drop_cols + [TARGET], axis=1)\n",
    "y = ohe[TARGET]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 3\n",
    "\n",
    "sk3 = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logistic', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Manually set pipeline names\n",
    "logistic_pipeline.name = 'Logistic Scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification models\n",
    "classif_models = [\n",
    "    LGBMClassifier(n_jobs=-1, random_state=5, objective='binary'),\n",
    "    # LogisticRegression(),\n",
    "    logistic_pipeline,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "    Model_compare = pd.DataFrame(columns=['Model Name', \n",
    "                                        'Model Parameters', \n",
    "                                        'Model Train ROC AUC', \n",
    "                                        'Model Test ROC AUC', \n",
    "                                        'Model Test ROC AUC Std', \n",
    "                                        'Model Time'])\n",
    "    \n",
    "    def evaluate_model(alg, idx):\n",
    "        if hasattr(alg, 'name'):\n",
    "            model_name = alg.name\n",
    "        else:\n",
    "            model_name = alg.__class__.__name__\n",
    "        features = important_features.get(model_name, [])\n",
    "\n",
    "        # Check if the list of important features is empty\n",
    "        if len(features) == 0:\n",
    "            # If empty, return results with zero values\n",
    "            print(f'Skipping {model_name} due to no important features.')\n",
    "            return {\n",
    "                'Model Name': model_name,\n",
    "                'Model Parameters': str(alg.get_params()),\n",
    "                'Model Train ROC AUC': 0,\n",
    "                'Model Test ROC AUC': 0,\n",
    "                'Model Test ROC AUC Std': 0,\n",
    "                'Model Time': \"0 min 0.00 sec\",\n",
    "            }\n",
    "        \n",
    "        cv_results = cross_validate(alg, \n",
    "                                    X[features], \n",
    "                                    y, cv=cv_split, \n",
    "                                    scoring='roc_auc', \n",
    "                                    return_train_score=True, \n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Time formatting\n",
    "        mean_fit_time = cv_results['fit_time'].mean()\n",
    "        minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "        # Results population\n",
    "        result = {\n",
    "            'Model Name': model_name,\n",
    "            'Model Parameters': str(alg.get_params()),\n",
    "            'Model Train ROC AUC': cv_results['train_score'].mean(),\n",
    "            'Model Test ROC AUC': cv_results['test_score'].mean(),\n",
    "            'Model Test ROC AUC Std': cv_results['test_score'].std(),\n",
    "            'Model Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "        }\n",
    "\n",
    "        print(f'Done with {model_name}.')\n",
    "        return result\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(tqdm(models, desc='Models'))]\n",
    "        for future in tqdm(futures, total=len(futures), desc='Progress'):\n",
    "            result = future.result()\n",
    "            results_list.append(result)\n",
    "\n",
    "    model_compare = pd.DataFrame(results_list)\n",
    "\n",
    "    model_compare.sort_values(by=['Model Test ROC AUC'], ascending=False, inplace=True)\n",
    "    model_compare.to_csv(f'results\\{experiment_name}.csv', index=False)\n",
    "\n",
    "    return model_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features_classif = {}\n",
    "\n",
    "for model in classif_models:\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    baseline_features_classif[model_name] = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca8940ec2994778a854dfa9742da678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a21ebce2f184fad8d93848f20c9234a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Logistic Scale.\n",
      "Done with LGBMClassifier.\n",
      "CPU times: total: 4.73 s\n",
      "Wall time: 5min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Model Parameters</th>\n",
       "      <th>Model Train ROC AUC</th>\n",
       "      <th>Model Test ROC AUC</th>\n",
       "      <th>Model Test ROC AUC Std</th>\n",
       "      <th>Model Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.875050</td>\n",
       "      <td>0.874902</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>3 min 8.20 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Scale</td>\n",
       "      <td>{'memory': None, 'steps': [('scaler', Standard...</td>\n",
       "      <td>0.841155</td>\n",
       "      <td>0.841152</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>1 min 47.60 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model Name                                   Model Parameters  \\\n",
       "0  LGBMClassifier  {'boosting_type': 'gbdt', 'class_weight': None...   \n",
       "1  Logistic Scale  {'memory': None, 'steps': [('scaler', Standard...   \n",
       "\n",
       "   Model Train ROC AUC  Model Test ROC AUC  Model Test ROC AUC Std  \\\n",
       "0             0.875050            0.874902                0.000185   \n",
       "1             0.841155            0.841152                0.000150   \n",
       "\n",
       "        Model Time  \n",
       "0   3 min 8.20 sec  \n",
       "1  1 min 47.60 sec  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline_models_classif = classif_evaluate_models(classif_models, X, y, baseline_features_classif, sk3, f'{experiment_name}')\n",
    "baseline_models_classif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
