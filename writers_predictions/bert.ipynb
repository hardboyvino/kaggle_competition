{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment_name = 'pytorch_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Incase I need the possible scorers list\n",
    "# import sklearn\n",
    "# sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_features = pd.read_csv('pytorch_bert_train.csv')\n",
    "bert_test_features = pd.read_csv('pytorch_bert_test.csv')\n",
    "\n",
    "comp_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19579, 1024), (8392, 1024), (19579, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_train_features.shape, bert_test_features.shape, comp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  author\n",
       "0  id26305  This process, however, afforded me no means of...       0\n",
       "1  id17569  It never once occurred to me that the fumbling...       1\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...       0\n",
       "3  id27763  How lovely is spring As we looked from Windsor...       2\n",
       "4  id12958  Finding nothing else, not even gold, the Super...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(comp_train[TARGET])\n",
    "\n",
    "comp_train[TARGET] = le.transform(comp_train[TARGET])\n",
    "comp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bert_train_features\n",
    "y = comp_train[TARGET]\n",
    "\n",
    "n_splits = 10\n",
    "sk10 = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LGBMClassifier(n_jobs=-1, random_state=5),\n",
    "    # XGBClassifier(random_state=5),\n",
    "    # RandomForestClassifier(random_state=5),\n",
    "    # AdaBoostClassifier(random_state=5),\n",
    "    # BaggingClassifier(random_state=5),\n",
    "    # ExtraTreesClassifier(random_state=5),\n",
    "    # HistGradientBoostingClassifier(random_state=5),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "    # Create a DataFrame to store comparison results\n",
    "    MLA_compare = pd.DataFrame(columns=['MLA Name', \n",
    "                                        'MLA Parameters', \n",
    "                                        'MLA Train Log Loss', \n",
    "                                        'MLA Test Log Loss', \n",
    "                                        'MLA Time'])\n",
    "    \n",
    "    def evaluate_model(alg, idx):\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        features = important_features.get(MLA_name, [])\n",
    "\n",
    "        # Check if the list of important features is empty\n",
    "        if len(features) == 0:\n",
    "            # If empty, return results with zero values\n",
    "            print(f'Skipping {MLA_name} due to no important features.')\n",
    "            return {\n",
    "                'MLA Name': MLA_name,\n",
    "                'MLA Parameters': str(alg.get_params()),\n",
    "                'MLA Train Log Loss': 0,\n",
    "                'MLA Test Log Loss': 0,\n",
    "                'MLA Time': \"0 min 0.00 sec\",\n",
    "            }\n",
    "\n",
    "        # Perform cross-validation\n",
    "        log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "        \n",
    "        cv_results = cross_validate(alg, X[features], y, \n",
    "                                    cv=cv_split, \n",
    "                                    scoring={'Log Loss': log_loss_scorer}, \n",
    "                                    return_train_score=True, \n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Format time\n",
    "        mean_fit_time = cv_results['fit_time'].mean()\n",
    "        minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "        print(f'Done with {MLA_name}.')\n",
    "        \n",
    "        # Populate results\n",
    "        return {\n",
    "            'MLA Name': MLA_name,\n",
    "            'MLA Parameters': str(alg.get_params()),\n",
    "            'MLA Train Log Loss': -cv_results['train_Log Loss'].mean() if 'train_Log Loss' in cv_results else 0,\n",
    "            'MLA Test Log Loss': -cv_results['test_Log Loss'].mean() if 'test_Log Loss' in cv_results else 0,\n",
    "            'MLA Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "        }\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(models)]\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results_list.append(result)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    MLA_compare = pd.DataFrame(results_list)\n",
    "\n",
    "    # Sort and save results\n",
    "    MLA_compare.sort_values(by=['MLA Test Log Loss'], ascending=True, inplace=True)\n",
    "    MLA_compare.to_csv(f'{experiment_name}_results.csv', index=False)\n",
    "\n",
    "    return MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "#     # Create a DataFrame to store comparison results\n",
    "#     MLA_compare = pd.DataFrame(columns=['MLA Name', \n",
    "#                                         'MLA Parameters', \n",
    "#                                         'MLA Train Log Loss', \n",
    "#                                         'MLA Test Log Loss', \n",
    "#                                         'MLA Time'])\n",
    "    \n",
    "#     def evaluate_model(alg, idx):\n",
    "#         MLA_name = alg.__class__.__name__\n",
    "#         features = important_features.get(MLA_name, [])\n",
    "\n",
    "#         # Check if the list of important features is empty\n",
    "#         if len(features) == 0:\n",
    "#             # If empty, return results with zero values\n",
    "#             print(f'Skipping {MLA_name} due to no important features.')\n",
    "#             return {\n",
    "#                 'MLA Name': MLA_name,\n",
    "#                 'MLA Parameters': str(alg.get_params()),\n",
    "#                 'MLA Train Log Loss': 0,\n",
    "#                 'MLA Test Log Loss': 0,\n",
    "#                 'MLA Time': \"0 min 0.00 sec\",\n",
    "#             }\n",
    "\n",
    "#         cv_results = cross_validate(alg, \n",
    "#                                     X[features], \n",
    "#                                     y, \n",
    "#                                     cv=cv_split, \n",
    "#                                     scoring='neg_log_loss', \n",
    "#                                     return_train_score=True, \n",
    "#                                     n_jobs=-1)\n",
    "\n",
    "#         # Format time\n",
    "#         mean_fit_time = cv_results['fit_time'].mean()\n",
    "#         minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "#         print(f'Done with {MLA_name}.')\n",
    "        \n",
    "#         # Populate results\n",
    "#         return {\n",
    "#             'MLA Name': MLA_name,\n",
    "#             'MLA Parameters': str(alg.get_params()),\n",
    "#             'MLA Train Log Loss': -cv_results['train_Log Loss'].mean() if 'train_Log Loss' in cv_results else 0,\n",
    "#             'MLA Test Log Loss': -cv_results['test_Log Loss'].mean() if 'test_Log Loss' in cv_results else 0,\n",
    "#             'MLA Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "#         }\n",
    "\n",
    "#     results_list = []\n",
    "\n",
    "#     # Use ThreadPoolExecutor for parallel execution\n",
    "#     with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#         futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(models)]\n",
    "#         for future in futures:\n",
    "#             result = future.result()\n",
    "#             if result:\n",
    "#                 results_list.append(result)\n",
    "\n",
    "#     # Create a DataFrame from the list of dictionaries\n",
    "#     MLA_compare = pd.DataFrame(results_list)\n",
    "\n",
    "#     # Sort and save results\n",
    "#     MLA_compare.sort_values(by=['MLA Test Log Loss'], ascending=True, inplace=True)\n",
    "#     MLA_compare.to_csv(f'{experiment_name}_results.csv', index=False)\n",
    "\n",
    "#     return MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = {}\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    baseline_features[model_name] = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LGBMClassifier.\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 4min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train Log Loss</th>\n",
       "      <th>MLA Test Log Loss</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>0.287492</td>\n",
       "      <td>0.601388</td>\n",
       "      <td>2 min 38.26 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MLA Name                                     MLA Parameters  \\\n",
       "0  LGBMClassifier  {'boosting_type': 'gbdt', 'class_weight': None...   \n",
       "\n",
       "   MLA Train Log Loss  MLA Test Log Loss         MLA Time  \n",
       "0            0.287492           0.601388  2 min 38.26 sec  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "baseline_models = evaluate_models(models, X, y, baseline_features, sk10, experiment_name)\n",
    "baseline_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "model2 = XGBClassifier(random_state=5)\n",
    "model3 = RandomForestClassifier(random_state=5)\n",
    "model4 = AdaBoostClassifier(random_state=5)\n",
    "model5 = BaggingClassifier(random_state=5)\n",
    "model6 = ExtraTreesClassifier(random_state=5)\n",
    "model7 = HistGradientBoostingClassifier(random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_final = model1.fit(X, y)\n",
    "# model2_final = model2.fit(X, y)\n",
    "# model3_final = model3.fit(X, y)\n",
    "# model4_final = model4.fit(X, y)\n",
    "# model5_final = model5.fit(X, y)\n",
    "# model6_final = model6.fit(X, y)\n",
    "# model7_final = model7.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model1_final.predict_proba(bert_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.334537</td>\n",
       "      <td>0.154933</td>\n",
       "      <td>0.510530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.629860</td>\n",
       "      <td>0.296828</td>\n",
       "      <td>0.073312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.970313</td>\n",
       "      <td>0.004949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.746332</td>\n",
       "      <td>0.215048</td>\n",
       "      <td>0.038620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.748922</td>\n",
       "      <td>0.131384</td>\n",
       "      <td>0.119693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EAP       HPL       MWS\n",
       "0  0.334537  0.154933  0.510530\n",
       "1  0.629860  0.296828  0.073312\n",
       "2  0.024738  0.970313  0.004949\n",
       "3  0.746332  0.215048  0.038620\n",
       "4  0.748922  0.131384  0.119693"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(prediction)\n",
    "pred_df.columns = le.classes_\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.334537</td>\n",
       "      <td>0.154933</td>\n",
       "      <td>0.510530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.629860</td>\n",
       "      <td>0.296828</td>\n",
       "      <td>0.073312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.970313</td>\n",
       "      <td>0.004949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.746332</td>\n",
       "      <td>0.215048</td>\n",
       "      <td>0.038620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.748922</td>\n",
       "      <td>0.131384</td>\n",
       "      <td>0.119693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.334537  0.154933  0.510530\n",
       "1  id24541  0.629860  0.296828  0.073312\n",
       "2  id00134  0.024738  0.970313  0.004949\n",
       "3  id27757  0.746332  0.215048  0.038620\n",
       "4  id04081  0.748922  0.131384  0.119693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission_df = pd.concat([submission['id'], pred_df], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_lgbm_0.601388.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "model2 = XGBClassifier(random_state=5)\n",
    "model3 = RandomForestClassifier(random_state=5)\n",
    "model4 = AdaBoostClassifier(random_state=5)\n",
    "model5 = BaggingClassifier(random_state=5)\n",
    "model6 = ExtraTreesClassifier(random_state=5)\n",
    "model7 = HistGradientBoostingClassifier(random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_results, model2_results, model3_results, model4_results, model5_results, model6_results, model7_results, y_test_list = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sk10.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model1.fit(X_train, y_train)\n",
    "    model1_results.append(model1.predict_proba(X_test))\n",
    "\n",
    "    # model2.fit(X_train, y_train)\n",
    "    # model2_results.append(model2.predict_proba(X_test))\n",
    "\n",
    "    # model3.fit(X_train, y_train)\n",
    "    # model3_results.append(model3.predict_proba(X_test))\n",
    "\n",
    "    # model4.fit(X_train, y_train)\n",
    "    # model4_results.append(model4.predict_proba(X_test))\n",
    "\n",
    "    # model5.fit(X_train, y_train)\n",
    "    # model5_results.append(model5.predict_proba(X_test))\n",
    "\n",
    "    # model6.fit(X_train, y_train)\n",
    "    # model6_results.append(model6.predict_proba(X_test))\n",
    "\n",
    "    # model7.fit(X_train, y_train)\n",
    "    # model7_results.append(model7.predict_proba(X_test))\n",
    "\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "    print(f'Done with fold {i+1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model1_weights, model2_weights, model3_weights, model4_weights, model5_weights, model6_weights, model7_weights, scores = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in tqdm(range(20000)):\n",
    "    weight_1 = np.random.random_sample(size=1)[0]\n",
    "    weight_2 = np.random.random_sample(size=1)[0]\n",
    "    weight_3 = np.random.random_sample(size=1)[0]\n",
    "    weight_4 = np.random.random_sample(size=1)[0]\n",
    "    weight_5 = np.random.random_sample(size=1)[0]\n",
    "    weight_6 = np.random.random_sample(size=1)[0]\n",
    "    weight_7 = np.random.random_sample(size=1)[0]\n",
    "\n",
    "    model1_weights.append(weight_1)\n",
    "    model2_weights.append(weight_2)\n",
    "    model3_weights.append(weight_3)\n",
    "    model4_weights.append(weight_4)\n",
    "    model5_weights.append(weight_5)\n",
    "    model6_weights.append(weight_6)\n",
    "    model7_weights.append(weight_7)\n",
    "\n",
    "    scores_in = []\n",
    "\n",
    "    for j in range(10):\n",
    "        weighted_pred = weight_1 * model1_results[j] \n",
    "        # + weight_2 * model2_results[j] + weight_3 * model3_results[j] + weight_4 * model4_results[j] + weight_5 * model5_results[j] + weight_6 * model6_results[j] + weight_7 * model7_results[j]\n",
    "        scores_in.append(log_loss(y_test_list[j], weighted_pred))\n",
    "        \n",
    "    scores.append(np.mean(scores_in))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
