{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Adeniyi Babalola\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, TFAutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19579, 3), (8392, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of unique lables in the training data target\n",
    "\n",
    "train_num_labels = train[TARGET].nunique()\n",
    "train_num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set model and tokenizer for finetuning\n",
    "# Ensure num_lables matches the number of labels\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-large-uncased', fast_tokenize=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-large-uncased', num_labels=train_num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Return the number of tokens\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Count the number of tokens in each row and make it its own feature (to be deleted)\n",
    "train['token_length'] = train['title'].apply(count_tokens)\n",
    "test['token_length'] = test['title'].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine the biggest token length in train\n",
    "train.sort_values(by='token_length', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine the biggest token length in test\n",
    "test.sort_values(by='token_length', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['title']\n",
    "        label = self.data.iloc[idx]['vps']\n",
    "        encoding = tokenizer(text, return_tensors='pt', padding='max_length', \n",
    "                            #  truncation=True, # if the max token length is less than 512 this is not needed\n",
    "                             max_length=self.max_len)\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Assume df is your DataFrame containing 'text' and 'label'\n",
    "# Found the max_length token (which was 46) and did max_length + 10 for the padding\n",
    "dataset = CustomDataset(train, tokenizer, max_len=55)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,  # Increased batch size\n",
    "    per_device_eval_batch_size=32,  # Increased batch size\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=3e-4,\n",
    "    seed=5,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the finetuned model for further use (rename as appropraite)\n",
    "# Loss score got low from 7.4 to 0.5\n",
    "\n",
    "model.save_pretrained('./bert-large-uncased_hair_trained_model')\n",
    "tokenizer.save_pretrained('./bert-large-uncased_hair_trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the new finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.7 s\n",
      "Wall time: 8.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use the pretrained model for embedding the training and test data\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased_trained_model')\n",
    "# model = AutoModel.from_pretrained('bert-large-uncased_trained_model')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "model = AutoModel.from_pretrained('google-bert/bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(sentences):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    batch_size = 64  # Adjust based on your memory availability\n",
    "    embeddings = []\n",
    "    \n",
    "    # Wrap the range generator with tqdm for a progress bar\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Processing batches\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, \n",
    "                           truncation=True, \n",
    "                           return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].detach().numpy())\n",
    "    \n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the title column into a list\n",
    "\n",
    "documents_train = train['text'].tolist()\n",
    "documents_test = test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 306/306 [4:51:56<00:00, 57.25s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12h 7min 35s\n",
      "Wall time: 4h 52min 3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_0</th>\n",
       "      <th>bert_1</th>\n",
       "      <th>bert_2</th>\n",
       "      <th>bert_3</th>\n",
       "      <th>bert_4</th>\n",
       "      <th>bert_5</th>\n",
       "      <th>bert_6</th>\n",
       "      <th>bert_7</th>\n",
       "      <th>bert_8</th>\n",
       "      <th>bert_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_1014</th>\n",
       "      <th>bert_1015</th>\n",
       "      <th>bert_1016</th>\n",
       "      <th>bert_1017</th>\n",
       "      <th>bert_1018</th>\n",
       "      <th>bert_1019</th>\n",
       "      <th>bert_1020</th>\n",
       "      <th>bert_1021</th>\n",
       "      <th>bert_1022</th>\n",
       "      <th>bert_1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622036</td>\n",
       "      <td>-0.783141</td>\n",
       "      <td>-0.768789</td>\n",
       "      <td>-0.661886</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.504465</td>\n",
       "      <td>-0.207343</td>\n",
       "      <td>-0.109943</td>\n",
       "      <td>-0.072224</td>\n",
       "      <td>0.794893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029174</td>\n",
       "      <td>-0.245753</td>\n",
       "      <td>-0.150378</td>\n",
       "      <td>0.802312</td>\n",
       "      <td>0.281644</td>\n",
       "      <td>0.201897</td>\n",
       "      <td>0.260581</td>\n",
       "      <td>-1.174859</td>\n",
       "      <td>0.127157</td>\n",
       "      <td>-0.273810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.701897</td>\n",
       "      <td>0.374661</td>\n",
       "      <td>-0.704999</td>\n",
       "      <td>-0.269579</td>\n",
       "      <td>0.633197</td>\n",
       "      <td>0.398946</td>\n",
       "      <td>0.595634</td>\n",
       "      <td>0.220773</td>\n",
       "      <td>0.074318</td>\n",
       "      <td>0.455393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075826</td>\n",
       "      <td>-0.567909</td>\n",
       "      <td>0.174853</td>\n",
       "      <td>-0.206451</td>\n",
       "      <td>-0.001130</td>\n",
       "      <td>0.299368</td>\n",
       "      <td>-0.053149</td>\n",
       "      <td>-0.235630</td>\n",
       "      <td>0.489391</td>\n",
       "      <td>-0.722882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.066654</td>\n",
       "      <td>-0.447025</td>\n",
       "      <td>-0.446701</td>\n",
       "      <td>0.109352</td>\n",
       "      <td>0.140808</td>\n",
       "      <td>0.046788</td>\n",
       "      <td>-0.435363</td>\n",
       "      <td>0.357560</td>\n",
       "      <td>0.830479</td>\n",
       "      <td>0.860996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875746</td>\n",
       "      <td>-0.177480</td>\n",
       "      <td>-0.641226</td>\n",
       "      <td>0.650413</td>\n",
       "      <td>0.383797</td>\n",
       "      <td>0.605456</td>\n",
       "      <td>-0.196379</td>\n",
       "      <td>-0.515951</td>\n",
       "      <td>0.297116</td>\n",
       "      <td>0.061242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.102057</td>\n",
       "      <td>-0.651902</td>\n",
       "      <td>-0.687253</td>\n",
       "      <td>0.218423</td>\n",
       "      <td>0.070657</td>\n",
       "      <td>0.326936</td>\n",
       "      <td>0.120010</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.599281</td>\n",
       "      <td>-0.026284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326366</td>\n",
       "      <td>-0.657595</td>\n",
       "      <td>-0.287512</td>\n",
       "      <td>0.891334</td>\n",
       "      <td>0.251273</td>\n",
       "      <td>0.203012</td>\n",
       "      <td>-0.006166</td>\n",
       "      <td>-0.761718</td>\n",
       "      <td>-0.435652</td>\n",
       "      <td>0.204584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.723254</td>\n",
       "      <td>-0.508487</td>\n",
       "      <td>-0.519215</td>\n",
       "      <td>0.261606</td>\n",
       "      <td>0.076165</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>-0.252534</td>\n",
       "      <td>0.405623</td>\n",
       "      <td>0.653150</td>\n",
       "      <td>0.391357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223673</td>\n",
       "      <td>-0.098319</td>\n",
       "      <td>-0.285708</td>\n",
       "      <td>0.296431</td>\n",
       "      <td>0.532552</td>\n",
       "      <td>0.990138</td>\n",
       "      <td>-0.120088</td>\n",
       "      <td>-1.080273</td>\n",
       "      <td>0.182172</td>\n",
       "      <td>0.078177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_0    bert_1    bert_2    bert_3    bert_4    bert_5    bert_6  \\\n",
       "0 -0.622036 -0.783141 -0.768789 -0.661886  0.028125  0.504465 -0.207343   \n",
       "1 -0.701897  0.374661 -0.704999 -0.269579  0.633197  0.398946  0.595634   \n",
       "2 -0.066654 -0.447025 -0.446701  0.109352  0.140808  0.046788 -0.435363   \n",
       "3 -0.102057 -0.651902 -0.687253  0.218423  0.070657  0.326936  0.120010   \n",
       "4 -0.723254 -0.508487 -0.519215  0.261606  0.076165  0.418882 -0.252534   \n",
       "\n",
       "     bert_7    bert_8    bert_9  ...  bert_1014  bert_1015  bert_1016  \\\n",
       "0 -0.109943 -0.072224  0.794893  ...   0.029174  -0.245753  -0.150378   \n",
       "1  0.220773  0.074318  0.455393  ...   0.075826  -0.567909   0.174853   \n",
       "2  0.357560  0.830479  0.860996  ...   0.875746  -0.177480  -0.641226   \n",
       "3  0.004033  0.599281 -0.026284  ...   0.326366  -0.657595  -0.287512   \n",
       "4  0.405623  0.653150  0.391357  ...   0.223673  -0.098319  -0.285708   \n",
       "\n",
       "   bert_1017  bert_1018  bert_1019  bert_1020  bert_1021  bert_1022  bert_1023  \n",
       "0   0.802312   0.281644   0.201897   0.260581  -1.174859   0.127157  -0.273810  \n",
       "1  -0.206451  -0.001130   0.299368  -0.053149  -0.235630   0.489391  -0.722882  \n",
       "2   0.650413   0.383797   0.605456  -0.196379  -0.515951   0.297116   0.061242  \n",
       "3   0.891334   0.251273   0.203012  -0.006166  -0.761718  -0.435652   0.204584  \n",
       "4   0.296431   0.532552   0.990138  -0.120088  -1.080273   0.182172   0.078177  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_embeddings = get_bert_embeddings(documents_train)\n",
    "bert_df_train = pd.DataFrame(bert_embeddings)\n",
    "bert_df_train.columns = ['bert_' + str(col) for col in bert_df_train.columns]\n",
    "bert_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df_train.to_csv('bert_large_uncased_baseline_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 132/132 [1:58:28<00:00, 53.85s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5h 25min 29s\n",
      "Wall time: 1h 58min 28s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_0</th>\n",
       "      <th>bert_1</th>\n",
       "      <th>bert_2</th>\n",
       "      <th>bert_3</th>\n",
       "      <th>bert_4</th>\n",
       "      <th>bert_5</th>\n",
       "      <th>bert_6</th>\n",
       "      <th>bert_7</th>\n",
       "      <th>bert_8</th>\n",
       "      <th>bert_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_1014</th>\n",
       "      <th>bert_1015</th>\n",
       "      <th>bert_1016</th>\n",
       "      <th>bert_1017</th>\n",
       "      <th>bert_1018</th>\n",
       "      <th>bert_1019</th>\n",
       "      <th>bert_1020</th>\n",
       "      <th>bert_1021</th>\n",
       "      <th>bert_1022</th>\n",
       "      <th>bert_1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.237395</td>\n",
       "      <td>-1.099664</td>\n",
       "      <td>-0.073115</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>0.217628</td>\n",
       "      <td>0.800265</td>\n",
       "      <td>-0.337267</td>\n",
       "      <td>-0.240212</td>\n",
       "      <td>-0.078137</td>\n",
       "      <td>0.266030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324019</td>\n",
       "      <td>-0.386438</td>\n",
       "      <td>-0.742862</td>\n",
       "      <td>1.121769</td>\n",
       "      <td>0.309127</td>\n",
       "      <td>0.823491</td>\n",
       "      <td>0.390766</td>\n",
       "      <td>-0.884747</td>\n",
       "      <td>-0.377234</td>\n",
       "      <td>-0.214326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.966147</td>\n",
       "      <td>-0.420513</td>\n",
       "      <td>-0.857052</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>0.568425</td>\n",
       "      <td>0.946378</td>\n",
       "      <td>-0.172185</td>\n",
       "      <td>-0.506385</td>\n",
       "      <td>0.672026</td>\n",
       "      <td>0.417897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685170</td>\n",
       "      <td>-0.477663</td>\n",
       "      <td>-0.984418</td>\n",
       "      <td>1.594175</td>\n",
       "      <td>-0.076365</td>\n",
       "      <td>1.213480</td>\n",
       "      <td>0.119592</td>\n",
       "      <td>-1.103788</td>\n",
       "      <td>-0.447266</td>\n",
       "      <td>0.409644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.281656</td>\n",
       "      <td>-0.408963</td>\n",
       "      <td>-0.722742</td>\n",
       "      <td>-0.256569</td>\n",
       "      <td>0.387469</td>\n",
       "      <td>0.430409</td>\n",
       "      <td>-0.060564</td>\n",
       "      <td>0.197533</td>\n",
       "      <td>0.254076</td>\n",
       "      <td>0.491151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287451</td>\n",
       "      <td>-0.456337</td>\n",
       "      <td>-1.118185</td>\n",
       "      <td>0.344271</td>\n",
       "      <td>-0.044690</td>\n",
       "      <td>0.719634</td>\n",
       "      <td>-0.304294</td>\n",
       "      <td>-1.084771</td>\n",
       "      <td>-0.141309</td>\n",
       "      <td>-0.464817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.034117</td>\n",
       "      <td>-0.712914</td>\n",
       "      <td>-0.519002</td>\n",
       "      <td>0.088926</td>\n",
       "      <td>0.187793</td>\n",
       "      <td>0.306727</td>\n",
       "      <td>0.189207</td>\n",
       "      <td>0.238950</td>\n",
       "      <td>0.309982</td>\n",
       "      <td>0.563780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023420</td>\n",
       "      <td>0.405448</td>\n",
       "      <td>-0.544392</td>\n",
       "      <td>1.045816</td>\n",
       "      <td>0.059901</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>-0.311360</td>\n",
       "      <td>-0.188767</td>\n",
       "      <td>0.372833</td>\n",
       "      <td>-0.634840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.446976</td>\n",
       "      <td>-0.504851</td>\n",
       "      <td>-0.614594</td>\n",
       "      <td>-0.866996</td>\n",
       "      <td>-0.157824</td>\n",
       "      <td>0.780376</td>\n",
       "      <td>0.328414</td>\n",
       "      <td>0.241635</td>\n",
       "      <td>0.217270</td>\n",
       "      <td>0.800562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184186</td>\n",
       "      <td>-0.050992</td>\n",
       "      <td>-0.419704</td>\n",
       "      <td>0.295030</td>\n",
       "      <td>0.179823</td>\n",
       "      <td>0.512639</td>\n",
       "      <td>-0.280976</td>\n",
       "      <td>-0.473329</td>\n",
       "      <td>-0.146180</td>\n",
       "      <td>-0.094580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_0    bert_1    bert_2    bert_3    bert_4    bert_5    bert_6  \\\n",
       "0 -0.237395 -1.099664 -0.073115  0.011293  0.217628  0.800265 -0.337267   \n",
       "1 -0.966147 -0.420513 -0.857052  0.046974  0.568425  0.946378 -0.172185   \n",
       "2 -0.281656 -0.408963 -0.722742 -0.256569  0.387469  0.430409 -0.060564   \n",
       "3 -1.034117 -0.712914 -0.519002  0.088926  0.187793  0.306727  0.189207   \n",
       "4 -0.446976 -0.504851 -0.614594 -0.866996 -0.157824  0.780376  0.328414   \n",
       "\n",
       "     bert_7    bert_8    bert_9  ...  bert_1014  bert_1015  bert_1016  \\\n",
       "0 -0.240212 -0.078137  0.266030  ...   0.324019  -0.386438  -0.742862   \n",
       "1 -0.506385  0.672026  0.417897  ...   0.685170  -0.477663  -0.984418   \n",
       "2  0.197533  0.254076  0.491151  ...   0.287451  -0.456337  -1.118185   \n",
       "3  0.238950  0.309982  0.563780  ...  -0.023420   0.405448  -0.544392   \n",
       "4  0.241635  0.217270  0.800562  ...   0.184186  -0.050992  -0.419704   \n",
       "\n",
       "   bert_1017  bert_1018  bert_1019  bert_1020  bert_1021  bert_1022  bert_1023  \n",
       "0   1.121769   0.309127   0.823491   0.390766  -0.884747  -0.377234  -0.214326  \n",
       "1   1.594175  -0.076365   1.213480   0.119592  -1.103788  -0.447266   0.409644  \n",
       "2   0.344271  -0.044690   0.719634  -0.304294  -1.084771  -0.141309  -0.464817  \n",
       "3   1.045816   0.059901   0.686347  -0.311360  -0.188767   0.372833  -0.634840  \n",
       "4   0.295030   0.179823   0.512639  -0.280976  -0.473329  -0.146180  -0.094580  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Vectorize the test data\n",
    "\n",
    "bert_embeddings = get_bert_embeddings(documents_test)\n",
    "bert_df_test = pd.DataFrame(bert_embeddings)\n",
    "bert_df_test.columns = ['bert_' + str(col) for col in bert_df_test.columns]\n",
    "bert_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df_test.to_csv('bert_large_uncased_baseline_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
