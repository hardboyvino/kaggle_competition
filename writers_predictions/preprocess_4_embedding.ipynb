{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Adeniyi Babalola\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (19579, 3)\n",
      "Test shape: (8392, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "print(f'Train shape: {df.shape}'), print(f'Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3916\n",
      "15663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=5)\n",
    "\n",
    "# Get the indices for the validation set\n",
    "for _, test_indx in sss.split(df, df['author']):\n",
    "    valid_df = df.iloc[test_indx]\n",
    "    train_df = df.drop(test_indx)\n",
    "\n",
    "print(len(valid_df)), print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose=True):\n",
    "    \"\"\"\n",
    "    sentences: list of list of words\n",
    "    return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable=(not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's populate the dictionary and display the first 5 elements and their count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [00:00<00:00, 176288.08it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 142013.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 391, 'process,': 5, 'however,': 290, 'afforded': 34, 'me': 2015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = df['text'].progress_apply(lambda x: x.split()).values\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "print({key: vocab[key] for key in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "vocab_path = 'bert_large_uncased_vocab.txt'  # Path to the vocab.txt file\n",
    "with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "    vocab_list = f.read().splitlines()\n",
    "\n",
    "# Convert the vocabulary list to indices\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, word_to_idx):\n",
    "    in_vocab = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            in_vocab[word] = word_to_idx[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print(f'Found embeddings for {len(in_vocab) / len(vocab):.2%} of words in the vocab')\n",
    "    print(f'Found embeddings for {k / (k + i):.2%}% of all text')\n",
    "\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47556/47556 [00:00<00:00, 952542.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 21.89% of words in the vocab\n",
      "Found embeddings for 75.57%% of all text\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 62.3 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "oov = check_coverage(vocab, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 10382),\n",
       " ('The', 2121),\n",
       " ('It', 880),\n",
       " ('He', 863),\n",
       " ('But', 623),\n",
       " ('In', 592),\n",
       " ('me,', 470),\n",
       " ('And', 447),\n",
       " ('This', 391),\n",
       " ('We', 372)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looks like capitalization and punctuations attached to words are the main reasons words seen as out of vocab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The punctuation & is confirmed True\n",
      "The punctuation ? is confirmed True\n",
      "The punctuation ! is confirmed True\n",
      "The punctuation . is confirmed True\n",
      "The punctuation , is confirmed True\n",
      "The punctuation \" is confirmed True\n",
      "The punctuation # is confirmed True\n",
      "The punctuation $ is confirmed True\n",
      "The punctuation % is confirmed True\n",
      "The punctuation ' is confirmed True\n",
      "The punctuation ( is confirmed True\n",
      "The punctuation ) is confirmed True\n",
      "The punctuation * is confirmed True\n",
      "The punctuation + is confirmed True\n",
      "The punctuation - is confirmed True\n",
      "The punctuation / is confirmed True\n",
      "The punctuation : is confirmed True\n",
      "The punctuation ; is confirmed True\n",
      "The punctuation < is confirmed True\n",
      "The punctuation = is confirmed True\n",
      "The punctuation > is confirmed True\n",
      "The punctuation @ is confirmed True\n",
      "The punctuation [ is confirmed True\n",
      "The punctuation \\ is confirmed True\n",
      "The punctuation ] is confirmed True\n",
      "The punctuation ^ is confirmed True\n",
      "The punctuation _ is confirmed True\n",
      "The punctuation ` is confirmed True\n",
      "The punctuation { is confirmed True\n",
      "The punctuation | is confirmed True\n",
      "The punctuation } is confirmed True\n",
      "The punctuation ~ is confirmed True\n",
      "The punctuation ] is confirmed True\n"
     ]
    }
   ],
   "source": [
    "for char in '&?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~]':\n",
    "    confirmation = char in word_to_idx\n",
    "    print(f'The punctuation {char} is confirmed {confirmation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Every punctuation is in bert-large and since the model reads meaning, it means there might some meaning in the context the punctuation appears so all I need to do is separate the punctuation from the word it is attached to so it becomes its own token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_punct(text):\n",
    "    text = str(text)\n",
    "    for punct in '&?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~]' + \"'“”’\": \n",
    "        text = text.replace(punct, f' {punct}')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [00:00<00:00, 102523.09it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 75562.72it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 177853.07it/s]\n",
      "100%|██████████| 28398/28398 [00:00<00:00, 943115.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 38.69% of words in the vocab\n",
      "Found embeddings for 87.01%% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x: clean_text_punct(x))\n",
    "\n",
    "sentences = df['text'].progress_apply(lambda x: x.split()).values\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "\n",
    "oov = check_coverage(vocab, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*38.71% of the words now, improvement of 16.82% which accounts for 86.94% of the total text. Not bad!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 10584),\n",
       " ('The', 2121),\n",
       " (\"'s\", 1261),\n",
       " ('It', 880),\n",
       " ('He', 876),\n",
       " ('But', 676),\n",
       " ('In', 592),\n",
       " ('And', 469),\n",
       " ('This', 413),\n",
       " ('We', 382)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Number of words improved but captilization seems to be the next big issue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [00:00<00:00, 248868.94it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 201167.21it/s]\n",
      "100%|██████████| 26008/26008 [00:00<00:00, 1298605.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 45.63% of words in the vocab\n",
      "Found embeddings for 93.07%% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "sentences = df['text'].progress_apply(lambda x: x.split()).values\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "\n",
    "oov = check_coverage(vocab, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now 45.63% from 38.71%, improved by 6.92% which is 93.01% of all the text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 1261),\n",
       " ('\"i', 213),\n",
       " ('perdita', 167),\n",
       " ('\"the', 131),\n",
       " ('countenance', 128),\n",
       " ('idris', 109),\n",
       " ('\"you', 104),\n",
       " ('beheld', 88),\n",
       " ('\"and', 86),\n",
       " ('gilman', 76)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I have noticed that quotes are messing punctuation cleaning so I need to deal with those specifically by removing them entirely*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_quotes(text):\n",
    "    text = str(text)\n",
    "    for punct in '\"': \n",
    "        text = text.replace(punct, f'{punct} ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [00:00<00:00, 549641.44it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 229121.29it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 148500.40it/s]\n",
      "100%|██████████| 25344/25344 [00:00<00:00, 967070.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 46.90% of words in the vocab\n",
      "Found embeddings for 93.51%% of all text\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x: clean_text_quotes(x))\n",
    "\n",
    "sentences = df['text'].progress_apply(lambda x: x.split()).values\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "\n",
    "oov = check_coverage(vocab, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*That added another 1.27% to the number of words in the vocab and an additional 0.35% to all the text that have embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 1261),\n",
       " ('perdita', 169),\n",
       " ('countenance', 128),\n",
       " ('idris', 109),\n",
       " ('beheld', 88),\n",
       " ('gilman', 76),\n",
       " ('alas', 75),\n",
       " (\"'t\", 63),\n",
       " ('exceedingly', 62),\n",
       " ('frightful', 61)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now to deal with contractions i.e. turning words like didn't to did not*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\n",
    "    \"'cause\": 'because',\n",
    "    ',cause': 'because',\n",
    "    ';cause': 'because',\n",
    "    \"ain't\": 'am not',\n",
    "    'ain,t': 'am not',\n",
    "    'ain;t': 'am not',\n",
    "    'ain´t': 'am not',\n",
    "    'ain’t': 'am not',\n",
    "    \"aren't\": 'are not',\n",
    "    'aren,t': 'are not',\n",
    "    'aren;t': 'are not',\n",
    "    'aren´t': 'are not',\n",
    "    'aren’t': 'are not',\n",
    "    \"can't\": 'cannot',\n",
    "    \"can't've\": 'cannot have',\n",
    "    'can,t': 'cannot',\n",
    "    'can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot',\n",
    "    'can;t;ve': 'cannot have',\n",
    "    'can´t': 'cannot',\n",
    "    'can´t´ve': 'cannot have',\n",
    "    'can’t': 'cannot',\n",
    "    'can’t’ve': 'cannot have',\n",
    "    \"could've\": 'could have',\n",
    "    'could,ve': 'could have',\n",
    "    'could;ve': 'could have',\n",
    "    \"couldn't\": 'could not',\n",
    "    \"couldn't've\": 'could not have',\n",
    "    'couldn,t': 'could not',\n",
    "    'couldn,t,ve': 'could not have',\n",
    "    'couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have',\n",
    "    'couldn´t': 'could not',\n",
    "    'couldn´t´ve': 'could not have',\n",
    "    'couldn’t': 'could not',\n",
    "    'couldn’t’ve': 'could not have',\n",
    "    'could´ve': 'could have',\n",
    "    'could’ve': 'could have',\n",
    "    \"didn't\": 'did not',\n",
    "    'didn,t': 'did not',\n",
    "    'didn;t': 'did not',\n",
    "    'didn´t': 'did not',\n",
    "    'didn’t': 'did not',\n",
    "    \"doesn't\": 'does not',\n",
    "    'doesn,t': 'does not',\n",
    "    'doesn;t': 'does not',\n",
    "    'doesn´t': 'does not',\n",
    "    'doesn’t': 'does not',\n",
    "    \"don't\": 'do not',\n",
    "    'don,t': 'do not',\n",
    "    'don;t': 'do not',\n",
    "    'don´t': 'do not',\n",
    "    'don’t': 'do not',\n",
    "    \"dun't\": 'do not',\n",
    "    \"hadn't\": 'had not',\n",
    "    \"hadn't've\": 'had not have',\n",
    "    'hadn,t': 'had not',\n",
    "    'hadn,t,ve': 'had not have',\n",
    "    'hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have',\n",
    "    'hadn´t': 'had not',\n",
    "    'hadn´t´ve': 'had not have',\n",
    "    'hadn’t': 'had not',\n",
    "    'hadn’t’ve': 'had not have',\n",
    "    \"hasn't\": 'has not',\n",
    "    'hasn,t': 'has not',\n",
    "    'hasn;t': 'has not',\n",
    "    'hasn´t': 'has not',\n",
    "    'hasn’t': 'has not',\n",
    "    \"haven't\": 'have not',\n",
    "    'haven,t': 'have not',\n",
    "    'haven;t': 'have not',\n",
    "    'haven´t': 'have not',\n",
    "    'haven’t': 'have not',\n",
    "    \"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\n",
    "    \"he'll\": 'he will',\n",
    "    \"he's\": 'he is',\n",
    "    'he,d': 'he would',\n",
    "    'he,d,ve': 'he would have',\n",
    "    'he,ll': 'he will',\n",
    "    'he,s': 'he is',\n",
    "    'he;d': 'he would',\n",
    "    'he;d;ve': 'he would have',\n",
    "    'he;ll': 'he will',\n",
    "    'he;s': 'he is',\n",
    "    'he´d': 'he would',\n",
    "    'he´d´ve': 'he would have',\n",
    "    'he´ll': 'he will',\n",
    "    'he´s': 'he is',\n",
    "    'he’d': 'he would',\n",
    "    'he’d’ve': 'he would have',\n",
    "    'he’ll': 'he will',\n",
    "    'he’s': 'he is',\n",
    "    \"how'd\": 'how did',\n",
    "    \"how'll\": 'how will',\n",
    "    \"how's\": 'how is',\n",
    "    'how,d': 'how did',\n",
    "    'how,ll': 'how will',\n",
    "    'how,s': 'how is',\n",
    "    'how;d': 'how did',\n",
    "    'how;ll': 'how will',\n",
    "    'how;s': 'how is',\n",
    "    'how´d': 'how did',\n",
    "    'how´ll': 'how will',\n",
    "    'how´s': 'how is',\n",
    "    'how’d': 'how did',\n",
    "    'how’ll': 'how will',\n",
    "    'how’s': 'how is',\n",
    "    \"i'd\": 'i would',\n",
    "    \"i'll\": 'i will',\n",
    "    \"i'm\": 'i am',\n",
    "    \"i've\": 'i have',\n",
    "    'i,d': 'i would',\n",
    "    'i,ll': 'i will',\n",
    "    'i,m': 'i am',\n",
    "    'i,ve': 'i have',\n",
    "    'i;d': 'i would',\n",
    "    'i;ll': 'i will',\n",
    "    'i;m': 'i am',\n",
    "    'i;ve': 'i have',\n",
    "    \"isn't\": 'is not',\n",
    "    'isn,t': 'is not',\n",
    "    'isn;t': 'is not',\n",
    "    'isn´t': 'is not',\n",
    "    'isn’t': 'is not',\n",
    "    \"it'd\": 'it would',\n",
    "    \"it'll\": 'it will',\n",
    "    \"It's\":'it is',\n",
    "    \"it's\": 'it is',\n",
    "    'it,d': 'it would',\n",
    "    'it,ll': 'it will',\n",
    "    'it,s': 'it is',\n",
    "    'it;d': 'it would',\n",
    "    'it;ll': 'it will',\n",
    "    'it;s': 'it is',\n",
    "    'it´d': 'it would',\n",
    "    'it´ll': 'it will',\n",
    "    'it´s': 'it is',\n",
    "    'it’d': 'it would',\n",
    "    'it’ll': 'it will',\n",
    "    'it’s': 'it is',\n",
    "    'i´d': 'i would',\n",
    "    'i´ll': 'i will',\n",
    "    'i´m': 'i am',\n",
    "    'i´ve': 'i have',\n",
    "    'i’d': 'i would',\n",
    "    'i’ll': 'i will',\n",
    "    'i’m': 'i am',\n",
    "    'i’ve': 'i have',\n",
    "    \"let's\": 'let us',\n",
    "    'let,s': 'let us',\n",
    "    'let;s': 'let us',\n",
    "    'let´s': 'let us',\n",
    "    'let’s': 'let us',\n",
    "    \"ma'am\": 'madam',\n",
    "    'ma,am': 'madam',\n",
    "    'ma;am': 'madam',\n",
    "    \"mayn't\": 'may not',\n",
    "    'mayn,t': 'may not',\n",
    "    'mayn;t': 'may not',\n",
    "    'mayn´t': 'may not',\n",
    "    'mayn’t': 'may not',\n",
    "    'ma´am': 'madam',\n",
    "    'ma’am': 'madam',\n",
    "    \"might've\": 'might have',\n",
    "    'might,ve': 'might have',\n",
    "    'might;ve': 'might have',\n",
    "    \"mightn't\": 'might not',\n",
    "    'mightn,t': 'might not',\n",
    "    'mightn;t': 'might not',\n",
    "    'mightn´t': 'might not',\n",
    "    'mightn’t': 'might not',\n",
    "    'might´ve': 'might have',\n",
    "    'might’ve': 'might have',\n",
    "    \"must've\": 'must have',\n",
    "    'must,ve': 'must have',\n",
    "    'must;ve': 'must have',\n",
    "    \"mustn't\": 'must not',\n",
    "    'mustn,t': 'must not',\n",
    "    'mustn;t': 'must not',\n",
    "    'mustn´t': 'must not',\n",
    "    'mustn’t': 'must not',\n",
    "    'must´ve': 'must have',\n",
    "    'must’ve': 'must have',\n",
    "    \"needn't\": 'need not',\n",
    "    'needn,t': 'need not',\n",
    "    'needn;t': 'need not',\n",
    "    'needn´t': 'need not',\n",
    "    'needn’t': 'need not',\n",
    "    \"n't\": 'not',\n",
    "    \"oughtn't\": 'ought not',\n",
    "    'oughtn,t': 'ought not',\n",
    "    'oughtn;t': 'ought not',\n",
    "    'oughtn´t': 'ought not',\n",
    "    'oughtn’t': 'ought not',\n",
    "    \"sha'n't\": 'shall not',\n",
    "    'sha,n,t': 'shall not',\n",
    "    'sha;n;t': 'shall not',\n",
    "    \"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not',\n",
    "    'shan;t': 'shall not',\n",
    "    'shan´t': 'shall not',\n",
    "    'shan’t': 'shall not',\n",
    "    'sha´n´t': 'shall not',\n",
    "    'sha’n’t': 'shall not',\n",
    "    \"she'd\": 'she would',\n",
    "    \"she'll\": 'she will',\n",
    "    \"she's\": 'she is',\n",
    "    'she,d': 'she would',\n",
    "    'she,ll': 'she will',\n",
    "    'she,s': 'she is',\n",
    "    'she;d': 'she would',\n",
    "    'she;ll': 'she will',\n",
    "    'she;s': 'she is',\n",
    "    'she´d': 'she would',\n",
    "    'she´ll': 'she will',\n",
    "    'she´s': 'she is',\n",
    "    'she’d': 'she would',\n",
    "    'she’ll': 'she will',\n",
    "    'she’s': 'she is',\n",
    "    \"should've\": 'should have',\n",
    "    'should,ve': 'should have',\n",
    "    'should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not',\n",
    "    'shouldn,t': 'should not',\n",
    "    'shouldn;t': 'should not',\n",
    "    'shouldn´t': 'should not',\n",
    "    'shouldn’t': 'should not',\n",
    "    'should´ve': 'should have',\n",
    "    'should’ve': 'should have',\n",
    "    \"that'd\": 'that would',\n",
    "    \"that's\": 'that is',\n",
    "    'that,d': 'that would',\n",
    "    'that,s': 'that is',\n",
    "    'that;d': 'that would',\n",
    "    'that;s': 'that is',\n",
    "    'that´d': 'that would',\n",
    "    'that´s': 'that is',\n",
    "    'that’d': 'that would',\n",
    "    'that’s': 'that is',\n",
    "    \"there'd\": 'there had',\n",
    "    \"there's\": 'there is',\n",
    "    'there,d': 'there had',\n",
    "    'there,s': 'there is',\n",
    "    'there;d': 'there had',\n",
    "    'there;s': 'there is',\n",
    "    'there´d': 'there had',\n",
    "    'there´s': 'there is',\n",
    "    'there’d': 'there had',\n",
    "    'there’s': 'there is',\n",
    "    \"they'd\": 'they would',\n",
    "    \"they'll\": 'they will',\n",
    "    \"they're\": 'they are',\n",
    "    \"they've\": 'they have',\n",
    "    'they,d': 'they would',\n",
    "    'they,ll': 'they will',\n",
    "    'they,re': 'they are',\n",
    "    'they,ve': 'they have',\n",
    "    'they;d': 'they would',\n",
    "    'they;ll': 'they will',\n",
    "    'they;re': 'they are',\n",
    "    'they;ve': 'they have',\n",
    "    'they´d': 'they would',\n",
    "    'they´ll': 'they will',\n",
    "    'they´re': 'they are',\n",
    "    'they´ve': 'they have',\n",
    "    'they’d': 'they would',\n",
    "    'they’ll': 'they will',\n",
    "    'they’re': 'they are',\n",
    "    'they’ve': 'they have',\n",
    "    \"wasn't\": 'was not',\n",
    "    'wasn,t': 'was not',\n",
    "    'wasn;t': 'was not',\n",
    "    'wasn´t': 'was not',\n",
    "    'wasn’t': 'was not',\n",
    "    \"we'd\": 'we would',\n",
    "    \"we'll\": 'we will',\n",
    "    \"we're\": 'we are',\n",
    "    \"we've\": 'we have',\n",
    "    'we,d': 'we would',\n",
    "    'we,ll': 'we will',\n",
    "    'we,re': 'we are',\n",
    "    'we,ve': 'we have',\n",
    "    'we;d': 'we would',\n",
    "    'we;ll': 'we will',\n",
    "    'we;re': 'we are',\n",
    "    'we;ve': 'we have',\n",
    "    \"weren't\": 'were not',\n",
    "    'weren,t': 'were not',\n",
    "    'weren;t': 'were not',\n",
    "    'weren´t': 'were not',\n",
    "    'weren’t': 'were not',\n",
    "    'we´d': 'we would',\n",
    "    'we´ll': 'we will',\n",
    "    'we´re': 'we are',\n",
    "    'we´ve': 'we have',\n",
    "    'we’d': 'we would',\n",
    "    'we’ll': 'we will',\n",
    "    'we’re': 'we are',\n",
    "    'we’ve': 'we have',\n",
    "    \"what'll\": 'what will',\n",
    "    \"what're\": 'what are',\n",
    "    \"what's\": 'what is',\n",
    "    \"what've\": 'what have',\n",
    "    'what,ll': 'what will',\n",
    "    'what,re': 'what are',\n",
    "    'what,s': 'what is',\n",
    "    'what,ve': 'what have',\n",
    "    'what;ll': 'what will',\n",
    "    'what;re': 'what are',\n",
    "    'what;s': 'what is',\n",
    "    'what;ve': 'what have',\n",
    "    'what´ll': 'what will',\n",
    "    'what´re': 'what are',\n",
    "    'what´s': 'what is',\n",
    "    'what´ve': 'what have',\n",
    "    'what’ll': 'what will',\n",
    "    'what’re': 'what are',\n",
    "    'what’s': 'what is',\n",
    "    'what’ve': 'what have',\n",
    "    \"where'd\": 'where did',\n",
    "    \"where's\": 'where is',\n",
    "    'where,d': 'where did',\n",
    "    'where,s': 'where is',\n",
    "    'where;d': 'where did',\n",
    "    'where;s': 'where is',\n",
    "    'where´d': 'where did',\n",
    "    'where´s': 'where is',\n",
    "    'where’d': 'where did',\n",
    "    'where’s': 'where is',\n",
    "    \"who'll\": 'who will',\n",
    "    \"who's\": 'who is',\n",
    "    'who,ll': 'who will',\n",
    "    'who,s': 'who is',\n",
    "    'who;ll': 'who will',\n",
    "    'who;s': 'who is',\n",
    "    'who´ll': 'who will',\n",
    "    'who´s': 'who is',\n",
    "    'who’ll': 'who will',\n",
    "    'who’s': 'who is',\n",
    "    \"won't\": 'will not',\n",
    "    'won,t': 'will not',\n",
    "    'won;t': 'will not',\n",
    "    'won´t': 'will not',\n",
    "    'won’t': 'will not',\n",
    "    \"wouldn't\": 'would not',\n",
    "    'wouldn,t': 'would not',\n",
    "    'wouldn;t': 'would not',\n",
    "    'wouldn´t': 'would not',\n",
    "\n",
    "    'wouldn’t': 'would not',\n",
    "    \"you'd\": 'you would',\n",
    "    \"you'll\": 'you will',\n",
    "    \"you're\": 'you are',\n",
    "    'you,d': 'you would',\n",
    "    'you,ll': 'you will',\n",
    "    'you,re': 'you are',\n",
    "    'you;d': 'you would',\n",
    "    'you;ll': 'you will',\n",
    "    'you;re': 'you are',\n",
    "    'you´d': 'you would',\n",
    "    'you´ll': 'you will',\n",
    "    'you´re': 'you are',\n",
    "    'you’d': 'you would',\n",
    "    'you’ll': 'you will',\n",
    "    'you’re': 'you are',\n",
    "    '´cause': 'because',\n",
    "    '’cause': 'because',\n",
    "    \"you've\": \"you have\",\n",
    "    \"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\n",
    "    \"here’s\": \"here is\",\n",
    "    'i\"\"m': 'i am',\n",
    "    \"i'am\": 'i am',\n",
    "    \"i'l\": \"i will\",\n",
    "    \"i'v\": 'i have',\n",
    "    \"wan't\": 'want',\n",
    "    \"was'nt\": \"was not\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'know\": \"you know\",\n",
    "    # \"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\n",
    "    \"arn't\": \"are not\",\n",
    "    \"agains't\": \"against\",\n",
    "    \"c'mon\": \"common\",\n",
    "    \"doens't\": \"does not\",\n",
    "    'don\"\"t': \"do not\",\n",
    "    \"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\n",
    "    \"shoudn't\": \"should not\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\n",
    "    \"u're\": \"you are\",\n",
    "     \"ya'll\": \"you all\",\n",
    "    \"you'r\": \"you are\",\n",
    "    \"you’ve\": \"you have\",\n",
    "    \"d'int\": \"did not\",\n",
    "    \"did'nt\": \"did not\",\n",
    "    \"din't\": \"did not\",\n",
    "    \"dont't\": \"do not\",\n",
    "    \"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\n",
    "    \"is'nt\": \"is not\",\n",
    "    \"‘I\":'I',\n",
    "    'ᴀɴᴅ':'and',\n",
    "    'ᴛʜᴇ':'the',\n",
    "    'ʜᴏᴍᴇ':'home',\n",
    "    'ᴜᴘ':'up',\n",
    "    'ʙʏ':'by',\n",
    "    'ᴀᴛ':'at',\n",
    "    '…and':'and',\n",
    "    'civilbeat':'civil beat',\n",
    "    'TrumpCare':'Trump care',\n",
    "    'Trumpcare':'Trump care',\n",
    "     'OBAMAcare':'Obama care',\n",
    "    'ᴄʜᴇᴄᴋ':'check',\n",
    "    'ғᴏʀ':'for',\n",
    "    'ᴛʜɪs':'this',\n",
    "    'ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\n",
    "    'ᴍᴏɴᴛʜ':'month',\n",
    "    'ᴡᴏʀᴋɪɴɢ':'working',\n",
    "    'ᴊᴏʙ':'job',\n",
    "    'ғʀᴏᴍ':'from',\n",
    "    'Sᴛᴀʀᴛ':'start',\n",
    "    'gubmit':'submit',\n",
    "    'CO₂':'carbon dioxide',\n",
    "    'ғɪʀsᴛ':'first',\n",
    "    'ᴇɴᴅ':'end',\n",
    "    'ᴄᴀɴ':'can',\n",
    "    'ʜᴀᴠᴇ':'have',\n",
    "    'ᴛᴏ':'to',\n",
    "    'ʟɪɴᴋ':'link',\n",
    "    'ᴏғ':'of',\n",
    "    'ʜᴏᴜʀʟʏ':'hourly',\n",
    "    'ᴡᴇᴇᴋ':'week',\n",
    "    'ᴇɴᴅ':'end',\n",
    "    'ᴇxᴛʀᴀ':'extra',\n",
    "    'Gʀᴇᴀᴛ':'great',\n",
    "    'sᴛᴜᴅᴇɴᴛs':'student',\n",
    "    'sᴛᴀʏ':'stay',\n",
    "    'ᴍᴏᴍs':'mother',\n",
    "    'ᴏʀ':'or',\n",
    "    'ᴀɴʏᴏɴᴇ':'anyone',\n",
    "    'ɴᴇᴇᴅɪɴɢ':'needing',\n",
    "    'ᴀɴ':'an',\n",
    "    'ɪɴᴄᴏᴍᴇ':'income',\n",
    "    'ʀᴇʟɪᴀʙʟᴇ':'reliable',\n",
    "    'ғɪʀsᴛ':'first',\n",
    "    'ʏᴏᴜʀ':'your',\n",
    "    'sɪɢɴɪɴɢ':'signing',\n",
    "    'ʙᴏᴛᴛᴏᴍ':'bottom',\n",
    "    'ғᴏʟʟᴏᴡɪɴɢ':'following',\n",
    "    'Mᴀᴋᴇ':'make',\n",
    "    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection',\n",
    "    'ɪɴᴛᴇʀɴᴇᴛ':'internet',\n",
    "    'financialpost':'financial post',\n",
    "     'ʜaᴠᴇ':' have ',\n",
    "     'ᴄaɴ':' can ',\n",
    "     'Maᴋᴇ':' make ',\n",
    "     'ʀᴇʟɪaʙʟᴇ':' reliable ',\n",
    "     'ɴᴇᴇᴅ':' need ',\n",
    "    'ᴏɴʟʏ':' only ',\n",
    "     'ᴇxᴛʀa':' extra ',\n",
    "     'aɴ':' an ',\n",
    "     'aɴʏᴏɴᴇ':' anyone ',\n",
    "     'sᴛaʏ':' stay ',\n",
    "     'Sᴛaʀᴛ':' start',\n",
    "     'SHOPO':'shop',\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"couldnt\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"doesnt\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"havent\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"I would\",\n",
    "    \"i'd\" : \"I had\",\n",
    "    \"i'll\" : \"I will\",\n",
    "    \"i'm\" : \"I am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"I have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"shouldnt\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"thats\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"theres\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"theyre\":  \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"tryin'\":\"trying\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_contraction(contraction_map):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_map.keys()))\n",
    "    return contraction_map, contraction_re\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_contraction(contraction_mapping)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [00:00<00:00, 269599.50it/s]\n",
      "100%|██████████| 19579/19579 [00:00<00:00, 195622.28it/s]\n",
      "100%|██████████| 25344/25344 [00:00<00:00, 781258.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 46.90% of words in the vocab\n",
      "Found embeddings for 93.51%% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df, ['text'])\n",
    "\n",
    "sentences = df['text'].progress_apply(lambda x: x.split()).values\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "\n",
    "oov = check_coverage(vocab, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 1261),\n",
       " ('perdita', 169),\n",
       " ('countenance', 128),\n",
       " ('idris', 109),\n",
       " ('beheld', 88),\n",
       " ('gilman', 76),\n",
       " ('alas', 75),\n",
       " (\"'t\", 63),\n",
       " ('exceedingly', 62),\n",
       " ('frightful', 61)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Considering the writers are such weirdos, a lot of contractions that end in 's are clearly possessive so not direct way to deal with it without messing up the context for an embedding (for now at least) so I will move on know 93% of text is covered even though that accounts for less than half the words being used*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum text length: 4678\n",
      "Index of the row with the maximum text length: 9215\n",
      "Text in the row with the maximum text length: diotima approached the fountain seated herself on a mossy mound near it and her disciples placed themselves on the grass near her without noticing me who sat close under her she continued her discourse addressing as it happened one or other of her listeners but before i attempt to repeat her words i will describe the chief of these whom she appeared to wish principally to impress one was a woman of about years of age in the full enjoyment of the most exquisite beauty her golden hair floated in ringlets on her shoulders her hazle eyes were shaded by heavy lids and her mouth the lips apart seemed to breathe sensibility but she appeared thoughtful unhappy her cheek was pale she seemed as if accustomed to suffer and as if the lessons she now heard were the only words of wisdom to which she had ever listened the youth beside her had a far different aspect his form was emaciated nearly to a shadow his features were handsome but thin worn his eyes glistened as if animating the visage of decay his forehead was expansive but there was a doubt perplexity in his looks that seemed to say that although he had sought wisdom he had got entangled in some mysterious mazes from which he in vain endeavoured to extricate himself as diotima spoke his colour went came with quick changes the flexible muscles of his countenance shewed every impression that his mind received he seemed one who in life had studied hard but whose feeble frame sunk beneath the weight of the mere exertion of life the spark of intelligence burned with uncommon strength within him but that of life seemed ever on the eve of fading at present i shall not describe any other of this groupe but with deep attention try to recall in my memory some of the words of diotima they were words of fire but their path is faintly marked on my recollection it requires a just hand , said she continuing her discourse , to weigh divide the good from evil on the earth they are inextricably entangled and if you would cast away what there appears an evil a multitude of beneficial causes or effects cling to it mock your labour when i was on earth and have walked in a solitary country during the silence of night have beheld the multitude of stars , the soft radiance of the moon reflected on the sea , which was studded by lovely islands when i have felt the soft breeze steal across my cheek as the words of love it has soothed cherished me then my mind seemed almost to quit the body that confined it to the earth with a quick mental sense to mingle with the scene that i hardly saw i felt then i have exclaimed , oh world how beautiful thou art oh brightest universe behold thy worshiper spirit of beauty of sympathy which pervades all things , now lifts my soul as with wings , how have you animated the light the breezes deep inexplicable spirit give me words to express my adoration ; my mind is hurried away but with language i cannot tell how i feel thy loveliness silence or the song of the nightingale the momentary apparition of some bird that flies quietly past all seems animated with thee more than all the deep sky studded with worlds \"  if the winds roared tore the sea and the dreadful lightnings seemed falling around me still love was mingled with the sacred terror i felt ; the majesty of loveliness was deeply impressed on me so also i have felt when i have seen a lovely countenance or heard solemn music or the eloquence of divine wisdom flowing from the lips of one of its worshippers a lovely animal or even the graceful undulations of trees inanimate objects have excited in me the same deep feeling of love beauty ; a feeling which while it made me alive eager to seek the cause animator of the scene , yet satisfied me by its very depth as if i had already found the solution to my enquires sic as if in feeling myself a part of the great whole i had found the truth secret of the universe but when retired in my cell i have studied contemplated the various motions and actions in the world the weight of evil has confounded me if i thought of the creation i saw an eternal chain of evil linked one to the other from the great whale who in the sea swallows destroys multitudes the smaller fish that live on him also torment him to madness to the cat whose pleasure it is to torment her prey i saw the whole creation filled with pain each creature seems to exist through the misery of another death havoc is the watchword of the animated world and man also even in athens the most civilized spot on the earth what a multitude of mean passions envy , malice a restless desire to depreciate all that was great and good did i see and in the dominions of the great being i saw man reduced ?\n"
     ]
    }
   ],
   "source": [
    "max_length_row_index = df['text'].apply(len).idxmax()\n",
    "max_length = len(df.loc[max_length_row_index, 'text'])\n",
    "\n",
    "print(\"Maximum text length:\", max_length)\n",
    "print(\"Index of the row with the maximum text length:\", max_length_row_index)\n",
    "print(\"Text in the row with the maximum text length:\", df.loc[max_length_row_index, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_numbers(text):\n",
    "    # Define a regular expression pattern to match numbers\n",
    "    pattern = r'\\d+'\n",
    "    \n",
    "    # Use re.findall() to find all matches of the pattern in the text\n",
    "    numbers = re.findall(pattern, text)\n",
    "    \n",
    "    # Return the list of found numbers\n",
    "    return numbers\n",
    "\n",
    "\n",
    "df['numbers'] = df['text'].apply(find_numbers)\n",
    "print(df['numbers'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.62 s\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "model = AutoModel.from_pretrained('google-bert/bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(sentences):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    batch_size = 64  # Adjust based on your memory availability\n",
    "    embeddings = []\n",
    "    \n",
    "    # Wrap the range generator with tqdm for a progress bar\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Processing batches\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].detach().numpy())\n",
    "    \n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 306/306 [3:54:38<00:00, 46.01s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7h 15min 49s\n",
      "Wall time: 3h 54min 48s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_0</th>\n",
       "      <th>bert_1</th>\n",
       "      <th>bert_2</th>\n",
       "      <th>bert_3</th>\n",
       "      <th>bert_4</th>\n",
       "      <th>bert_5</th>\n",
       "      <th>bert_6</th>\n",
       "      <th>bert_7</th>\n",
       "      <th>bert_8</th>\n",
       "      <th>bert_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_1014</th>\n",
       "      <th>bert_1015</th>\n",
       "      <th>bert_1016</th>\n",
       "      <th>bert_1017</th>\n",
       "      <th>bert_1018</th>\n",
       "      <th>bert_1019</th>\n",
       "      <th>bert_1020</th>\n",
       "      <th>bert_1021</th>\n",
       "      <th>bert_1022</th>\n",
       "      <th>bert_1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622036</td>\n",
       "      <td>-0.783141</td>\n",
       "      <td>-0.768789</td>\n",
       "      <td>-0.661886</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.504465</td>\n",
       "      <td>-0.207343</td>\n",
       "      <td>-0.109943</td>\n",
       "      <td>-0.072224</td>\n",
       "      <td>0.794893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029174</td>\n",
       "      <td>-0.245753</td>\n",
       "      <td>-0.150378</td>\n",
       "      <td>0.802312</td>\n",
       "      <td>0.281644</td>\n",
       "      <td>0.201897</td>\n",
       "      <td>0.260581</td>\n",
       "      <td>-1.174859</td>\n",
       "      <td>0.127157</td>\n",
       "      <td>-0.273810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.701897</td>\n",
       "      <td>0.374661</td>\n",
       "      <td>-0.704999</td>\n",
       "      <td>-0.269579</td>\n",
       "      <td>0.633197</td>\n",
       "      <td>0.398946</td>\n",
       "      <td>0.595634</td>\n",
       "      <td>0.220773</td>\n",
       "      <td>0.074318</td>\n",
       "      <td>0.455393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075826</td>\n",
       "      <td>-0.567909</td>\n",
       "      <td>0.174853</td>\n",
       "      <td>-0.206451</td>\n",
       "      <td>-0.001130</td>\n",
       "      <td>0.299368</td>\n",
       "      <td>-0.053149</td>\n",
       "      <td>-0.235630</td>\n",
       "      <td>0.489391</td>\n",
       "      <td>-0.722882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.066654</td>\n",
       "      <td>-0.447025</td>\n",
       "      <td>-0.446701</td>\n",
       "      <td>0.109352</td>\n",
       "      <td>0.140808</td>\n",
       "      <td>0.046788</td>\n",
       "      <td>-0.435363</td>\n",
       "      <td>0.357560</td>\n",
       "      <td>0.830479</td>\n",
       "      <td>0.860996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875746</td>\n",
       "      <td>-0.177480</td>\n",
       "      <td>-0.641226</td>\n",
       "      <td>0.650413</td>\n",
       "      <td>0.383797</td>\n",
       "      <td>0.605456</td>\n",
       "      <td>-0.196379</td>\n",
       "      <td>-0.515951</td>\n",
       "      <td>0.297116</td>\n",
       "      <td>0.061242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.102057</td>\n",
       "      <td>-0.651902</td>\n",
       "      <td>-0.687253</td>\n",
       "      <td>0.218423</td>\n",
       "      <td>0.070657</td>\n",
       "      <td>0.326936</td>\n",
       "      <td>0.120010</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.599281</td>\n",
       "      <td>-0.026284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326366</td>\n",
       "      <td>-0.657595</td>\n",
       "      <td>-0.287512</td>\n",
       "      <td>0.891334</td>\n",
       "      <td>0.251273</td>\n",
       "      <td>0.203012</td>\n",
       "      <td>-0.006166</td>\n",
       "      <td>-0.761718</td>\n",
       "      <td>-0.435652</td>\n",
       "      <td>0.204584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.723254</td>\n",
       "      <td>-0.508487</td>\n",
       "      <td>-0.519215</td>\n",
       "      <td>0.261606</td>\n",
       "      <td>0.076165</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>-0.252534</td>\n",
       "      <td>0.405623</td>\n",
       "      <td>0.653150</td>\n",
       "      <td>0.391357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223673</td>\n",
       "      <td>-0.098319</td>\n",
       "      <td>-0.285708</td>\n",
       "      <td>0.296431</td>\n",
       "      <td>0.532552</td>\n",
       "      <td>0.990138</td>\n",
       "      <td>-0.120088</td>\n",
       "      <td>-1.080273</td>\n",
       "      <td>0.182172</td>\n",
       "      <td>0.078177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_0    bert_1    bert_2    bert_3    bert_4    bert_5    bert_6  \\\n",
       "0 -0.622036 -0.783141 -0.768789 -0.661886  0.028125  0.504465 -0.207343   \n",
       "1 -0.701897  0.374661 -0.704999 -0.269579  0.633197  0.398946  0.595634   \n",
       "2 -0.066654 -0.447025 -0.446701  0.109352  0.140808  0.046788 -0.435363   \n",
       "3 -0.102057 -0.651902 -0.687253  0.218423  0.070657  0.326936  0.120010   \n",
       "4 -0.723254 -0.508487 -0.519215  0.261606  0.076165  0.418882 -0.252534   \n",
       "\n",
       "     bert_7    bert_8    bert_9  ...  bert_1014  bert_1015  bert_1016  \\\n",
       "0 -0.109943 -0.072224  0.794893  ...   0.029174  -0.245753  -0.150378   \n",
       "1  0.220773  0.074318  0.455393  ...   0.075826  -0.567909   0.174853   \n",
       "2  0.357560  0.830479  0.860996  ...   0.875746  -0.177480  -0.641226   \n",
       "3  0.004033  0.599281 -0.026284  ...   0.326366  -0.657595  -0.287512   \n",
       "4  0.405623  0.653150  0.391357  ...   0.223673  -0.098319  -0.285708   \n",
       "\n",
       "   bert_1017  bert_1018  bert_1019  bert_1020  bert_1021  bert_1022  bert_1023  \n",
       "0   0.802312   0.281644   0.201897   0.260581  -1.174859   0.127157  -0.273810  \n",
       "1  -0.206451  -0.001130   0.299368  -0.053149  -0.235630   0.489391  -0.722882  \n",
       "2   0.650413   0.383797   0.605456  -0.196379  -0.515951   0.297116   0.061242  \n",
       "3   0.891334   0.251273   0.203012  -0.006166  -0.761718  -0.435652   0.204584  \n",
       "4   0.296431   0.532552   0.990138  -0.120088  -1.080273   0.182172   0.078177  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_embeddings = get_bert_embeddings(documents_train)\n",
    "bert_df_train = pd.DataFrame(bert_embeddings)\n",
    "bert_df_train.columns = ['bert_' + str(col) for col in bert_df_train.columns]\n",
    "bert_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df_train.to_csv('pytorch_bert_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
