# Think Like a Data Scientist - [Kaggle Text Classification Community Competition ](https://www.kaggle.com/competitions/ise-competition-1)
# What are the competition goals?
1. Predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley and HP Lovecraft. The predictions are evaluted using logloss.
2. Learn and apply ensembling method (both optuna and stacking).
3. Make a comprehensive note of methods and ideas used in this competition so as write a medium post and also use the knowledge in the YouTube project I am working on.
4. Build a methodology roadmap for text classification projects going forward
5. Write all code by hand. No copy pasting.
6. Create helper functions that can be reused for future problems and update the helper_functions.py file.

# What are my assumptions about the data and the competition?
*The aims of listing my assumptions is to check if they hold true and where not verifable acknowledge my assumptions*
1. A combination of feature engineering and BERT will produce a score better than the current competition private score.
2. The text excerpts match the original text. There was no error in data gathering and if there is, it is consistent between train and test data.

# What is my guiding light for this competition?
Bloom's Taxonomy and Ultralearning.

Remember -> Understand -> Apply -> Analyse -> Evaluate -> Create

# Has someone does this before?
I will consider the novelty from the angle of this being a text classification problem and then the answer is yes, many people have done text classification before.

To enable me gather information, I will go with the following plan.
- [ ] Search Kaggle for 'text classification'.
- [ ] Identify a list of top 30 Discussions (Topics) sorted by relevance. Let's call this ***Route A***
- [ ] Identify a list of top 15 Datasets sorted by relevanve. Let's call this ***Route B***
- [ ] Identify a list of top 10 Competitions sorted by relevance. Let's call this ***Route C***

## Route A - Discussions
* Write down methods from the top discussions (if there is code screenshot and save)
* Read the comments to see if there are any extra nuggets of wisdom that can be gotten

## Route B - Datasets
* Sort Code in Dataset by 'Most Votes' and then 'Most Comments', get the top 10 code or all Gold posts (whichever is more) for each sort. When sorting by 'Most Comments' after 'Most Votes', if post already exist in the list, go to the next available post.
* Study the notebooks for interesting and useful ideas.
* Screenshot any useful code. Ensure to name the code snippets properly and save in the appropraite folder.

## Route C - Competitions
* Sort Code in Competition by 'Most Votes' then 'Public Score' then 'Most Comments. Get top 10 posts or all Gold posts (whichever is more) for each sort. Like in Datasets route, ensure there are no duplicates posts considered.
* Do same for Discussion tab sorting by 'Most Votes' and then 'Most Comments'.
* Ensure as much useful information and code is gathered.

## Route Combination
Based off all the knowledge gathered so far, create a flowchart on the plan for the project referencing code snippets or ideas as annotations where necessary.

After this is done, take a 1 day break to recover from what I assume has been a stressful 6 days and then commence the actual execution of the project.

During execution, since code must have come from different sources. All code will have to be rewritten to match my style and project structure. ChatGPT (and other GPTs) are allowed as well as good old googling and reading of books.

# Gathering of Route Lists
## ROUTE A - DISCUSSIONS
List of top 30 dicussions
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
19. 
20. 
21. 
22. 
23. 
24. 
25. 
26. 
27. 
28. 
29. 
30. 

## ROUTE B - DATASETS
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 

## ROUTE C - COMPETITIONS
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 

## Data Assessment/Visualization: Poking and Proding
- There are no missing values

### Questions from Data Assessment
- If we run correlation feature selection how many features will be left due to the high correlation between features?
- Is data distribution density difference between train and original enough to affect the model?

## PROGRESS
