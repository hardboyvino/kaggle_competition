{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.model_selection import cross_validate, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "experiment_name = 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19579, 1024), (8392, 1024), (19579,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('bert_large_uncased_baseline_train.csv')\n",
    "test = pd.read_csv('bert_large_uncased_baseline_test.csv')\n",
    "\n",
    "target_data = pd.read_csv('train.csv')['author']\n",
    "\n",
    "train.shape, test.shape, target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(target_data)\n",
    "target_data_le = le.transform(target_data)\n",
    "target_data_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.copy()\n",
    "y = target_data_le\n",
    "\n",
    "n_splits = 3\n",
    "k3 = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=50))\n",
    "])\n",
    "\n",
    "# ridge_pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('nystroem', Nystroem(n_components=500, random_state=5)),\n",
    "#     ('ridge', Ridge())\n",
    "# ])\n",
    "\n",
    "linear_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Manually set pipeline names\n",
    "knn_pipeline.name = 'KNN'\n",
    "# ridge_pipeline.name = 'Nystroem Ridge'\n",
    "linear_pipeline.name = 'LR Pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    linear_pipeline,\n",
    "    LGBMClassifier(n_jobs=-1, random_state=5),\n",
    "    XGBClassifier(random_state=5),\n",
    "    RandomForestClassifier(random_state=5),\n",
    "    ExtraTreesClassifier(random_state=5),\n",
    "    HistGradientBoostingClassifier(random_state=5),\n",
    "    CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100),\n",
    "    knn_pipeline,\n",
    "    # ridge_pipeline,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y, important_features, cv_split, experiment_name):\n",
    "    Model_compare = pd.DataFrame(columns=['Model Name', \n",
    "                                        'Model Parameters', \n",
    "                                        'Model Train Log Loss', \n",
    "                                        'Model Test Log Loss', \n",
    "                                        'Model Test Log Loss Std', \n",
    "                                        'Model Time'])\n",
    "    \n",
    "    def evaluate_model(alg, idx):\n",
    "        if hasattr(alg, 'name'):\n",
    "            model_name = alg.name\n",
    "        else:\n",
    "            model_name = alg.__class__.__name__\n",
    "        features = important_features.get(model_name, [])\n",
    "\n",
    "        # Check if the list of important features is empty\n",
    "        if len(features) == 0:\n",
    "            # If empty, return results with zero values\n",
    "            print(f'Skipping {model_name} due to no important features.')\n",
    "            return {\n",
    "                'Model Name': model_name,\n",
    "                'Model Parameters': str(alg.get_params()),\n",
    "                'Model Train Log Loss': 0,\n",
    "                'Model Test Log Loss': 0,\n",
    "                'Model Test Log Loss Std': 0,\n",
    "                'Model Time': \"0 min 0.00 sec\",\n",
    "            }\n",
    "        \n",
    "        cv_results = cross_validate(alg, \n",
    "                                    X[features], \n",
    "                                    y, cv=cv_split, \n",
    "                                    scoring='neg_log_loss', \n",
    "                                    return_train_score=True, \n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Time formatting\n",
    "        mean_fit_time = cv_results['fit_time'].mean()\n",
    "        minutes, seconds = divmod(mean_fit_time, 60)\n",
    "\n",
    "        # Results population\n",
    "        result = {\n",
    "            'Model Name': model_name,\n",
    "            'Model Parameters': str(alg.get_params()),\n",
    "            'Model Train Log Loss': -cv_results['train_score'].mean(),\n",
    "            'Model Test Log Loss': -cv_results['test_score'].mean(),\n",
    "            'Model Test Log Loss Std': cv_results['test_score'].std(),\n",
    "            'Model Time': f\"{int(minutes)} min {seconds:.2f} sec\",\n",
    "        }\n",
    "\n",
    "        print(f'Done with {model_name}.')\n",
    "        return result\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        futures = [executor.submit(evaluate_model, alg, idx) for idx, alg in enumerate(tqdm(models, desc='Models'))]\n",
    "        for future in tqdm(futures, total=len(futures), desc='Progress'):\n",
    "            result = future.result()\n",
    "            results_list.append(result)\n",
    "\n",
    "    model_compare = pd.DataFrame(results_list)\n",
    "\n",
    "    model_compare.sort_values(by=['Model Test Log Loss'], ascending=True, inplace=True)\n",
    "    model_compare.to_csv(f'{experiment_name}_results.csv', index=False)\n",
    "\n",
    "    return model_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = {}\n",
    "\n",
    "for model in models:\n",
    "    if hasattr(model, 'name'):\n",
    "        model_name = model.name\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    baseline_features[model_name] = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3752cf538c74ae4a5e17193720b31eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456e3ed99bfd4bb48b517e8a5e585706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LogisticRegression.\n",
      "Done with ExtraTreesClassifier.\n",
      "Done with LR Pipeline.\n",
      "Done with RandomForestClassifier.\n",
      "Done with LGBMClassifier.\n",
      "Done with KNN.\n",
      "Done with HistGradientBoostingClassifier.\n",
      "Done with CatBoostClassifier.\n",
      "Done with XGBClassifier.\n",
      "CPU times: total: 3.16 s\n",
      "Wall time: 55min 52s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Model Parameters</th>\n",
       "      <th>Model Train Log Loss</th>\n",
       "      <th>Model Test Log Loss</th>\n",
       "      <th>Model Test Log Loss Std</th>\n",
       "      <th>Model Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>4.118485e-01</td>\n",
       "      <td>0.570330</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0 min 13.85 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>{'verbose': False, 'random_state': 5, 'early_s...</td>\n",
       "      <td>2.032693e-01</td>\n",
       "      <td>0.576469</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>32 min 47.66 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HistGradientBoostingClassifier</td>\n",
       "      <td>{'categorical_features': None, 'early_stopping...</td>\n",
       "      <td>1.611123e-01</td>\n",
       "      <td>0.598627</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>7 min 3.50 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>2.297708e-01</td>\n",
       "      <td>0.609872</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>3 min 26.15 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR Pipeline</td>\n",
       "      <td>{'memory': None, 'steps': [('scaler', Standard...</td>\n",
       "      <td>3.931900e-01</td>\n",
       "      <td>0.614112</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>0 min 12.67 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'objective': 'binary:logistic', 'use_label_en...</td>\n",
       "      <td>2.556679e-02</td>\n",
       "      <td>0.614505</td>\n",
       "      <td>0.008706</td>\n",
       "      <td>54 min 42.47 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'memory': None, 'steps': [('scaler', Standard...</td>\n",
       "      <td>7.277288e-01</td>\n",
       "      <td>0.761946</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0 min 1.22 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>2.269657e-01</td>\n",
       "      <td>0.839461</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>1 min 59.66 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'class_...</td>\n",
       "      <td>2.109424e-15</td>\n",
       "      <td>0.855681</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0 min 39.65 sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model Name  \\\n",
       "0              LogisticRegression   \n",
       "7              CatBoostClassifier   \n",
       "6  HistGradientBoostingClassifier   \n",
       "2                  LGBMClassifier   \n",
       "1                     LR Pipeline   \n",
       "3                   XGBClassifier   \n",
       "8                             KNN   \n",
       "4          RandomForestClassifier   \n",
       "5            ExtraTreesClassifier   \n",
       "\n",
       "                                    Model Parameters  Model Train Log Loss  \\\n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...          4.118485e-01   \n",
       "7  {'verbose': False, 'random_state': 5, 'early_s...          2.032693e-01   \n",
       "6  {'categorical_features': None, 'early_stopping...          1.611123e-01   \n",
       "2  {'boosting_type': 'gbdt', 'class_weight': None...          2.297708e-01   \n",
       "1  {'memory': None, 'steps': [('scaler', Standard...          3.931900e-01   \n",
       "3  {'objective': 'binary:logistic', 'use_label_en...          2.556679e-02   \n",
       "8  {'memory': None, 'steps': [('scaler', Standard...          7.277288e-01   \n",
       "4  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...          2.269657e-01   \n",
       "5  {'bootstrap': False, 'ccp_alpha': 0.0, 'class_...          2.109424e-15   \n",
       "\n",
       "   Model Test Log Loss  Model Test Log Loss Std        Model Time  \n",
       "0             0.570330                 0.005785   0 min 13.85 sec  \n",
       "7             0.576469                 0.002246  32 min 47.66 sec  \n",
       "6             0.598627                 0.006025    7 min 3.50 sec  \n",
       "2             0.609872                 0.003729   3 min 26.15 sec  \n",
       "1             0.614112                 0.008366   0 min 12.67 sec  \n",
       "3             0.614505                 0.008706  54 min 42.47 sec  \n",
       "8             0.761946                 0.004983    0 min 1.22 sec  \n",
       "4             0.839461                 0.002836   1 min 59.66 sec  \n",
       "5             0.855681                 0.003275   0 min 39.65 sec  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline_models = evaluate_models(models, X, y, baseline_features, k3, f'{experiment_name}')\n",
    "baseline_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model2 = linear_pipeline\n",
    "model3 = LGBMClassifier(n_jobs=-1, random_state=5)\n",
    "model4 = XGBClassifier(random_state=5)\n",
    "model5 = ExtraTreesClassifier(random_state=5)\n",
    "model6 = HistGradientBoostingClassifier(random_state=5)\n",
    "model7 = CatBoostClassifier(random_state=5, verbose=False, early_stopping_rounds=100)\n",
    "model8 = knn_pipeline\n",
    "model9 = RandomForestClassifier(random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optuna weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Lists to store predictions and true values\n",
    "preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8, preds9, y_vals = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# Perform cross-validation\n",
    "for i, (train_index, val_index) in enumerate(k3.split(X)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Fit models on the training fold\n",
    "    model1.fit(X_train, y_train)\n",
    "    model2.fit(X_train, y_train)\n",
    "    model3.fit(X_train, y_train)\n",
    "    model4.fit(X_train, y_train)\n",
    "    model5.fit(X_train, y_train)\n",
    "    model6.fit(X_train, y_train)\n",
    "    model7.fit(X_train, y_train)\n",
    "    model8.fit(X_train, y_train)\n",
    "    model9.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation fold\n",
    "    pred1 = model1.predict(X_val)\n",
    "    pred2 = model2.predict(X_val)\n",
    "    pred3 = model3.predict(X_val)\n",
    "    pred4 = model4.predict(X_val)\n",
    "    pred5 = model5.predict(X_val)\n",
    "    pred6 = model6.predict(X_val)\n",
    "    pred7 = model7.predict(X_val)\n",
    "    pred8 = model8.predict(X_val)\n",
    "    pred9 = model9.predict(X_val)\n",
    "\n",
    "    # Store predictions and true values\n",
    "    preds1.append(pred1)\n",
    "    preds2.append(pred2)\n",
    "    preds3.append(pred3)\n",
    "    preds4.append(pred4)\n",
    "    preds5.append(pred5)\n",
    "    preds6.append(pred6)\n",
    "    preds7.append(pred7)\n",
    "    preds8.append(pred8)\n",
    "    preds9.append(pred9)\n",
    "    y_vals.append(y_val)\n",
    "\n",
    "    print(f'Done with fold {i+1}')\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation\n",
    "preds1 = np.concatenate(preds1)\n",
    "preds2 = np.concatenate(preds2)\n",
    "preds3 = np.concatenate(preds3)\n",
    "preds4 = np.concatenate(preds4)\n",
    "preds5 = np.concatenate(preds5)\n",
    "preds6 = np.concatenate(preds6)\n",
    "preds7 = np.concatenate(preds7)\n",
    "preds8 = np.concatenate(preds8)\n",
    "preds9 = np.concatenate(preds9)\n",
    "y_vals = np.concatenate(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # List of all model predictions\n",
    "    models = [preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8, preds9]\n",
    "\n",
    "    # Suggest binary variables to select models\n",
    "    selected_models = [trial.suggest_int(f'select_model_{i}', 0, 1) for i in range(len(models))]\n",
    "\n",
    "    # Ensure at least one model is selected\n",
    "    if sum(selected_models) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    # Suggest weights for each model\n",
    "    weights = [trial.suggest_float(f'w{i}', 0, 1) for i in range(len(models))]\n",
    "\n",
    "    # Select models and their corresponding weights\n",
    "    selected_preds = [models[i] for i in range(len(models)) if selected_models[i]]\n",
    "    selected_weights = [weights[i] for i in range(len(models)) if selected_models[i]]\n",
    "\n",
    "    # # Normalize the weights\n",
    "    # total_weight = sum(selected_weights)\n",
    "    # selected_weights = [w / total_weight for w in selected_weights]\n",
    "\n",
    "    # Compute the blended predictions\n",
    "    blended_preds = sum(w * p for w, p in zip(selected_weights, selected_preds))\n",
    "\n",
    "    # Compute the MAE\n",
    "    mae = mean_absolute_error(y_vals, blended_preds)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize', study_name='mae_with_bert')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: {}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "#   Value: 0.21366240098504596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the best found weights for final prediction\n",
    "select_model_1 = trial.params['select_model_0']\n",
    "select_model_2 = trial.params['select_model_1']\n",
    "select_model_3 = trial.params['select_model_2']\n",
    "select_model_4 = trial.params['select_model_3']\n",
    "select_model_5 = trial.params['select_model_4']\n",
    "select_model_6 = trial.params['select_model_5']\n",
    "select_model_7 = trial.params['select_model_6']\n",
    "select_model_8 = trial.params['select_model_7']\n",
    "select_model_9 = trial.params['select_model_8']\n",
    "\n",
    "best_w1 = trial.params['w0']\n",
    "best_w2 = trial.params['w1']\n",
    "best_w3 = trial.params['w2']\n",
    "best_w4 = trial.params['w3']\n",
    "best_w5 = trial.params['w4']\n",
    "best_w6 = trial.params['w5']\n",
    "best_w7 = trial.params['w6']\n",
    "best_w8 = trial.params['w7']\n",
    "best_w9 = trial.params['w8']\n",
    "\n",
    "# Best trial:\n",
    "#   Value: 0.13884898356282638\n",
    "#   Params: \n",
    "#     select_model_0: 0\n",
    "#     select_model_1: 1\n",
    "#     select_model_2: 0\n",
    "#     select_model_3: 0\n",
    "#     select_model_4: 0\n",
    "#     select_model_5: 1\n",
    "#     select_model_6: 0\n",
    "#     select_model_7: 0\n",
    "#     select_model_8: 1\n",
    "#     w0: 0.22379923583104128\n",
    "#     w1: 0.00022255362990576655\n",
    "#     w2: 0.8692260835078589\n",
    "#     w3: 0.801887498510167\n",
    "#     w4: 0.17577840191712701\n",
    "#     w5: 0.023900882561639553\n",
    "#     w6: 0.41999673367380513\n",
    "#     w7: 0.5145131188279755\n",
    "#     w8: 0.11767006738147931\n",
    "\n",
    "\n",
    "# total_weight = best_w1 + best_w2 + best_w3\n",
    "# best_w1 /= total_weight\n",
    "# best_w2 /= total_weight\n",
    "# best_w3 /= total_weight\n",
    "\n",
    "# Make final ensemble predictions with the best weights\n",
    "final_ensemble_pred = (select_model_1 * best_w1 * model1.predict(test)) + (select_model_2 * best_w2 * model2.predict(test)) + (select_model_3 * best_w3 * model3.predict(test)) + (select_model_4 * best_w4 * model4.predict(test)) + (select_model_5 * best_w5 * model5.predict(test)) + (select_model_6 * best_w6 * model6.predict(test)) + (select_model_7 * best_w7 * model7.predict(test)) + (select_model_8 * best_w8 * model8.predict(test)) + (select_model_9 * best_w9 * model9.predict(test))\n",
    "\n",
    "# Adjust validation target data to replace negative values with 0.05\n",
    "final_ensemble_pred_adjusted = np.maximum(final_ensemble_pred, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest weights for each model\n",
    "    w1 = trial.suggest_float('w1', 0, 1)\n",
    "    w2 = trial.suggest_float('w2', 0, 1)\n",
    "    w3 = trial.suggest_float('w3', 0, 1)\n",
    "    w4 = trial.suggest_float('w4', 0, 1)\n",
    "    w5 = trial.suggest_float('w5', 0, 1)\n",
    "    w6 = trial.suggest_float('w6', 0, 1)\n",
    "    w7 = trial.suggest_float('w7', 0, 1)\n",
    "    w8 = trial.suggest_float('w8', 0, 1)\n",
    "    w9 = trial.suggest_float('w9', 0, 1)\n",
    "\n",
    "    # # Normalize the weights\n",
    "    # total_weight = w1 + w2 + w3\n",
    "    # w1 /= total_weight\n",
    "    # w2 /= total_weight\n",
    "    # w3 /= total_weight\n",
    "\n",
    "    # Compute the blended predictions\n",
    "    blended_preds = w1 * preds1 + w2 * preds2 + w3 * preds3 + w4 * preds4 + w5 * preds5 + w6 * preds6 + w7 * preds7 + w8 * preds8 + w9 * preds9\n",
    "\n",
    "    # Compute the MAE\n",
    "    mae = mean_absolute_error(y_vals, blended_preds)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize', study_name='mae_with_bert')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: {}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the best found weights for final prediction\n",
    "best_w1 = trial.params['w1']\n",
    "best_w2 = trial.params['w2']\n",
    "best_w3 = trial.params['w3']\n",
    "best_w4 = trial.params['w4']\n",
    "best_w5 = trial.params['w5']\n",
    "best_w6 = trial.params['w6']\n",
    "best_w7 = trial.params['w7']\n",
    "best_w8 = trial.params['w8']\n",
    "best_w9 = trial.params['w9']\n",
    "\n",
    "# Best trial:\n",
    "#   Value: 0.21366240098504596\n",
    "#   Params: \n",
    "#     w1: 0.08471910659519587\n",
    "#     w2: 3.409643176334812e-05\n",
    "#     w3: 0.08501403202520018\n",
    "#     w4: 0.04662479238055933\n",
    "#     w5: 0.17637806864578287\n",
    "#     w6: 0.03739525358225952\n",
    "#     w7: 0.02184565223397708\n",
    "#     w8: 0.3732534808432757\n",
    "#     w9: 0.017576063578009754\n",
    "\n",
    "\n",
    "# total_weight = best_w1 + best_w2 + best_w3\n",
    "# best_w1 /= total_weight\n",
    "# best_w2 /= total_weight\n",
    "# best_w3 /= total_weight\n",
    "\n",
    "# Make final ensemble predictions with the best weights\n",
    "final_ensemble_pred = best_w1 * model1.predict(test) + best_w2 * model2.predict(test) + best_w3 * model3.predict(test) + best_w4 * model4.predict(test) + best_w5 * model5.predict(test) + best_w6 * model6.predict(test) + best_w7 * model7.predict(test) + best_w8 * model8.predict(test) + best_w9 * model9.predict(test)\n",
    "\n",
    "# Adjust validation target data to replace negative values with 0.05\n",
    "final_ensemble_pred_adjusted = np.maximum(final_ensemble_pred, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_score = mean_absolute_error(target_test_data, final_ensemble_pred)\n",
    "ensemble_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ensemble_pred_df = pd.DataFrame(final_ensemble_pred, columns=['Predicted'])\n",
    "\n",
    "test_true_pred_df = pd.concat([target_test_data, final_ensemble_pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_pred_df.sort_values(by='vps', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_pred_df.sort_values(by='vps', ascending=False).iloc[[12, 22, 32, 42, 52, 62, 72, 82, 92, 102]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
