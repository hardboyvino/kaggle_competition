{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21252159-55dc-410c-9c35-b979dccfdfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Adeniyi Babalola\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82abad9-38c1-4bb5-a18e-af8fb91f0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment_name = 'pytorch_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50bf547-a099-4295-8e18-30bdd8a89fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (19579, 3)\n",
      "Test shape: (8392, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "print(f'Train shape: {df.shape}'), print(f'Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f828a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d224c89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1958, 17621)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=5)\n",
    "\n",
    "# Get the indices for the validation set\n",
    "for _, test_indx in sss.split(df, df[TARGET]):\n",
    "    valid_df = df.iloc[test_indx]\n",
    "    train_df = df.drop(test_indx)\n",
    "\n",
    "len(valid_df), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a9218f-c590-4b90-a3d4-2b8f1adc5202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.98 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set model and tokenizer for finetuning. Ensure num_labels matches the number of labels\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-large-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4246c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df the same as the valid_df so I do not have to change too much code\n",
    "\n",
    "df = valid_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c069a4fd-9c18-4192-81f4-8789f97f3c15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>encoded_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12281</th>\n",
       "      <td>id14874</td>\n",
       "      <td>But the memory of past sorrow is it not presen...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author  \\\n",
       "12281  id14874  But the memory of past sorrow is it not presen...    EAP   \n",
       "\n",
       "       encoded_labels  \n",
       "12281               0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['encoded_labels'] = label_encoder.fit_transform(df['author'])  # Convert string labels to integers\n",
    "\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2020c0-325a-413d-ac94-f0a39aa22a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1762\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 84\n",
      "  Number of trainable parameters = 335144963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa52a0cea139432681e099ce7468b43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['encoded_labels']\n",
    "        encoding = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = CustomDataset(df, tokenizer, max_len=512)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    # eval_strategy='epoch',\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    # warmup_steps=500,\n",
    "    # weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    learning_rate=3e-4,\n",
    "    save_steps=5000,\n",
    "    dataloader_num_workers=4,\n",
    "    seed=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a7d4b-cfd3-49ad-b445-148471c69aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the finetuned model for further use (rename as appropraite)\n",
    "\n",
    "model.save_pretrained('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e57600-5a6b-443d-924e-04c2d0771bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the pretrained model for embedding the training and test data\n",
    "\n",
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./trained_model')\n",
    "model = AutoModel.from_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8df74-e8e9-4d51-861a-f7a3acf4152e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(sentences):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    batch_size = 64  # Adjust based on your memory availability\n",
    "    embeddings = []\n",
    "    \n",
    "    # Wrap the range generator with tqdm for a progress bar\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Processing batches\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].detach().numpy())\n",
    "    \n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9791f-ac6c-49b3-a3ad-84c0c6e3fc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the text column in to a list\n",
    "\n",
    "documents_train = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f2869-6d80-434f-ae6b-a6def25e83bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bert_embeddings = get_bert_embeddings(documents_train)\n",
    "bert_df_train = pd.DataFrame(bert_embeddings)\n",
    "bert_df_train.columns = ['bert_' + str(col) for col in bert_df_train.columns]\n",
    "bert_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b406f-0009-47dc-9194-8582b9610406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the NLP embedding into a CSV which can be used the training data\n",
    "\n",
    "bert_df_train.to_csv('pytorch_bert_large_uncased_finetuned_valid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
