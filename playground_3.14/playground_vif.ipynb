{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test datasets\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target for train and validation data\n",
    "\n",
    "X_train = df_train.drop([\"id\", \"yield\"], axis=1)\n",
    "y_train = df_train[\"yield\"]\n",
    "\n",
    "X_test = df_test.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clonesize',\n",
       " 'honeybee',\n",
       " 'bumbles',\n",
       " 'andrena',\n",
       " 'osmia',\n",
       " 'MaxOfUpperTRange',\n",
       " 'MinOfUpperTRange',\n",
       " 'AverageOfUpperTRange',\n",
       " 'MaxOfLowerTRange',\n",
       " 'MinOfLowerTRange',\n",
       " 'AverageOfLowerTRange',\n",
       " 'RainingDays',\n",
       " 'AverageRainingDays',\n",
       " 'fruitset',\n",
       " 'fruitmass',\n",
       " 'seeds']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = list(X_train.columns)\n",
    "\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df, predictors):\n",
    "    \"\"\"\n",
    "    Calculates the Variance Inflation Factor (VIF) for the given predictor variables in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df (DataFrame): The DataFrame containing the predictor variables.\n",
    "    predictors (list): A list of column names for the predictor variables.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    vif (DataFrame): A DataFrame containing the predictor variable names and their corresponding VIF values.\n",
    "    \"\"\"\n",
    "    X = df[predictors]\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variable\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the VIF threshold\n",
    "vif_threshold = 11\n",
    "\n",
    "# Remove features with VIF above the threshold one at a time\n",
    "while True:\n",
    "    vif = calculate_vif(X_train, predictors)\n",
    "    max_vif = vif[\"VIF\"].max()\n",
    "\n",
    "    if max_vif <= vif_threshold:\n",
    "        break\n",
    "\n",
    "    # Identify the predictor with the highest VIF and remove it\n",
    "    predictor_to_remove = vif.loc[vif[\"VIF\"] == max_vif, \"variable\"].values[0]\n",
    "    predictors.remove(predictor_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X_train and X_test to use only columns remaining after collinearity calculation\n",
    "\n",
    "X_train = X_train[predictors]\n",
    "X_test = X_test[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Evaluates the given model using cross-validation and calculates the Mean Absolute Errors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model (estimator object): The model to be evaluated.\n",
    "    X (DataFrame): The feature matrix.\n",
    "    y (Series): The target variable.\n",
    "    selected_features (Index): The selected feature names.\n",
    "    n_splits (int): The number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mae_scores (list): A list of MAE for each fold.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the MAE\n",
    "    mae_scores = []\n",
    "\n",
    "    # Create a KFold object for cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into train and test sets for the current fold\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        # Predict probabilities for the test (keep only the probability of the positive class)\n",
    "        y_pred = model.predict(X_test_cv)\n",
    "\n",
    "        # Calculate the MAE for the current fold\n",
    "        mae = mean_absolute_error(y_test_cv, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    # Return the MAE, rounded to 5 decimal places\n",
    "    return [round(value, 3) for value in mae_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "\n",
    "models = {\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=5),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=5),\n",
    "    \"CatBoost\": CatBoostRegressor(silent=True, random_seed=5),\n",
    "    \"SVR\": SVR(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Lasso\n",
      "MAE Scores: [827.479, 834.203, 826.284, 842.843, 825.666]\n",
      "Average MAE: 831.295\n",
      "Std Deviation: 6.529\n",
      "\n",
      "Model: LightGBM\n",
      "MAE Scores: [809.091, 818.55, 811.42, 822.542, 815.523]\n",
      "Average MAE: 815.425\n",
      "Std Deviation: 4.829\n",
      "\n",
      "Model: RandomForest\n",
      "MAE Scores: [810.053, 821.265, 813.503, 824.36, 818.189]\n",
      "Average MAE: 817.474\n",
      "Std Deviation: 5.160\n",
      "\n",
      "Model: CatBoost\n",
      "MAE Scores: [810.937, 821.369, 811.997, 822.878, 819.096]\n",
      "Average MAE: 817.255\n",
      "Std Deviation: 4.889\n",
      "\n",
      "Model: SVR\n",
      "MAE Scores: [882.668, 902.168, 896.376, 901.635, 892.227]\n",
      "Average MAE: 895.015\n",
      "Std Deviation: 7.170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model using the best subset of features\n",
    "for name, model in models.items():\n",
    "    \"\"\"\n",
    "    Loops through each model, and evaluates the model using cross-validation. \n",
    "    Prints the MAE scores, average MAE, and standard deviation\n",
    "    for each model.\n",
    "    \"\"\"\n",
    "    # Evaluate the model using cross-validation with the selected features\n",
    "    mae_scores = evaluate_model(model, X_train, y_train)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    std = np.std(mae_scores)\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict for the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    df_test[\"yield\"] = y_test_pred.round(3)\n",
    "\n",
    "    # Save the output DataFrame to a CSV file\n",
    "    df_test[[\"id\", \"yield\"]].to_csv(f\"submission_{name}.csv\", index=False)\n",
    "\n",
    "    # Print the results for the current model\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"MAE Scores: {mae_scores}\")\n",
    "    print(f\"Average MAE: {mean_mae:.3f}\")\n",
    "    print(f\"Std Deviation: {std:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "Rounding fruitset and fruitmass made the model worse. Discard the idea!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
