{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia',\n",
       "       'MaxOfUpperTRange', 'MinOfUpperTRange', 'AverageOfUpperTRange',\n",
       "       'MaxOfLowerTRange', 'MinOfLowerTRange', 'AverageOfLowerTRange',\n",
       "       'RainingDays', 'AverageRainingDays', 'fruitset', 'fruitmass', 'seeds',\n",
       "       'yield'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the train and test datasets\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target for train and validation data\n",
    "\n",
    "X_train = df_train.drop([\"id\", \"yield\"], axis=1)\n",
    "y_train = df_train[\"yield\"]\n",
    "\n",
    "X_test = df_test.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop = [\"MinOfUpperTRange\", \"AverageOfUpperTRange\", \"MaxOfLowerTRange\", \"MinOfLowerTRange\", \"AverageOfLowerTRange\",\"AverageRainingDays\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns that are perfectly correlated with each other from train and test\n",
    "\n",
    "X_train = X_train.drop(column_to_drop, axis=1)\n",
    "X_test = X_test.drop(column_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y, rs, n_splits=5):\n",
    "    \"\"\"\n",
    "    Evaluates the given model using cross-validation and calculates the Mean Absolute Errors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model (estimator object): The model to be evaluated.\n",
    "    X (DataFrame): The feature matrix.\n",
    "    y (Series): The target variable.\n",
    "    selected_features (Index): The selected feature names.\n",
    "    n_splits (int): The number of folds for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mae_scores (list): A list of MAE for each fold.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the MAE\n",
    "    mae_scores = []\n",
    "\n",
    "    # Create a KFold object for cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into train and test sets for the current fold\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model on the training data\n",
    "        rs.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        # Predict probabilities for the test (keep only the probability of the positive class)\n",
    "        y_pred = rs.predict(X_test_cv)\n",
    "\n",
    "        # Calculate the MAE for the current fold\n",
    "        mae = mean_absolute_error(y_test_cv, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    # Return the MAE, rounded to 5 decimal places\n",
    "    return [round(value, 3) for value in mae_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids\n",
    "\n",
    "lasso_params = {\n",
    "    \"alpha\": [1e-10, 0.1, 0.5, 1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "lightgbm_params = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'num_leaves': [10, 20, 30, 40, 50],\n",
    "    'feature_fraction': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'bagging_fraction': [0.5, 0.7, 0.9, 1.0, 1.1],\n",
    "    'bagging_freq': [1, 2, 3, 4, 5],\n",
    "    'min_data_in_leaf': [1, 5, 10, 20, 30],\n",
    "    'min_gain_to_split': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'lambda_l1': [0, 0.001, 0.01, 0.1, 1.0],\n",
    "    'lambda_l2': [0, 0.001, 0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'min_samples_leaf': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"depth\": [3, 5, 7, 9],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
    "    \"iterations\": [30, 50, 100, 500, 1000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for each model\n",
    "\n",
    "param_grids = {\n",
    "    \"Lasso\": lasso_params,\n",
    "    \"LightGBM\": lightgbm_params,\n",
    "    \"RandomForest\": rf_params,\n",
    "    \"CatBoost\": catboost_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "\n",
    "models = {\n",
    "    \"Lasso\": Lasso(random_state=5),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=5, n_jobs=-1),\n",
    "    # \"RandomForest\": RandomForestRegressor(random_state=5, n_jobs=-1),\n",
    "    \"CatBoost\": CatBoostRegressor(silent=True, random_seed=5),\n",
    "    # \"SVR\": SVR(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Lasso\n",
      "Selected features: Index(['bumbles', 'andrena', 'osmia', 'MaxOfUpperTRange', 'RainingDays',\n",
      "       'fruitset', 'seeds'],\n",
      "      dtype='object')\n",
      "Best RandomizedSearch features: {'alpha': 1e-10}\n",
      "Best RandomizedSearch Score: -376.6603809600663\n",
      "MAE Scores: [377.766, 384.3, 379.012, 373.334, 368.963]\n",
      "Average MAE: 376.675\n",
      "Std Deviation: 5.204\n",
      "\n",
      "Model: LightGBM\n",
      "Selected features: Index(['clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia',\n",
      "       'MaxOfUpperTRange', 'RainingDays', 'fruitset', 'fruitmass', 'seeds'],\n",
      "      dtype='object')\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=1.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Best RandomizedSearch features: {'num_leaves': 30, 'min_gain_to_split': 1.0, 'min_data_in_leaf': 5, 'max_depth': 11, 'learning_rate': 0.05, 'lambda_l2': 0.01, 'lambda_l1': 1.0, 'feature_fraction': 0.9, 'bagging_freq': 1, 'bagging_fraction': 0.5}\n",
      "Best RandomizedSearch Score: -355.11567895382103\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=2.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=2.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=2.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=2.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=2.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=2.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_gain_to_split is set=1.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "MAE Scores: [348.989, 361.627, 352.795, 354.597, 351.5]\n",
      "Average MAE: 353.902\n",
      "Std Deviation: 4.273\n",
      "\n",
      "Model: CatBoost\n",
      "Selected features: Index(['honeybee', 'MaxOfUpperTRange', 'fruitset', 'seeds'], dtype='object')\n",
      "Best RandomizedSearch features: {'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 500, 'depth': 7}\n",
      "Best RandomizedSearch Score: -354.1262568290135\n",
      "MAE Scores: [352.982, 361.267, 352.62, 351.479, 348.303]\n",
      "Average MAE: 353.330\n",
      "Std Deviation: 4.297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model using the best subset of features\n",
    "for name, model in models.items():\n",
    "    \"\"\"\n",
    "    Loops through each model, and evaluates the model using cross-validation. \n",
    "    Prints the MAE scores, average MAE, and standard deviation\n",
    "    for each model.\n",
    "    \"\"\"\n",
    "    #Initialize SFS with the current model\n",
    "    sfs = SFS(model,\n",
    "              k_features=\"best\",\n",
    "              forward=True,\n",
    "              floating=False,\n",
    "              scoring=\"neg_mean_absolute_error\",\n",
    "              cv=3,\n",
    "              n_jobs=-1)\n",
    "    \n",
    "    # Perform SFS on the training data\n",
    "    sfs = sfs.fit(X_train, y_train)\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features = X_train.columns[list(sfs.k_feature_idx_)]\n",
    "\n",
    "    #Print the results for the current model\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "    # Run RandomizedSearch on each model\n",
    "    rs = RandomizedSearchCV(model, param_grids[name], cv=3, scoring=\"neg_mean_absolute_error\", n_iter=100, n_jobs=-1)\n",
    "    rs.fit(X_train[selected_features], y_train)\n",
    "    best_params = rs.best_params_\n",
    "\n",
    "    print(f\"Best RandomizedSearch features: {rs.best_params_}\")\n",
    "    print(f\"Best RandomizedSearch Score: {rs.best_score_}\")\n",
    "    \n",
    "    # Evaluate the model using cross-validation with the selected features\n",
    "    mae_scores = evaluate_model(X_train[selected_features], y_train, rs)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    std = np.std(mae_scores)\n",
    "\n",
    "    # # Train the model on the training set\n",
    "    # rs.fit(X_train[selected_features], y_train)\n",
    "\n",
    "    # Predict for the test set\n",
    "    y_test_pred = rs.predict(X_test[selected_features])\n",
    "\n",
    "    df_test[\"yield\"] = y_test_pred.round(3)\n",
    "\n",
    "    # Save the output DataFrame to a CSV file\n",
    "    df_test[[\"id\", \"yield\"]].to_csv(f\"submission_{name}.csv\", index=False)\n",
    "\n",
    "    # Print the results for the current model\n",
    "    # print(f\"Model: {name}\")\n",
    "    print(f\"MAE Scores: {mae_scores}\")\n",
    "    print(f\"Average MAE: {mean_mae:.3f}\")\n",
    "    print(f\"Std Deviation: {std:.3f}\")\n",
    "\n",
    "    # try:\n",
    "    #     plt.figure(figsize=(10, 7))\n",
    "    #     plt.plot(model.feature_importances_, label=name)\n",
    "    #     plt.xticks(np.arange(X_train.shape[1]), X_train.columns.tolist(), rotation=90)\n",
    "    #     plt.legend()\n",
    "    \n",
    "    # except AttributeError: # Incase the model does not have \"feature_importances_\"\n",
    "    #     pass\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Roughly 30 minutes run time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
